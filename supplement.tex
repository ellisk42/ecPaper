\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{stmaryrd}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\sem}[1]{\llbracket #1 \rrbracket}
\newcommand{\system}{\textsc{CoCoSea}~}
\newcommand{\systemEnding}{\textsc{CoCoSea}}
\newcommand{\lowerBound}{\mathscr{L}}
\newcommand{\code}[1]{{\footnotesize\texttt{#1}}}
\newcommand{\codechar}[1]{{\footnotesize{\texttt{"#1"}}}}

\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{amsthm}

\usepackage{subfig} 
\usepackage{fancyvrb}


\usepackage{caption}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{circuitikz}
\usetikzlibrary{fit,bayesnet}
\usepackage{dsfont}
\usepackage{amsmath}

\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
 


% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em E}}
\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}

\title{Supplement to: Learning Libraries of Subroutines for Neurally--Guided Bayesian Program Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Learning abstract geometry}
The brifly presented geometry project is a recent addition to the set of
problems adressed with our method.

The original DSL, while inspired by the Logo~\cite{abelson1974logo} in the sense
that ones controls a moving on-off point, was designed to be congitively
plausible in the sense that some abstractions are baked in for curves and a few
structures --- it was first described in~\cite{sablemeyer2017geom}.

It is to be thought as a turtle with as internal state a position, a facing
direction, a speed, an acceleration, an angular speed and an angular
acceleration.

The DSL is the following:

\begin{center}
\tt
\begin{tabular}{rl}
  \toprule
  \textrm{\emph{name}}    & \textrm{\emph{type}} \\
  \midrule
  Concat    & \text{prog}      \to \text{prog}        \to \text{prog} \\
  Repeat    & var option        \to \text{prog}       \to \text{prog} \\
  Embed     & \text{prog}      \to \text{prog}                        \\
  Define    & \text{var}        \to \text{prog}                       \\
  Turn      & \text{var option} \to \text{prog}                       \\
  Integrate & \text{var option} \to \text{bool}       \to             \\
            & \text{var option} \to \text{var option} \to             \\
            & \text{prog}                                             \\
  \midrule
  True      & bool                                                    \\
  False     & bool                                                    \\
  \midrule
  Nothing   & \text{var option}                                       \\
  Just      & \text{var}        \to \text{var option}                 \\
  \midrule
  Unit      & var                                                     \\
  Name      & var                                                     \\
  Next      & \text{var}        \to \text{var}                        \\
  Prev      & \text{var}        \to \text{var}                        \\
  Double    & \text{var}        \to \text{var}                        \\
  Half      & \text{var}        \to \text{var}                        \\
  Opposite  & \text{var}        \to \text{var}                        \\
  \bottomrule
\end{tabular}
\end{center}

In this context programs to be learned correspond to a single shape since they
don't take an input. All tasks will ignore input and produces a single output.
This differs from the previous examples where each target could be evaluated on
various input.

Some elements of the semantics are common, the others are as follow.
\texttt{Repeat} takes a \texttt{variable} and a \texttt{prog} and repeats
said \texttt{prog} $n$ times where $n$ is the evaluation of the
\texttt{variable} --- if not set it is defaulted to two. \texttt{Embed} of a
\texttt{prog} means that said \texttt{prog} will be executed and then returns to
the current state --- leaving what has been drawn in the meantime on the canvas.

\begin{wrapfigure}{r}{0.25\textwidth}
  \begin{center}
    \includegraphics[width = 0.25\textwidth]{figures/DSLGeom.eps}
  \end{center}\label{dslgeom}\caption{Top: Some defaults for \texttt{Integrate}.
  Bottom, the result of the \texttt{Cross} example bellow.}
\end{wrapfigure}

\texttt{Integrate} is the main instruction and the only one that draws anything:
\texttt{Integrate(t,p,a,c)} takes a time \texttt{var}, a pen \texttt{bool}, an
acceleration \texttt{var} and an angular speed \texttt{var}, and it moves the
turtle according to these parameters --- with or without actually drawing
depending on the pen variable \texttt{p}.

The default values of these parameters are set such that \texttt{Integrate(nothing,
true, nothing, nothing)} draws a unit segment, \texttt{Integrate(nothing, true,
nothing, Just(Unit))} draws a circle of unit length, and
\texttt{Integrate(nothing, true, Just(Unit), Just(unit))} draws the first spire
of a spiral. Playing with the first arguments decides on the duration during
which the arguments are integrated.

In the \texttt{var} type, most are self explicit, while \texttt{Opposite(v)}
takes the opposite of \texttt{v}. The behaviour of \texttt{Name} was originally
designed to handle arbitrary variables in a call by name fashion but was latter
reduced to a single storage location, which proves to be enough for the
targeted shapes. One can therefore store elements in that placeholder using
\texttt{Define} and retrieve it through \texttt{Name}.

Because of these defaults, the following program draws a cross:

\[
  \text{\texttt{Cross = Repeat(Double(Double(Unit)), Concat(Embed(Integrate), Turn(None)))}}
\]

Describing highly regular complex shapes in this language is easy to do as a
human but quickly escapes the reach of naÃ¯ve enumeration search. By using a
curriculum of shapes our approach compresses the search in the corresponding
directions --- another way to say this is that upon being given a dataset of
shapes, it picks up the simple ones and abstract them as building blocks for
latter staged of search.

For example, the simplest way to draw a square in this language is already of
length 12. Placing several around, for example to draw a grid is out of reach of
the initial search. However by first abstracting as a primitive the segment ---
thus reducing the length by four ---, then assuming that after a segment it
often needs to draw something else, then abstracting the first arguments of
\texttt{Repeat} --- further reducing the length by two --- as well as the one of
\texttt{Turn} and finally making the square a primitive on its own once it
starts using it often enough, the length of this particular shape drops.

\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{rcl}
      \toprule
      New Primitive & Type & Definition \\
      \midrule
      Segment = $f_0$ & \texttt{prog} & \texttt{Concat(Nothing, True, Nothing, Nothing)} \\
      Right-angle = $f_1$ & \texttt{prog} & \texttt{Turn(Nothing)} \\
      RepeatTwice = $f_2$ & \texttt{prog} $\to$ \texttt{prog} & $\lambda p.$ \texttt{Repeat(nothing,} $p$) \\
      AddToUnitSegment = $f_3$ & \texttt{prog} $\to$ \texttt{prog} & $\lambda p.$ \texttt{Concat(}$f_0, p$) \\
      Square = $f_4$ & \texttt{prog} & $f_3(f_3(f_2(f_1)))$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{How our method compresses the square step by step. On the example
  given in the main article this was produced by the compressor after the second
  search phase and leads to the second jump in success, the first one being the
  abstraction of the Segment. Names are \emph{not} produced by the compressor
  and are here as indication to help the reader.}
\end{table}

The underlying hypothesis is that the new primitive distort the space of search
toward something that looks more like what human actually produce and moves away
from semantically valid but meaningless programs --- in a sense, learns to care
about what \emph{matters} rather than what is \emph{true} in a very
pragmatic-like way.

In Figure \ref{alltasksgeom} are listed all the shapes used for this project
without particular order.

\begin{figure}[h]
\centering
\includegraphics[width = \textwidth]{figures/geomAllTasks.png}
\caption{The set of tasks for the geometry domain}
\label{alltasksgeom}
\end{figure}

Since this is a first step in broader project of program induction for abstract
geometry the likelyhood is currently all-or-none --- ongoing work moves this to
a neural net based distance function to abstract away from noise in the shapes.

\newpage

\section{An Illustration of the 3 iterations of our algorithm}
Below we diagram the iterations employed by our algorithm. At each stage of the algorithm,
we have shaded the observed variables in gray and left the unobserved variables white.
Black lines correspond to a connection from the top-down
generative model, while red lines correspond to connections from the bottom-up recognition model.

\begin{figure}[h]
\centering
  \includegraphics{figures/iterations.eps}
%\begin{tikzpicture}
  %\begin{scope}[shift={(2.5,0)}]
    %\node[obs] at (3,3) (dx){$\mathcal{D}$};
    %\node[latent] at (3.5,1.75) (zp){$p$};
    %\node[obs] at (4,3) (tx){$\theta^{(x)}$};
    %\node[obs] at (3.5,0.7) (xp) {$x$};
    %\node[align = center] at ([yshift = -0.6cm,xshift = 0.5cm]xp.south) {Search: Infer $p$};
    %\draw [->,red] (xp.east) to[out = 0,in = 0] node(nn){} (tx.east);
    %\draw [->,red] (tx) -- (zp);
    %\draw [->] (dx) -- (zp);
    %\node at (nn) {
      %\begin{tikzpicture}[x=2.5cm,y=1.25cm,transform canvas={scale=0.2,shift={+(-1,2.5)}}]
        %\tikzstyle{neuron}=[circle,fill=blue!50,minimum size=20pt]
        %\fill[fill=white] (-0.25,-0.5) rectangle (2.25,-4.5);
        %\node[rectangle] at (1,1) {};
        %\foreach \name / \y in {1,...,4}
            %\node[neuron] (I-\name) at (0,-\y) {};
        %\foreach \name / \y in {1,...,3}
            %\node[neuron] (H-\name) at (1,-\y-0.5) {};
        %\foreach \name / \y in {1,...,4}
            %\node[neuron] (O-\name) at (2,-\y) {};
        %\foreach \source in {1,...,4}
            %\foreach \dest in {1,...,3}
                %\draw [-latex] (I-\source) -- (H-\dest);
        %\foreach \source in {1,...,3}
            %\foreach \dest in {1,...,4}
                %\draw [-latex] (H-\source) -- (O-\dest);
      %\end{tikzpicture}
    %};
    %\node[shift={+(0,-0.65)}] at (nn) [fill=white]{ $q(x)$ };
  %\end{scope}

  %\begin{scope}[shift={(0.7,-4.2)}]
    %\node[obs] at (3,3) (dt){$\mathcal{D},\theta$};
    %\node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]dt.south) (p2){$p_2$};
    %\node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p2.south) (x2){$x_2$};
    %\draw [->] (p2.south) -- (x2.north);
    %\draw [->] (dt.south) -- (p2.north);


    %\node[obs] at ([yshift = 0cm,xshift = -0.5cm]p2.west) (p1){$p_1$};
    %\node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p1.south) (x1){$x_1$};
    %\draw [->] (p1.south) -- (x1.north);
    %\draw [->] (dt.south) -- (p1.north);

    %\node[obs] at ([yshift = 0cm,xshift = 0.5cm]p2.east) (p3){$p_3$};
    %\node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p3.south) (x3){$x_3$};
    %\draw [->] (p3.south) -- (x3.north);
    %\draw [->] (dt.south) -- (p3.north);

    %\node at ([yshift = 0cm,xshift = 0.3cm]p3.east) {$\cdots $};
    %\node at ([yshift = 0cm,xshift = 0.3cm]x3.east) {$\cdots $};
      
    %\node[obs] at ([yshift = 0cm,xshift = 1.3cm]p3.east) (zp){$p$};
    %\node[obs] at ([yshift = 0cm,xshift = 1.3cm]x3.east) (xp) {$x$};
    %\draw [->] (zp.south) -- (xp.north);
    %\plate {}{(zp)(xp)}{$x\in X$};

    %\draw[dashed,cyan,very thick] ([yshift = 0.5cm,xshift = -2]p1.west)
    %rectangle  ([yshift = -0.45cm,xshift = +2]xp.east);
    %\node[align = center] at ([yshift = -1cm,xshift = 0.6cm]x2.south) {Compile: Train $q$\\training data: cyan $(x,p)$};
  %\end{scope}
    
  %\begin{scope}[shift={(7.7,-4)}]
    %\node[latent] at (0.5,3) (d){$\mathcal{D}$};
    %\node[latent] at (1.5,3) (t){$\theta$};
    %\node[obs] at (1,1.75) (z){$p$};
     %\node[latent] at (3.5,1.5) (tx){$\theta^{(x)}$};
    %\node[obs] at (1,0.5) (x) {$x$};
    %\edge {z}{x};
    %\edge {d,t}{z};
    %\plate {}{(z)(x)}{$x\in X$};
    %\node[align = center] at ([yshift = -1cm]x.south) {Compress: Induce $(\mathcal{D},\theta)$};
  %\end{scope}

%\end{tikzpicture}
\end{figure}

\section{Generative model over the programs}

Alg.~\ref{programGenerativeModel} is a procedure for drawing
samples from the generative model $(\mathcal{D},\theta)$.  In practice, we
enumerate programs in order of their probability under  Alg.~\ref{programGenerativeModel} rather than sample them.

\begin{figure}
  \centering
  \begin{minipage}{0.5\textwidth}    
    \begin{algorithm}[H]
       \caption{Generative model over programs}
       \label{programGenerativeModel}
       \begin{algorithmic}
    %%      \STATE \textbf{function} sampleProgramFromDSL$(\mathcal{D}, \theta, \tau)$:
    %%   \STATE {\bfseries Input:} DSL $\mathcal{D}$, weight vector $\theta$, type $\tau$
    %%   \STATE \textbf{Output:} a program whose type unifies with $\tau$
    %%   \STATE \textbf{return} sample$(\mathcal{D}, \theta, \varnothing, \tau)$
    %% \STATE
         \STATE \textbf{function} sample$(\mathcal{D}, \theta, \mathcal{E}, \tau)$:
      \STATE {\bfseries Input:} DSL $(\mathcal{D},\theta)$, environment $\mathcal{E}$, type $\tau$
      \STATE \textbf{Output:} a program whose type unifies with $\tau$
      \IF{$\tau = \alpha\to\beta$}
      \STATE var $\gets$ an unused variable name
      \STATE body $\sim$ sample$(\mathcal{D},\theta,\{\text{var}:\alpha\}\cup\mathcal{E},\beta)$
       \STATE \textbf{return} \code{(lambda (}var\code{) }body\code{)}
       \ENDIF
       %   \ELSE
       \STATE $\text{primitives} \gets\{p | p: \tau' \in \mathcal{D}\cup\mathcal{E}$ \\
          \hspace*{6.9em}$\text{if }\tau\text{ can unify with yield}(\tau') \} $
       
       \STATE Draw $e\sim \text{primitives}$, w.p. $\propto\theta_e$ if $e\in \mathcal{D}$ \\
          \hspace*{8.8em}w.p. $\propto\frac{\theta_{var}}{|\text{variables}|}$ if $e\in \mathcal{E}$
       \STATE Unify $\tau$ with yield$(\tau')$.
       \STATE $\left\{\alpha_k \right\}_{k = 1}^K\gets\text{args}(\tau')$ 
    %   \STATE unify$(\tau,\beta)$
       \FOR{$k=1$ {\bfseries to} $K$}
     \STATE $a_k\sim\text{sample}(\mathcal{D},\theta,\mathcal{E},\alpha_k)$
     \ENDFOR
     \STATE \textbf{return} \code{(}$e\;a_1\; a_2\; \cdots\; a_K$\code{)}
     \STATE\textbf{where:}
     \STATE yield$(\tau) = \begin{cases}
       \text{yield}(\beta)   &\text{ if }\tau = \alpha\to \beta\\
       \tau   &\text{ otherwise.}
     \end{cases}$ 
     \STATE  args$(\tau) = \begin{cases}
       [\alpha] + \text{args}(\beta)   &\text{ if }\tau = \alpha\to \beta\\
       []   &\text{ otherwise.}
     \end{cases}$
    \end{algorithmic}
    \end{algorithm}
  \end{minipage}
\end{figure}

\section{Program Representation}\label{programrepresentation}
We choose to represent programs using $\lambda$-calculus~\cite{pierce}.
A $\lambda$-calculus expression is either:
\begin{itemize}
  \item[--] A \emph{primitive}, like the number 5 or the function \texttt{sum}.
  \item[--] A \emph{variable}, like $x$, $y$, or $z$.
  \item[--] A $\lambda$\emph{-abstraction}, which creates a new function.  $\lambda$-abstractions have a variable and a body. The body is a $\lambda$-calculus expression. Abstractions are written as $\lambda \text{var}. \text{body}$ or in Lisp syntax as \mbox{\texttt{(lambda (\textrm{var}) \textrm{body})}}.
  \item[--] An \emph{application} of a function to an argument. Both the function and the argument are $\lambda$-calculus expressions. The application of the function $f$ to the argument $x$ is written as $f\; x$ or as \texttt{($f$ x)}.
\end{itemize}

For example, the function which squares the logarithm of a number is
$\lambda x.\text{\tt(square (log $x$))}$, and the identity function $f(x) = x$ is $\lambda x.x$. The
$\lambda$-calculus serves as a spartan but expressive Turing complete
program representation, and distills the essential features of functional
programming languages like Lisp.

However, many $\lambda$-calculus expressions correspond to ill-typed programs, such as the program that takes the logarithm of the Boolean \texttt{true} (i.e., \texttt{log true}) or which applies the number five to the identity function
(i.e., $5 \; (\lambda x.x)$).
We use a well-established typing system for $\lambda$-calculus called \emph{Hindley-Milner typing}~\cite{pierce}, which is used in programming languages like OCaml.
The purpose of the typing system is to ensure that our programs never call a function with a type it is not expecting (like trying to take the logarithm of \texttt{true}).
Hindley-Milner has two important features:
Feature 1: It supports \emph{parametric polymorphism}, meaning that types can have variables in them, called \emph{type variables}. Lowercase Greek letters are conventionally used for  type variables.
For example, the type of the identity function is $\alpha\to\alpha$, meaning it takes something of type $\alpha$ and return something of type $\alpha$. A function that returns the first element of a list has the type $[\alpha]\to\alpha$. Type variables are not the same as variables introduced by $\lambda$-abstractions.
Feature 2: Remarkably, there is a  simple algorithm for automatically inferring the polymorphic Hindley-Milner type of a $\lambda$-calculus expression~\cite{damas1982principal}.
Our generative model over programs performs Hindley-Milner type inference during sampling:
\emph{Unify} in the generative model uses the machinery of Hindley-Milner to
ensure that the generated programs have valid polymorphic types.
A satisfactory exposition of Hindley-Milner is beyond the scope of this paper,
but~\cite{pierce} offers a nice overview of lambda calculus and typing systems like Hindley-Milner.

%% \begin{figure}
%%   \begin{align}
%%     \text{primitive types} & 
%%     \end{align}
%%   \end{figure}

\section{Neural Recognition Model Architecture}

The neural recognition model regresses from an observation (set of input/output pairs: $\left\{(i_n,o_n) \right\}_{n\leq N}$) to a $|\mathcal{D}| + 1$ dimensional vector. Each input/output pair is processed by an identical encoder network;
the outputs of the encoders are average and passed to an MLP with 1 hidden layer, 32 hidden units, and a ReLU activation:
\begin{align}
  q(x) = \text{MLP}\left(\text{Average}\left(\left\{\text{encoder}\left(i_n,o_n \right) \right\}_{n\leq N} \right) \right)
\end{align}



For the string editing and list domains,
the inputs and outputs are sequences. Our encoder for these domains is a bidirectional GRU with 64 hidden units that reads each input/output pair; we concatenate the input and output along with a special delimiter
symbol between them.
We MaxPool the final hidden unit activations in the GRU along both passes of the bidirectional GRU.

For symbolic regression,
the input/outputs are densely sampled points along the curve of the function.
We rendered these points to a graph,
and pass the image of the graph to a convolutional network,
which acts as the encoder.

\section{DSL Induction}

\subsection{Structure Learning}

We use Alg.~\ref{grammarInductionAlgorithm} to search for the structure of the DSL that best explains the frontiers.

\setcounter{algorithm}{2} % there are two algorithms in the main paper.
\begin{algorithm}[hb]
   \caption{DSL Induction Algorithm}
   \label{grammarInductionAlgorithm}
   \begin{algorithmic}
     \STATE {\bfseries Input:} Set of frontiers $\{\mathcal{F}_x\}$
     \STATE \textbf{Hyperparameters:} Pseudocounts $\alpha$, regularization parameter $\lambda$
     \STATE \textbf{Output:} DSL $\mathcal{D}$, weight vector $\theta$
     \STATE Define $L(\mathcal{D},\theta) =  \prod_x \sum_{p\in \mathcal{F}_x} \probability[p|\mathcal{D},\theta]$
     \STATE Define $\theta^*(\mathcal{D}) = \argmax_\theta \text{Dir}(\theta|\alpha) L(\mathcal{D},\theta)$
     \STATE Define $\text{score}(\mathcal{D}) = \log \probability[\mathcal{D}] + L(\mathcal{D},\theta^*) - \|\theta\|_0$
     \STATE $\mathcal{D}\gets$ every primitive in $\{\mathcal{F}_x\}$
     \WHILE {true}
     \STATE N $\gets \{\mathcal{D}\cup \{s\} | x\in X, p\in \mathcal{F}_x, s\text{ a fragment  of }p\}$
     \STATE $\mathcal{D}'\gets \argmax_{\mathcal{D}'\in N}\text{score}(\mathcal{D}') $
     \STATE \textbf{if }$\text{score}(\mathcal{D}') < \text{score}(\mathcal{D})$\textbf{ return }$\mathcal{D},\theta^*(\mathcal{D})$
     \STATE $\mathcal{D}\gets\mathcal{D}'$
     \ENDWHILE
   \end{algorithmic}
\end{algorithm}

\subsection{Estimating $\theta$}

We use an EM algorithm to estimate the continuous parameters of the DSL, e.g. $\theta$.
Suppressing dependencies on $\mathcal{D}$, the EM updates are
\begin{align}
\label{maximizeStep}  \theta& = \argmax_\theta \log P(\theta) + \sum_x \expect_{Q_x}\left[\log \probability\left[p|\theta \right] \right]\\
  Q_x(p)&\propto \probability[x|p]\probability[p|\theta]
  \end{align}
In the M step of EM we will update $\theta$ by instead maximizing a lower bound on $\log \probability[p|\theta]$,
making our approach an instance of Generalized EM.

We write $c(e,p)$ to mean the number of times that primitive $e$ was used in program $p$; $c(p)= \sum_{e\in \mathcal{D}}c(e,p)$ to mean the total number of primitives used in program $p$; $R(p)$ to mean the sequence of types input to sample in Alg.~1 of the main paper. Jensen's inequality gives a lower bound on the likelihood:
\begin{align*}
  &\sum_x\expect_{Q_x}\left[  \log \probability[p|\theta] \right] =\\
  &\sum_{e\in \mathcal{D}} \log \theta_e \sum_x\expect\left[c(e,p_x) \right] -
  \sum_\tau\expect\left[\sum_x c(\tau,p_x)  \right]\log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
 =   &\sum_e C(e)\log \theta_e  - \beta\sum_\tau\frac{\expect\left[\sum_x c(\tau,p_x)  \right]}{\beta}\log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
 \geq    &\sum_e C(e)\log \theta_e  - \beta\log \sum_\tau\frac{\expect\left[\sum_x c(\tau,p_x)  \right]}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
     =     &\sum_e C(e)\log \theta_e  - \beta\log \sum_\tau\frac{R(\tau)}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  
  %% &\geq\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \frac{1}{c(p)}\sum_{\tau\in R(p)} \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  %% & = \sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log\frac{1}{c(p)} \sum_{e\in \mathcal{D}} r(e,p)\theta_e
\end{align*}
where we have defined
\begin{align*}
  C(e)&\triangleq  \sum_x\expect\left[c(e,p_x) \right]\\
  R(\tau)&\triangleq \expect\left[\sum_x c(\tau,p_x)  \right]\\
  \beta&\triangleq\sum_\tau \expect\left[\sum_x c(\tau,p_x)  \right]
\end{align*}
Crucially it was defining $\beta$ that let us use Jensen's inequality. 
Recalling from the main paper that $P(\theta)\triangleq\text{Dir}(\alpha)$,
we have the following lower bound on M-step objective:
\begin{align}
\sum_e (C(e) + \alpha)\log \theta_e  - \beta\log \sum_\tau\frac{R(\tau)}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e    
\end{align}
Differentiate with respect to $\theta_e$, where $e:\tau$, and set to zero to obtain:
\begin{align}
  &  \frac{C(e) + \alpha}{\theta_e}\propto\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')\\
&  \theta_e\propto\frac{C(e) + \alpha}{\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')}
\end{align}
The above is our estimator for $\theta_e$.
Despite the convoluted derivation, the above estimator has an intuitive interpretation.
The quantity $C(e)$ is the expected number of times that we used $e$.
The quantity $\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')$
is the expected number of times that we \emph{could have} used $e$.
The hyperparameter $\alpha$ acts as pseudocounts that are
added to the number of times that we used each primitive,
and are not added to the number of times that we could have used each primitive.


We are only maximizing a lower bound on the log posterior; when is this lower bound tight?
This lower bound is tight whenever all
of the types of the expressions in the DSL are not polymorphic, in which case our DSL is equivalent to a PCFG
and this estimator is equivalent to the inside/outside algorithm.
Polymorphism introduces context-sensitivity to the DSL,
and exactly maximizing the likelihood with respect to $\theta$
becomes intractable,
so for domains with polymorphic types we use this estimator.

\begin{comment}
\section{Initial Domain-Specific Languages}
\subsection{Boolean Circuits}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    nand & $\text{bool}\to \text{bool}\to \text{bool}$ \\  \hline
\end{tabular}
\end{center}

\subsection{Symbolic Regression}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  + & $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$ \\
  *&  $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$\\
  real&$\mathbb{R}$\\\hline
\end{tabular}
\end{center}


\subsection{String Editing}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  0&$\mathbb{Z}$\\
  +1&$\mathbb{Z}\to \mathbb{Z}$\\
  -1&$\mathbb{Z}\to \mathbb{Z}$\\
  split&$\text{char}\to \text{string}\to \left[\text{string} \right]$\\
  join&$\text{string}\to \left[\text{string} \right]\to \text{string}$\\
  map&$(\text{string}\to \text{string})$\\
  &$\qquad\quad\to\left[\text{string} \right]\to \left[\text{string} \right]$\\
  nth&$\mathbb{Z}\to \left[\text{string} \right]\to \text{string}$\\
  slice&$\mathbb{Z}\to \mathbb{Z}\to \text{string}\to \text{string}$\\
  length&$\text{string}\to \mathbb{Z}$\\
  ""&string\\
  ","\quad" "&char\\
  "<"\quad">" &char \\
  chr->str&$\text{char}\to \text{string}$\\
  trim&$\text{string}\to \text{string}$\\
  upper&$\text{string}\to \text{string}$\\
  lower&$\text{string}\to \text{string}$\\
  capitalize&$\text{string}\to \text{string}$\\\hline
\end{tabular}
\end{center}


\vfill


\subsection{List Functions}
The lower section with seven primitives designates those given to the
\emph{rich} DSL variant and not the \emph{base} variant. The rich variant is
not supplied the \texttt{if} primitive, because its primary intention is to
enable the learning of the lower five concepts.
Greek letters represent polymorphic types.
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    empty & $\left[\alpha\right]$ \\
    singleton & $\alpha \to \left[\alpha\right]$ \\
    concat & $\left[\alpha\right] \to \left[\alpha\right] \to \left[\alpha\right]$ \\
    mapi & $(\mathbb{Z} \to \alpha \to \beta)$ \\
    & \qquad\qquad$\to \left[\alpha\right] \to \left[\beta\right]$ \\
    reducei & $(\beta \to \mathbb{Z} \to \alpha \to \beta)$\\
    & \qquad\qquad$\to \beta \to \left[\alpha\right] \to \beta$ \\
    \hline
    if & bool $\to \alpha \to \alpha \to \alpha$ \\
    \hline
    true & bool \\
    not & bool $\to$ bool \\
%UNUSED+UNNECESSARY:    and & bool $\to$ bool $\to$ bool \\
%UNUSED+UNNECESSARY:    or & bool $\to$ bool $\to$ bool \\
    0 $\cdots$ 5 & $\mathbb{Z}$ \\
    + & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    * & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    negate & $\mathbb{Z} \to \mathbb{Z}$ \\
    mod & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    eq? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    gt? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    is-prime & $\mathbb{Z} \to$ bool \\
    is-square & $\mathbb{Z} \to$ bool \\
    range & $\mathbb{Z} \to$ [$\mathbb{Z}$] \\
    sort & $\left[\mathbb{Z}\right] \to \left[\mathbb{Z}\right]$ \\
    \hline
    sum & $\left[\mathbb{Z}\right] \to \mathbb{Z}$ \\
    reverse & $\left[\alpha\right] \to \left[\alpha\right]$ \\
    all & $(\alpha \to$ bool$) \to \left[\alpha\right] \to$ bool \\
    any & $(\alpha \to$ bool$) \to \left[\alpha\right] \to$ bool \\
    index & $\mathbb{Z} \to \left[\alpha\right] \to \alpha$ \\
    filter & $(\alpha \to$ bool$) \to \left[\alpha\right] \to \left[\alpha\right]$ \\
    slice & $\mathbb{Z} \to \mathbb{Z} \to \left[\alpha\right] \to \left[\alpha\right]$ \\
  \hline
\end{tabular}
\end{center}
  \end{comment}

\section{Hyperparameters \& Implementation Details}
We set structure penalty $\lambda = 1$ (Eq.~5 of the main paper) and
smoothness parameter $\alpha = 10$ (Eq.~6 of the main paper)
for all experiments.
For list processing and text editing we used a search timeout of two hours;
because the symbolic regression problems are easier,
we used a timeout of only five minutes for these.


Because the frontiers can become very large in later iterations of the algorithm,
we only keep around the top $10^4$ programs in the frontier $\mathcal{F}_x$ as measured by $\probability[x,p|\mathcal{D},\theta]$.




\section{Why not the ELBO Bound?}

Our lower bound $\lowerBound$ is unconventional,
and one might wonder why we do not instead maximize an ELBO-style bound like in a VAE or in the EM algorithm.
Surprisingly, maximizing an ELBO-style bound
leads to a pathological behavior that  causes the model to easily become trapped in local optima.


If we were to maximize the ELBO bound to perform inference in
our generative model,
then, during DSL induction, we would seek a new $(\mathcal{D}^*,\theta^*)$ maximizing the following lower bound on the likelihood (along with an unimportant regularizing term on the DSL):
\begin{align}
  &  \sum_{x\in X}\expect_{p\sim Q_x}\left[\log \probability[p|\mathcal{D}^*,\theta^*] \right] \label{EM}\\
  & Q_x(p)\triangleq \probability[p|x,\mathcal{D},\theta]
\end{align}
where $(\mathcal{D},\theta)$ is our current estimate of the generative
model.  These equations fall out of an EM-style derivation, and one
could replace $Q_x(p)$ with the recognition model $q(p|x)$, either
using importance sampling (so the expectation in Eq.~\ref{EM} is taken
over $q$ and we reweigh using $Q_x$) or by directly using $q$ as our approximate posterior over the program that solves task $x$.

We do not maximize a bound of this form because it
takes an expectation over the \emph{previous} iteration's
posterior over programs,
so the approximate posterior $Q_x$ at the next iteration
ends up being very close to previous approximate posterior.
Intuitively,
we want the DSL induction to be a function \emph{only} of the programs that we have found,
and \emph{not} be a function of how the previous generative model weighed them.
In practice,
we found that maximizing EM-style bounds, like the ELBO,
leads to a kind of hysteresis effect,
where the next generative model too closely matches the previous one,
causing the algorithm to easily become trapped in local optima.

\begin{comment}
b
\section{Example String Editing Tasks}



We automatically generated 600 string editing problems split 75/25 train/test.
The following are representative problems:


\begin{center}
\begin{tabular}{|l|l|}
  \hline Input&Output\\
  \hline\multicolumn{2}{|c|}{Double}\\
  \hline
vlxS & vlxSvlxS \\
 JkVC & JkVCJkVC \\
 dbxD & dbxDdbxD \\
 gQgB & gQgBgQgB \\
  \hline\multicolumn{2}{|c|}{Extract string delimited by ',' (inclusive),' '}\\
  \hline
LsR,uhMZI wIL & ,uhMZI  \\
 tMY,KdF hhT & ,KdF  \\
 UUPdC,UAd kCgtq & ,UAd  \\
 gQgGN,VMJ FjrXZ & ,VMJ \\
  \hline\multicolumn{2}{|c|}{Apply first 2 characters to input delimited by '.'}\\
  \hline
OhCQN.DlH.LdkJ.pLh & OhDlLdpL \\
 CZr.mveZr.dvjP.YMJpg & CZmvdvYM \\
 bpub.VkkZ.dYdPr & bpVkdY \\
 zjNA.wFwY.WMMqh.OFdbr & zjwFWMOF \\
  \hline\multicolumn{2}{|c|}{Apply first 2 characters to string delimited by ' ','.'}\\
  \hline
fNA YWyL.JDD & YW \\
 YWozt TPW.nczz & TP \\
 RJwiT dpW.uFU & dp \\
 FTp gNMo.JsZB & gN \\
  \hline\multicolumn{2}{|c|}{Apply strip to string delimited by ',','.'}\\
  \hline
iNEP,  GigW .Xlm & GigW \\
 xhpH,  QvwnW .mfK & QvwnW \\
 weWIC,IQjhWa .fGKz & IQjhWa \\
 xZUm, FpdE.ETcr & FpdE \\
  \hline\multicolumn{2}{|c|}{Drop first character}\\
  \hline
WYPvm & YPvm \\
 terHk & erHk \\
 asWM & sWM \\
 tBdT & BdT \\
\hline
\end{tabular}
\end{center}
  \end{comment}
\vfill

\section{List Processing Data Set}
Each list processing tasks we created is in described in
Tbl~\ref{listdataset}.

\begin{table}[h]
  \centering\footnotesize
  \begin{tabular}{|l|l|}\hline
    add-k for k $\in\{0..5\}$&
kth-largest for k $\in\{1..5\}$\\
    append-index-k for k $\in\{1..5\}$&
kth-smallest for k $\in\{1..5\}$\\
    append-k for k $\in\{0..5\}$&
last\\
    bool-identify-geq-k for k $\in\{0..5\}$&
len\\
    bool-identify-is-mod-k for k $\in\{1..5\}$&
max\\
    bool-identify-is-prime&
min\\
    bool-identify-k for k $\in\{0..5\}$&
modulo-k for k $\in\{1..5\}$\\
    caesar-cipher-k-modulo-n&
mult-k for k $\in\{0..5\}$\\
    \qquad for k $\in\{0..5\}$ and n $\in\{1..5\}$&
odds\\
    count-head-in-tail&
pop\\
    count-k for k $\in\{0..5\}$&
pow-k for k $\in\{1..5\}$\\
    drop-k for k $\in\{0..5\}$&
prepend-index-k for k $\in\{1..5\}$\\
    dup&
prepend-k for k $\in\{0..5\}$\\
    empty&
product\\
    evens&
range\\
    fibonacci&
remove-empty-lists\\
    has-head-in-tail&
remove-eq-k for k $\in\{0..3\}$\\
    has-k for k $\in\{0..5\}$&
remove-gt-k for k $\in\{0..3\}$\\
    head&
remove-index-k for k $\in\{1..5\}$\\
    index-head&
remove-mod-head\\
    index-k for k $\in\{1..5\}$&
remove-mod-k for k $\in\{2..5\}$\\
    is-evens&
repeat\\
    is-mod-k for k $\in\{1..5\}$&
repeat-k for k $\in\{1..5\}$\\
    is-odds&
repeat-many\\
    is-primes&
replace-all-with-index-k for k $\in\{1..5\}$\\
    is-squares&
reverse\\
    keep-eq-k for k $\in\{0..3\}$&
rotate-k for k $\in\{1..5\}$\\
    keep-gt-k for k $\in\{0..3\}$&
slice-k-n for k $\in\{1..5\}$ and n $\in\{1..5\}$\\
    keep-mod-head&
sort\\
    keep-mod-k for k $\in\{1..5\}$&
sum\\
    keep-primes&
tail\\
    keep-squares&
take-k for k $\in\{1..5\}$\\
\hline
  \end{tabular}
  \normalsize
  ~\\~\\\caption{Our list processing data set}\label{listdataset}
\end{table}

\section{Learned DSLs}

Here we present representative DSLs learned by our model. DSL primitives
discovered by the algorithm are prefixed with \lstinline!#!.
Variables are prefixed with \lstinline!$!, and we adopt De Bruijn indices to
model bound variables~\cite{pierce}.

\onecolumn

\lstset{
  basicstyle=\footnotesize,
  escapechar=@,
  breaklines=true,
  breakatwhitespace=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}

\subsection{List processing}
\begin{lstlisting}
#(+ 1 1)
#(@$\lambda$@ (cdr (cdr $0)))
#(@$\lambda$@ (foldr $0 1 (@$\lambda$@ (@$\lambda$@ (* $0 $1)))))
#(@$\lambda$@ (cons (car $0) nil))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0))))))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 (is-nil $0) (@$\lambda$@ (@$\lambda$@ (if $0 $0 (eq? $3 $1)))))))
#(@$\lambda$@ (map (@$\lambda$@ (eq? $1 $0))))
#(@$\lambda$@ (* $0 (* $0 $0)))
#(+ 1 #(+ 1 1))
#(@$\lambda$@ (map (@$\lambda$@ (gt? $0 $1))))
#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (if (is-square $1) (cons $1 $0) $0)))))
#(@$\lambda$@ (map (@$\lambda$@ (eq? $0 (length (range $0)))) $0))
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $1)) (range $1))))
#(@$\lambda$@ (cdr (#(@$\lambda$@ (cdr (cdr $0))) $0)))
#(@$\lambda$@ (map (@$\lambda$@ (index 1 $1))))
#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (cons $1 (cons $1 $0))))))
#(@$\lambda$@ (@$\lambda$@ (cons (car $0) $1)))
#(+ 1 #(+ 1 #(+ 1 1)))
#(@$\lambda$@ (map (@$\lambda$@ (gt? 1 (mod $0 $1)))))
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (mod (+ $0 $1) $2)))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $0 $0) $0))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (cons $0 nil) $1)))
#(+ #(+ 1 #(+ 1 1)) #(+ 1 1))
#(@$\lambda$@ (map (@$\lambda$@ (+ #(+ 1 #(+ 1 1)) (+ $1 $0)))))
#(@$\lambda$@ (map (@$\lambda$@ (+ $0 $1))))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 (is-nil $0) (@$\lambda$@ (@$\lambda$@ (gt? $1 (#(@$\lambda$@ (* $0 (* $0 $0))) $3)))))))
#(@$\lambda$@ (foldr $0 0 (@$\lambda$@ (@$\lambda$@ (+ $0 (#(@$\lambda$@ (foldr $0 1 (@$\lambda$@ (@$\lambda$@ (* $0 $1))))) (range $1)))))))
#(@$\lambda$@ (#(@$\lambda$@ (cdr (#(@$\lambda$@ (cdr (cdr $0))) $0))) (cdr $0)))
#(@$\lambda$@ (#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (if (is-square $1) (cons $1 $0) $0))))) (map (@$\lambda$@ (* $0 (+ $0 $0))) $0)))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (#(@$\lambda$@ (cons (car $0) nil)) $0) $1)))
#(@$\lambda$@ (@$\lambda$@ (length (#(@$\lambda$@ (#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (if (is-square $1) (cons $1 $0) $0))))) (map (@$\lambda$@ (* $0 (+ $0 $0))) $0))) (map (@$\lambda$@ (- $1 $0)) $1)))))
#(@$\lambda$@ (@$\lambda$@ (is-square (#(@$\lambda$@ (foldr $0 1 (@$\lambda$@ (@$\lambda$@ (* $0 $1))))) (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (mod (+ $0 $1) $2))))) (length (#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $0 $0) $0)) $0)) $1 $0)))))
#(@$\lambda$@ (@$\lambda$@ (foldr (cdr $0) $1 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (#(@$\lambda$@ (cons (car $0) nil)) $0) $1))) (cdr $0) $0))))))
#(@$\lambda$@ (is-nil (#(@$\lambda$@ (#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (if (is-square $1) (cons $1 $0) $0))))) (map (@$\lambda$@ (* $0 (+ $0 $0))) $0))) (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (mod (+ $0 $1) $2))))) #(+ 1 1) 1 $0))))
#(@$\lambda$@ (@$\lambda$@ (gt? (#(@$\lambda$@ (@$\lambda$@ (length (#(@$\lambda$@ (#(@$\lambda$@ (foldr $0 nil (@$\lambda$@ (@$\lambda$@ (if (is-square $1) (cons $1 $0) $0))))) (map (@$\lambda$@ (* $0 (+ $0 $0))) $0))) (map (@$\lambda$@ (- $1 $0)) $1))))) $0 $1) 1)))
\end{lstlisting}

\subsection{Text editing}
\begin{lstlisting}
#(@$\lambda$@ (@$\lambda$@ (foldr $0 $0 (@$\lambda$@ (@$\lambda$@ (if (char-eq? $1 $3) nil (cons $1 $0)))))))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0))))))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 $0 (@$\lambda$@ (@$\lambda$@ (cdr (if (char-eq? $1 $3) $2 $0)))))))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $1 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (cons $0 $1))))
#(@$\lambda$@ (@$\lambda$@ (foldr $0 $0 (@$\lambda$@ (@$\lambda$@ (foldr $0 $0 (@$\lambda$@ (@$\lambda$@ (if (char-eq? $1 $5) (cdr $2) $0)))))))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (cons (car $0) (cons $1 $2))))) (#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (cons (car $0) (cons $1 $2))))) nil) '.' $0) '.'))
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (if (char-eq? $1 $0) $2 $0)))))
#(@$\lambda$@ (map (@$\lambda$@ (index $0 $1))))
#(@$\lambda$@ (unfold $0 (@$\lambda$@ (is-nil $0)) (@$\lambda$@ (car $0)) (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (foldr $0 $0 (@$\lambda$@ (@$\lambda$@ (cdr (if (char-eq? $1 $3) $2 $0))))))) SPACE $0))))
#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (cons (car $0) (cons $1 $2))))) nil)
\end{lstlisting}

\subsection{Symbolic regression}
\begin{lstlisting}
#(@$\lambda$@ (/. (/. REAL $0) $0))
#(@$\lambda$@ (+. $0 REAL))
#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. (#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+.  $0 REAL)) (*. $0 REAL))) (*. (#(@$\lambda$@ (+. $0 REAL)) $0) $0))) $0) $0))) $0))))
#(@$\lambda$@ (/. (#(@$\lambda$@ (/. (/. REAL $0) $0)) $0) $0))
#(@$\lambda$@ (@$\lambda$@ (#(/. REAL) (/. (#(@$\lambda$@ (+. $0 REAL)) $0) $1))))
#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (#(@$\lambda$@ (#(/. REAL) (#(@$\lambda$@ (+. $0 REAL)) $0))) $0)))
#(#(@$\lambda$@ (@$\lambda$@ (#(/. REAL) (/. (#(@$\lambda$@ (+. $0 REAL)) $0) $1)))) (#(@$\lambda$@ (/. (#(@$\lambda$@ (/. (/.  REAL $0) $0)) $0) $0)) REAL))
#(@$\lambda$@ (/. (#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL))) (*. (#(@$\lambda$@ (+. $0 REAL)) $0) $0))) $0) (#(@$\lambda$@ (+. $0 REAL)) $0)))
\end{lstlisting}

\bibliography{main}
\bibliographystyle{icml2018}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
