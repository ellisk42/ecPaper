%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\system}{\textsc{DreamCoder}~}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}


\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{listings}
\usepackage{amsthm}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig}
\usepackage{fancyvrb}


\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit,bayesnet}
%\usetikzlibrary{arrows.meta}
%\usetikzlibrary{positioning}
%\usetikzlibrary{decorations.text}
%\usetikzlibrary{decorations.pathmorphing}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{hyperref}
%\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays



% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em E}}
\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}



% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Bootstrapping Domain Specific Languages for Bayesian Program Learning}

\begin{document}

\twocolumn[
\icmltitle{Supplement: Bootstrapping Domain Specific Languages for Bayesian Program Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\section{An illustration of the wake-sleep cycle}
Below we diagram the wake/sleep cycles employed by our algorithm. At each stage of the cycle,
we have shaded the observed variables in gray and left the unobserved variables white.
Black lines correspond to a connection from the top-down
generative model, while red lines correspond to connections from the bottom-up recognition model.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \begin{scope}[shift={(2.5,0)}]
    \node[obs] at (3,3) (dx){$\mathcal{D}$};
    \node[latent] at (3.5,1.75) (zp){$p$};
    \node[obs] at (4,3) (tx){$\theta^{(x)}$};
    \node[obs] at (3.5,0.7) (xp) {$x$};
    \node[align = center] at ([yshift = -0.6cm,xshift = 0.5cm]xp.south) {Wake: Infer $p$};
    \draw [->,red] (xp.east) to[out = 0,in = 0] node(nn){} (tx.east);
    \draw [->,red] (tx) -- (zp);
    \draw [->] (dx) -- (zp);
    \node at (nn) {
      \begin{tikzpicture}[x=2.5cm,y=1.25cm,transform canvas={scale=0.2,shift={+(-1,2.5)}}]
        \tikzstyle{neuron}=[circle,fill=blue!50,minimum size=20pt]
        \fill[fill=white] (-0.25,-0.5) rectangle (2.25,-4.5);
        \node[rectangle] at (1,1) {};
        \foreach \name / \y in {1,...,4}
            \node[neuron] (I-\name) at (0,-\y) {};
        \foreach \name / \y in {1,...,3}
            \node[neuron] (H-\name) at (1,-\y-0.5) {};
        \foreach \name / \y in {1,...,4}
            \node[neuron] (O-\name) at (2,-\y) {};
        \foreach \source in {1,...,4}
            \foreach \dest in {1,...,3}
                \draw [-latex] (I-\source) -- (H-\dest);
        \foreach \source in {1,...,3}
            \foreach \dest in {1,...,4}
                \draw [-latex] (H-\source) -- (O-\dest);
      \end{tikzpicture}
    };
    \node[shift={+(0,-0.65)}] at (nn) [fill=white]{ $q(x)$ };
  \end{scope}

  \begin{scope}[shift={(0.7,-4.2)}]
    \node[obs] at (3,3) (dt){$\mathcal{D},\theta$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]dt.south) (p2){$p_2$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p2.south) (x2){$x_2$};
    \draw [->] (p2.south) -- (x2.north);
    \draw [->] (dt.south) -- (p2.north);


    \node[obs] at ([yshift = 0cm,xshift = -0.5cm]p2.west) (p1){$p_1$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p1.south) (x1){$x_1$};
    \draw [->] (p1.south) -- (x1.north);
    \draw [->] (dt.south) -- (p1.north);

    \node[obs] at ([yshift = 0cm,xshift = 0.5cm]p2.east) (p3){$p_3$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p3.south) (x3){$x_3$};
    \draw [->] (p3.south) -- (x3.north);
    \draw [->] (dt.south) -- (p3.north);

    \node at ([yshift = 0cm,xshift = 0.3cm]p3.east) {$\cdots $};
    \node at ([yshift = 0cm,xshift = 0.3cm]x3.east) {$\cdots $};
      
    \node[obs] at ([yshift = 0cm,xshift = 1.3cm]p3.east) (zp){$p$};
    \node[obs] at ([yshift = 0cm,xshift = 1.3cm]x3.east) (xp) {$x$};
    \draw [->] (zp.south) -- (xp.north);
    \plate {}{(zp)(xp)}{$x\in X$};

    \draw[dashed,cyan,very thick] ([yshift = 0.5cm,xshift = -0.5]p1.west) rectangle  ([yshift = -0.5cm,xshift = -0.5]xp.east);
    \node[align = center] at ([yshift = -1cm,xshift = 0.6cm]x2.south) {Sleep-R: Train $q$\\training data: cyan $(x,p)$};
  \end{scope}
    
  \begin{scope}[shift={(7.7,-4)}]
    \node[latent] at (0.5,3) (d){$\mathcal{D}$};
    \node[latent] at (1.5,3) (t){$\theta$};
    \node[obs] at (1,1.75) (z){$p$};
    % \node[latent] at (3.5,1.5) (tx){$\theta^{(x)}$};
    \node[obs] at (1,0.5) (x) {$x$};
    \edge {z}{x};
    \edge {d,t}{z};
    \plate {}{(z)(x)}{$x\in X$};
    \node[align = center] at ([yshift = -1cm]x.south) {Sleep-G: Induce $(\mathcal{D},\theta)$};
  \end{scope}

\end{tikzpicture}
\end{figure}


\section{Program Representation}\label{programrepresentation}
We choose to represent programs using $\lambda$-calculus~\cite{pierce}.
A $\lambda$-calculus expression is either:
\begin{itemize}
  \item[--] A \emph{primitive}, like the number 5 or the function \texttt{sum}.
  \item[--] A \emph{variable}, like $x$, $y$, or $z$.
  \item[--] A $\lambda$\emph{-abstraction}, which creates a new function.  $\lambda$-abstractions have a variable and a body. The body is a $\lambda$-calculus expression. Abstractions are written as $\lambda \text{var}. \text{body}$ or in Lisp syntax as \mbox{\texttt{(lambda (\textrm{var}) \textrm{body})}}.
  \item[--] An \emph{application} of a function to an argument. Both the function and the argument are $\lambda$-calculus expressions. The application of the function $f$ to the argument $x$ is written as $f\; x$ or as \texttt{($f$ x)}.
\end{itemize}

For example, the function which squares the logarithm of a number is
$\lambda x.\text{\tt(square (log $x$))}$, and the identity function $f(x) = x$ is $\lambda x.x$. The
$\lambda$-calculus serves as a spartan but expressive Turing complete
program representation, and distills the essential features of functional
programming languages like Lisp.

However, many $\lambda$-calculus expressions correspond to ill-typed programs, such as the program that takes the logarithm of the Boolean \texttt{true} (i.e., \texttt{log true}) or which applies the number five to the identity function
(i.e., $5 \; (\lambda x.x)$).
We use a well-established typing system for $\lambda$-calculus called \emph{Hindley-Milner typing}~\cite{pierce}, which is used in programming languages like OCaml.
The purpose of the typing system is to ensure that our programs never call a function with a type it is not expecting (like trying to take the logarithm of \texttt{true}).
Hindley-Milner has two important features:
Feature 1: It supports \emph{parametric polymorphism}, meaning that types can have variables in them, called \emph{type variables}. Lowercase Greek letters are conventionally used for  type variables.
For example, the type of the identity function is $\alpha\to\alpha$, meaning it takes something of type $\alpha$ and return something of type $\alpha$. A function that returns the first element of a list has the type $[\alpha]\to\alpha$. Type variables are not the same as variables introduced by $\lambda$-abstractions.
Feature 2: Remarkably, there is a  simple algorithm for automatically inferring the polymorphic Hindley-Milner type of a $\lambda$-calculus expression~\cite{damas1982principal}.
Our generative model over programs performs Hindley-Milner type inference during sampling:
\emph{Unify} in the generative model uses the machinery of Hindley-Milner to
ensure that the generated programs have valid polymorphic types.
A satisfactory exposition of Hindley-Milner is beyond the scope of this paper,
but~\cite{pierce} offers a nice overview of lambda calculus and typing systems like Hindley-Milner.

%% \begin{figure}
%%   \begin{align}
%%     \text{primitive types} & 
%%     \end{align}
%%   \end{figure}

\section{Neural recognition model architecture}

The neural recognition model regresses from an observation (set of input/output pairs: $\left\{(i_n,o_n) \right\}_{n\leq N}$) to a $|\mathcal{D}| + 1$ dimensional vector. Each input/output pair is processed by an identical encoder network;
the outputs of the encoders are max pooled and passed to an MLP with 1 hidden layer, 16 hidden units, and a ReLU activation:
\begin{align}
  q(x) = \text{MLP}\left(\text{MaxPool}\left(\left\{\text{encoder}\left(i_n,o_n \right) \right\}_{n\leq N} \right) \right)
\end{align}
For the Boolean circuits and symbolic regression domains,
the inputs and outputs have a fixed dimensionality,
and the set of possible inputs is prespecified. For these fixed-dimensionality domains our encoder is an MLP with 1 hidden layer having 16  units and a ReLU activation function. The input to the encoder is
the sequence of program outputs $\left\{o_n \right\}_{n\leq N}$.

For the string editing and list domains,
the inputs and outputs are sequences. Our encoder for these domains is a bidirectional GRU with 16 hidden units that reads each input/output pair; we concatenate the input and output along with a special delimiter
symbol between them.
We MaxPool the final hidden unit activations in the GRU along both passes of the bidirectional GRU.

\section{DSL Induction}

\subsection{Structure learning}

We use Alg.~\ref{grammarInductionAlgorithm} to search for the structure of the DSL that best explains the frontiers.

\setcounter{algorithm}{2} % there are two algorithms in the main paper.
\begin{algorithm}[tb]
   \caption{DSL Induction Algorithm}
   \label{grammarInductionAlgorithm}
   \begin{algorithmic}
     \STATE {\bfseries Input:} Set of frontiers $\{\mathcal{F}_x\}$
     \STATE \textbf{Hyperparameters:} Pseudocounts $\alpha$, regularization parameter $\lambda$
     \STATE \textbf{Output:} DSL $\mathcal{D}$, weight vector $\theta$
     \STATE Define $L(\mathcal{D},\theta) =  \prod_x \sum_{z\in \mathcal{F}_x} \probability[z|\mathcal{D},\theta]$
     \STATE Define $\theta^*(\mathcal{D}) = \argmax_\theta \text{Dir}(\theta|\alpha) L(\mathcal{D},\theta)$
     \STATE Define $\text{score}(\mathcal{D}) = \log \probability[\mathcal{D}] + L(\mathcal{D},\theta^*) - \|\theta\|_0$
     \STATE $\mathcal{D}\gets$ every primitive in $\{\mathcal{F}_x\}$
     \WHILE {true}
     \STATE N $\gets \{\mathcal{D}\cup \{s\} | x\in X, z\in \mathcal{F}_x, s\text{ subexpression of }z\}$
     \STATE $\mathcal{D}'\gets \argmax_{\mathcal{D}'\in N}\text{score}(\mathcal{D}') $
     \STATE \textbf{if }$\text{score}(\mathcal{D}') < \text{score}(\mathcal{D})$\textbf{ return }$\mathcal{D},\theta^*(\mathcal{D})$
     \STATE $\mathcal{D}\gets\mathcal{D}'$
     \ENDWHILE
   \end{algorithmic}
\end{algorithm}

\subsection{Estimating $\theta$}



We write $c(e,p)$ to mean the number of times that primitive $e$ was used in program $p$; $c(p)= \sum_{e\in \mathcal{D}}c(e,p)$ to mean the total number of primitives used in program $p$; $R(p)$ to mean the sequence of types input to sample in Alg.~1. Jensen's inequality gives an intuitive lower bound on the likelihood of a program $p$:
\begin{align*}
  \log \probability[p|\theta]&\stackrel{+}{ = }\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - \sum_{\tau\in R(p)} \log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  &\stackrel{+}{\geq }\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \sum_{\tau\in R(p)} \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  & = \sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \sum_{e\in \mathcal{D}} r(e,p)\theta_e
\end{align*}
where  $r(e:\tau',p) = \sum_{\tau\in R(p)} \indicator[\text{canUnify}(\tau,\tau')]$.

Differentiate with respect to $\theta_e$ and set to zero:
%\begin{align*}
%  \frac{c(e, p)}{\theta_e} &= c(p)\frac{r(e, p)}{\sum_{e'\in\mathcal{D}} r(e', p)\theta_{e'}}
%\end{align*}
%or more succinctly,
\begin{align}
  \frac{c(x,p)}{\theta_x} &= N\frac{a(x)}{\sum_y a(y)\theta_y}
\end{align}
where $N=c(p)$ and $a(x) = r(x, p)$. From here forward, we similarly let
$c(x)=c(x,p)$ for notational convenience.

This equality holds if $\theta_x = c(x)/a(x)$:
\begin{align}
  \frac{c(x)}{\theta_x} &= a(x).\\
N\frac{a(x)}{\sum_y a(y)\theta_y}& = N\frac{a(x)}{\sum_y c(y)}   = N\frac{a(x)}{N} = a(x).
\end{align}
If this equality holds then $\theta_x \propto c(x)/a(x)$:
\begin{align}
  \theta_x = \frac{c(x)}{a(x)}\times \underbrace{\frac{\sum_y a(y)\theta_y}{N}}_{\text{Independent of $x$}}.
\end{align}

Now what we are actually after is the parameters that maximize the joint log probability of the data and parameters $\theta$, which we  write $J$:
\begin{align}
  J& = L + \log \text{Dir}(\theta|\alpha)\\
  &\stackrel{+}{\geq } \sum_x c(x)\log \theta_x - N \log \sum_x a(x)\theta_x  + \sum_x(\alpha_x - 1)\log \theta_x\\
  & = \sum_x (c(x) + \alpha_x - 1)\log \theta_x -  N \log \sum_x a(x)\theta_x
\end{align}
We identify the parameters of the prior over $\theta$, $\alpha$, as  pseudocounts that are added to the \emph{counts} ($c(x)$), but not to the \emph{possible counts} ($a(x)$).


This lower bound is tight whenever all
of the types of the expressions in the DSL are not polymorphic, in which case our DSL is equivalent to a PCFG
and this estimator is equivalent to the inside/outside algorithm.
Polymorphism introduces context-sensitivity to the DSL,
and exactly maximizing the likelihood with respect to $\theta$
becomes intractable,
so for domains with polymorphic types we use this estimator.

\section{Initial domain-specific languages}
\subsection{Boolean circuits}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    nand & $\text{bool}\to \text{bool}\to \text{bool}$ \\  \hline
\end{tabular}
\end{center}

\subsection{Symbolic regression}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  + & $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$ \\
  *&  $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$\\
  real&$\mathbb{R}$\\\hline
\end{tabular}
\end{center}


\subsection{String editing}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  0&$\mathbb{Z}$\\
  +1&$\mathbb{Z}\to \mathbb{Z}$\\
  -1&$\mathbb{Z}\to \mathbb{Z}$\\
  split&$\text{char}\to \text{string}\to \left[\text{string} \right]$\\
  join&$\text{string}\to \left[\text{string} \right]\to \text{string}$\\
  map&$(\text{string}\to \text{string})$\\
  &$\qquad\quad\to\left[\text{string} \right]\to \left[\text{string} \right]$\\
  nth&$\mathbb{Z}\to \left[\text{string} \right]\to \text{string}$\\
  slice&$\mathbb{Z}\to \mathbb{Z}\to \text{string}\to \text{string}$\\
  length&$\text{string}\to \mathbb{Z}$\\
  chr->str&$\text{char}\to \text{string}$\\
  trim&$\text{string}\to \text{string}$\\
  upper&$\text{string}\to \text{string}$\\
  lower&$\text{string}\to \text{string}$\\
  capitalize&$\text{string}\to \text{string}$\\\hline
\end{tabular}
\end{center}
We also provide the constant empty string which has type string and also provide
constant characters which have the type char.


\vfill


\subsection{List functions}
The lower section with seven primitives designates those given to the
\emph{rich} DSL variant and not the \emph{base} variant. The rich variant is
not supplied the \texttt{if} primitive, because its primary intention is to
enable the learning of the lower five concepts.
Greek letters represent polymorphic types.
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    empty & [$\alpha$] \\
    singleton & $\alpha \to$ [$\alpha$] \\
    concat & [$\alpha$] $\to$ [$\alpha$] $\to$ [$\alpha$] \\
    mapi & $(\mathbb{Z} \to \alpha \to \beta)$ \\
    & \qquad\qquad$\to$ [$\alpha$] $\to$ [$\beta$] \\
    reducei & $(\beta \to \mathbb{Z} \to \alpha \to \beta)$\\
    & \qquad\qquad$\to \beta \to$ [$\alpha$] $\to \beta$ \\
    \hline
    if & bool $\to \alpha \to \alpha \to \alpha$ \\
    \hline
    true & bool \\
    not & bool $\to$ bool \\
%UNUSED+UNNECESSARY:    and & bool $\to$ bool $\to$ bool \\
%UNUSED+UNNECESSARY:    or & bool $\to$ bool $\to$ bool \\
    0 $\cdots$ 5 & $\mathbb{Z}$ \\
    + & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    * & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    negate & $\mathbb{Z} \to \mathbb{Z}$ \\
    mod & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    eq? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    gt? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    is-prime & $\mathbb{Z} \to$ bool \\
    is-square & $\mathbb{Z} \to$ bool \\
    range & $\mathbb{Z} \to$ [$\mathbb{Z}$] \\
    sort & [$\mathbb{Z}$] $\to$ [$\mathbb{Z}$] \\
    \hline
    sum & [$\mathbb{Z}$] $\to \mathbb{Z}$ \\
    reverse & [$\alpha$] $\to$ [$\alpha$] \\
    all & $(\alpha \to$ bool$) \to$ [$\alpha$] $\to$ bool \\
    any & $(\alpha \to$ bool$) \to$ [$\alpha$] $\to$ bool \\
    index & $\mathbb{Z} \to$ [$\alpha$] $\to \alpha$ \\
    filter & $(\alpha \to$ bool$) \to$ [$\alpha$] $\to$ [$\alpha$] \\
    slice & $\mathbb{Z} \to \mathbb{Z} \to$ [$\alpha$] $\to$ [$\alpha$] \\
  \hline
\end{tabular}
\end{center}

\vfill

\section{Hyperparameters \& Implementation details}
We set $\alpha = 10$ and $\lambda = 1$ for all experiments except for string
editing. For string editing we found that more strongly regularizing
the DSL was necessary,
and so we set $\lambda = 5$.
We believe this is because
we have many more tasks for string editing than we do for the other domains.
The frontier size used for Boolean circuits was $5000$,
for symbolic regression we used $500$,
for string editing we used $10^5$,
and for list functions we used $10^4$.



Because the frontiers can become very large in later iterations of the algorithm,
we only keep around the top $10^4$ programs in the frontier $\mathcal{F}_x$ as measured by $\probability[x,p|\mathcal{D},\theta]$.




\section{Why not the ELBO bound?}

If we were to maximize the ELBO bound to perform inference in
our generative model,
then, during DSL induction, we would seek a new $(\mathcal{D}^*,\theta^*)$ maximizing the following lower bound on the likelihood (along an unimportant regularizing term on the DSL):
\begin{align}
  &  \sum_{x\in X}\expect_{p\sim Q_x}\left[\log \probability[p|\mathcal{D}^*,\theta^*] \right] \label{EM}\\
  & Q_x(p)\triangleq \probability[p|x,\mathcal{D},\theta]
\end{align}
where $(\mathcal{D},\theta)$ is our current estimate of the generative
model.  These equations fall out of an EM-style derivation, and one
could replace $Q_x(p)$ with the recognition model $q(p|x)$, either
using importance sampling (so the expectation in Eq.~\ref{EM} is taken
over $q$ and we reweigh using $Q_x$) or by directly using $q$ as our approximate posterior over the program that solves task $x$.

How the recognition model is worked into this is unimportant to the reason why we do not maximize a bound of this form:
this bound takes an expectation over the \emph{previous} sleep cycle's
posterior over programs,
and so the approximate posterior $Q_x$ at the next iteration
ends up being very close to previous approximate posterior.
Intuitively,
we want the DSL induction to be a function \emph{only} of the programs that we have found,
and \emph{not} be a function of how the previous generative model weighed them.
In practice,
we found that maximizing EM-style bounds, like the ELBO,
leads to a kind of hysteresis effect,
where the next generative model too closely matches the previous one,
causing the algorithm to easily become trapped in local optima.



\bibliography{main}
\bibliographystyle{icml2018}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
