%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\system}{\textsc{DreamCoder}~}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}


\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{listings}
\usepackage{amsthm}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig}
\usepackage{fancyvrb}


\usepackage{caption}
\usepackage{subcaption}

\fvset{fontsize=\footnotesize}

\usepackage{amssymb}
\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize\ttfamily,
  breaklines=true}
\usepackage{wrapfig}
\usepackage{tabularx}


\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{fit,bayesnet}
%\usetikzlibrary{arrows.meta}
%\usetikzlibrary{positioning}
%\usetikzlibrary{decorations.text}
%\usetikzlibrary{decorations.pathmorphing}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{hyperref}
%\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\argmin}{argmin} % no space, limits underneath in displays



% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em E}}
\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}



% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\textsc{DreamCoder}: Bootstrapping Domain Specific Languages for Neurally-Guided Bayesian Program Learning}

\begin{document}

\twocolumn[
\icmltitle{Supplement: Bootstrapping Domain Specific Languages for Neurally-Guided Bayesian Program Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\section{An Illustration of the Wake-Sleep Cycle}
Below we diagram the wake/sleep cycles employed by our algorithm. At each stage of the cycle,
we have shaded the observed variables in gray and left the unobserved variables white.
Black lines correspond to a connection from the top-down
generative model, while red lines correspond to connections from the bottom-up recognition model.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \begin{scope}[shift={(2.5,0)}]
    \node[obs] at (3,3) (dx){$\mathcal{D}$};
    \node[latent] at (3.5,1.75) (zp){$p$};
    \node[obs] at (4,3) (tx){$\theta^{(x)}$};
    \node[obs] at (3.5,0.7) (xp) {$x$};
    \node[align = center] at ([yshift = -0.6cm,xshift = 0.5cm]xp.south) {Wake: Infer $p$};
    \draw [->,red] (xp.east) to[out = 0,in = 0] node(nn){} (tx.east);
    \draw [->,red] (tx) -- (zp);
    \draw [->] (dx) -- (zp);
    \node at (nn) {
      \begin{tikzpicture}[x=2.5cm,y=1.25cm,transform canvas={scale=0.2,shift={+(-1,2.5)}}]
        \tikzstyle{neuron}=[circle,fill=blue!50,minimum size=20pt]
        \fill[fill=white] (-0.25,-0.5) rectangle (2.25,-4.5);
        \node[rectangle] at (1,1) {};
        \foreach \name / \y in {1,...,4}
            \node[neuron] (I-\name) at (0,-\y) {};
        \foreach \name / \y in {1,...,3}
            \node[neuron] (H-\name) at (1,-\y-0.5) {};
        \foreach \name / \y in {1,...,4}
            \node[neuron] (O-\name) at (2,-\y) {};
        \foreach \source in {1,...,4}
            \foreach \dest in {1,...,3}
                \draw [-latex] (I-\source) -- (H-\dest);
        \foreach \source in {1,...,3}
            \foreach \dest in {1,...,4}
                \draw [-latex] (H-\source) -- (O-\dest);
      \end{tikzpicture}
    };
    \node[shift={+(0,-0.65)}] at (nn) [fill=white]{ $q(x)$ };
  \end{scope}

  \begin{scope}[shift={(0.7,-4.2)}]
    \node[obs] at (3,3) (dt){$\mathcal{D},\theta$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]dt.south) (p2){$p_2$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p2.south) (x2){$x_2$};
    \draw [->] (p2.south) -- (x2.north);
    \draw [->] (dt.south) -- (p2.north);


    \node[obs] at ([yshift = 0cm,xshift = -0.5cm]p2.west) (p1){$p_1$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p1.south) (x1){$x_1$};
    \draw [->] (p1.south) -- (x1.north);
    \draw [->] (dt.south) -- (p1.north);

    \node[obs] at ([yshift = 0cm,xshift = 0.5cm]p2.east) (p3){$p_3$};
    \node[obs] at ([yshift = -0.7cm,xshift = 0.0cm]p3.south) (x3){$x_3$};
    \draw [->] (p3.south) -- (x3.north);
    \draw [->] (dt.south) -- (p3.north);

    \node at ([yshift = 0cm,xshift = 0.3cm]p3.east) {$\cdots $};
    \node at ([yshift = 0cm,xshift = 0.3cm]x3.east) {$\cdots $};
      
    \node[obs] at ([yshift = 0cm,xshift = 1.3cm]p3.east) (zp){$p$};
    \node[obs] at ([yshift = 0cm,xshift = 1.3cm]x3.east) (xp) {$x$};
    \draw [->] (zp.south) -- (xp.north);
    \plate {}{(zp)(xp)}{$x\in X$};

    \draw[dashed,cyan,very thick] ([yshift = 0.5cm,xshift = -0.5]p1.west) rectangle  ([yshift = -0.5cm,xshift = -0.5]xp.east);
    \node[align = center] at ([yshift = -1cm,xshift = 0.6cm]x2.south) {Sleep-R: Train $q$\\training data: cyan $(x,p)$};
  \end{scope}
    
  \begin{scope}[shift={(7.7,-4)}]
    \node[latent] at (0.5,3) (d){$\mathcal{D}$};
    \node[latent] at (1.5,3) (t){$\theta$};
    \node[obs] at (1,1.75) (z){$p$};
    % \node[latent] at (3.5,1.5) (tx){$\theta^{(x)}$};
    \node[obs] at (1,0.5) (x) {$x$};
    \edge {z}{x};
    \edge {d,t}{z};
    \plate {}{(z)(x)}{$x\in X$};
    \node[align = center] at ([yshift = -1cm]x.south) {Sleep-G: Induce $(\mathcal{D},\theta)$};
  \end{scope}

\end{tikzpicture}
\end{figure}



\section{Program Representation}\label{programrepresentation}
We choose to represent programs using $\lambda$-calculus~\cite{pierce}.
A $\lambda$-calculus expression is either:
\begin{itemize}
  \item[--] A \emph{primitive}, like the number 5 or the function \texttt{sum}.
  \item[--] A \emph{variable}, like $x$, $y$, or $z$.
  \item[--] A $\lambda$\emph{-abstraction}, which creates a new function.  $\lambda$-abstractions have a variable and a body. The body is a $\lambda$-calculus expression. Abstractions are written as $\lambda \text{var}. \text{body}$ or in Lisp syntax as \mbox{\texttt{(lambda (\textrm{var}) \textrm{body})}}.
  \item[--] An \emph{application} of a function to an argument. Both the function and the argument are $\lambda$-calculus expressions. The application of the function $f$ to the argument $x$ is written as $f\; x$ or as \texttt{($f$ x)}.
\end{itemize}

For example, the function which squares the logarithm of a number is
$\lambda x.\text{\tt(square (log $x$))}$, and the identity function $f(x) = x$ is $\lambda x.x$. The
$\lambda$-calculus serves as a spartan but expressive Turing complete
program representation, and distills the essential features of functional
programming languages like Lisp.

However, many $\lambda$-calculus expressions correspond to ill-typed programs, such as the program that takes the logarithm of the Boolean \texttt{true} (i.e., \texttt{log true}) or which applies the number five to the identity function
(i.e., $5 \; (\lambda x.x)$).
We use a well-established typing system for $\lambda$-calculus called \emph{Hindley-Milner typing}~\cite{pierce}, which is used in programming languages like OCaml.
The purpose of the typing system is to ensure that our programs never call a function with a type it is not expecting (like trying to take the logarithm of \texttt{true}).
Hindley-Milner has two important features:
Feature 1: It supports \emph{parametric polymorphism}, meaning that types can have variables in them, called \emph{type variables}. Lowercase Greek letters are conventionally used for  type variables.
For example, the type of the identity function is $\alpha\to\alpha$, meaning it takes something of type $\alpha$ and return something of type $\alpha$. A function that returns the first element of a list has the type $[\alpha]\to\alpha$. Type variables are not the same as variables introduced by $\lambda$-abstractions.
Feature 2: Remarkably, there is a  simple algorithm for automatically inferring the polymorphic Hindley-Milner type of a $\lambda$-calculus expression~\cite{damas1982principal}.
Our generative model over programs performs Hindley-Milner type inference during sampling:
\emph{Unify} in the generative model uses the machinery of Hindley-Milner to
ensure that the generated programs have valid polymorphic types.
A satisfactory exposition of Hindley-Milner is beyond the scope of this paper,
but~\cite{pierce} offers a nice overview of lambda calculus and typing systems like Hindley-Milner.

%% \begin{figure}
%%   \begin{align}
%%     \text{primitive types} & 
%%     \end{align}
%%   \end{figure}

\section{Neural Recognition Model Architecture}

The neural recognition model regresses from an observation (set of input/output pairs: $\left\{(i_n,o_n) \right\}_{n\leq N}$) to a $|\mathcal{D}| + 1$ dimensional vector. Each input/output pair is processed by an identical encoder network;
the outputs of the encoders are max pooled and passed to an MLP with 1 hidden layer, 16 hidden units, and a ReLU activation:
\begin{align}
  q(x) = \text{MLP}\left(\text{MaxPool}\left(\left\{\text{encoder}\left(i_n,o_n \right) \right\}_{n\leq N} \right) \right)
\end{align}
For the Boolean circuits and symbolic regression domains,
the inputs and outputs have a fixed dimensionality,
and the set of possible inputs is prespecified. For these fixed-dimensionality domains our encoder is an MLP with 1 hidden layer having 16  units and a ReLU activation function. The input to the encoder is
the sequence of program outputs $\left\{o_n \right\}_{n\leq N}$.

For the string editing and list domains,
the inputs and outputs are sequences. Our encoder for these domains is a bidirectional GRU with 16 hidden units that reads each input/output pair; we concatenate the input and output along with a special delimiter
symbol between them.
We MaxPool the final hidden unit activations in the GRU along both passes of the bidirectional GRU.

\section{DSL Induction}

\subsection{Structure Learning}

We use Alg.~\ref{grammarInductionAlgorithm} to search for the structure of the DSL that best explains the frontiers.

\setcounter{algorithm}{2} % there are two algorithms in the main paper.
\begin{algorithm}[hb]
   \caption{DSL Induction Algorithm}
   \label{grammarInductionAlgorithm}
   \begin{algorithmic}
     \STATE {\bfseries Input:} Set of frontiers $\{\mathcal{F}_x\}$
     \STATE \textbf{Hyperparameters:} Pseudocounts $\alpha$, regularization parameter $\lambda$
     \STATE \textbf{Output:} DSL $\mathcal{D}$, weight vector $\theta$
     \STATE Define $L(\mathcal{D},\theta) =  \prod_x \sum_{p\in \mathcal{F}_x} \probability[p|\mathcal{D},\theta]$
     \STATE Define $\theta^*(\mathcal{D}) = \argmax_\theta \text{Dir}(\theta|\alpha) L(\mathcal{D},\theta)$
     \STATE Define $\text{score}(\mathcal{D}) = \log \probability[\mathcal{D}] + L(\mathcal{D},\theta^*) - \|\theta\|_0$
     \STATE $\mathcal{D}\gets$ every primitive in $\{\mathcal{F}_x\}$
     \WHILE {true}
     \STATE N $\gets \{\mathcal{D}\cup \{s\} | x\in X, p\in \mathcal{F}_x, s\text{ subexpression of }p\}$
     \STATE $\mathcal{D}'\gets \argmax_{\mathcal{D}'\in N}\text{score}(\mathcal{D}') $
     \STATE \textbf{if }$\text{score}(\mathcal{D}') < \text{score}(\mathcal{D})$\textbf{ return }$\mathcal{D},\theta^*(\mathcal{D})$
     \STATE $\mathcal{D}\gets\mathcal{D}'$
     \ENDWHILE
   \end{algorithmic}
\end{algorithm}

\subsection{Estimating $\theta$}

%% We use an EM algorithm to estimate the continuous parameters of the DSL, e.g. $\theta$.
%% Suppressing dependencies on $\mathcal{D}$, the EM updates are
%% \begin{align}
%% \label{maximizeStep}  \theta& = \argmax_\theta \log P(\theta) + \sum_x \expect_{Q_x}\left[\log \probability\left[p|\theta \right] \right]\\
%%   Q_x(p)&\propto \probability[x|p]\probability[p|\theta]
%%   \end{align}
%% In the M step of EM we will update $\theta$ by instead maximizing a lower bound on $\log \probability[p|\theta]$.
We write $c(e,p)$ to mean the number of times that primitive $e$ was used in program $p$; $c(p)= \sum_{e\in \mathcal{D}}c(e,p)$ to mean the total number of primitives used in program $p$; $R(p)$ to mean the sequence of types input to sample in Alg.~1 of the main paper. Jensen's inequality gives an intuitive lower bound on the likelihood of a program $p$:
\begin{align*}
  \log \probability[p|\theta]&\stackrel{+}{ = }\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - \sum_{\tau\in R(p)} \log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  &\stackrel{+}{\geq }\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \sum_{\tau\in R(p)} \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  & = \sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \sum_{e\in \mathcal{D}} r(e,p)\theta_e
\end{align*}
where we have defined
$$r(e:\tau',p)\triangleq\sum_{\tau\in R(p)} \indicator[\text{canUnify}(\tau,\tau')]$$

%% Returning to equation~\ref{maximizeStep}:
%% \begin{align*}
%% \log P(\theta) +& \sum_x \expect_{Q_x}\left[\log \probability\left[p|\theta \right] \right]\geq \\
%% \log   \text{Dir}(\theta|\alpha) +& \sum_x\sum_{e\in \mathcal{D}}\expect_{Q_x}\left[ c(e,p)\right]\log \theta_e -\\
%% &\sum_x  \expect_{Q_x}\left[c(p)\log \sum_{e\in \mathcal{D}}r(e,p)\theta_e \right]
%%   \end{align*}




Differentiating with respect to $\theta_e$ and setting to zero:
%% \begin{align}
%%   & \frac{\alpha + \sum_x\expect_{Q_x}\left[ c(e,p)\right]}{\theta_e} -
%% \theta_e\sum_x  \expect_{Q_x}\left[c(p)\frac{r(p,e)}{\sum_{e'\in \mathcal{D}} r(e',p)\theta_{e'}} \right]  
%%   \end{align}
%%
\begin{align*}
  \frac{c(e, p)}{\theta_e} &= c(p)\frac{r(e, p)}{\sum_{e'\in\mathcal{D}} r(e', p)\theta_{e'}} \\
  \frac{c(x,p)}{\theta_x} &= N\frac{a(x)}{\sum_y a(y)\theta_y}
\end{align*}
where $N=c(p)$ and $a(x) = r(x, p)$. From here forward, we similarly let
$c(x)=c(x,p)$ for notational convenience.

%% This equality holds if $\theta_x = c(x)/a(x)$:
%% \begin{align}
%%   \frac{c(x)}{\theta_x} &= a(x).\\
%% N\frac{a(x)}{\sum_y a(y)\theta_y}& = N\frac{a(x)}{\sum_y c(y)}   = N\frac{a(x)}{N} = a(x).
%% \end{align}
If the above equality holds then $\theta_x \propto c(x)/a(x)$:
\begin{align}
  \theta_x = \frac{c(x)}{a(x)}\times \underbrace{\frac{\sum_y a(y)\theta_y}{N}}_{\text{Independent of $x$}}
\end{align}

Now what we are actually after is the parameters that maximize the joint log probability of the data and parameters $\theta$, which we  write $J$:
\begin{align}
  \nonumber J& = L + \log \text{Dir}(\theta|\alpha)\\
  \nonumber&\stackrel{+}{\geq } \sum_x c(x)\log \theta_x - N \log \sum_x a(x)\theta_x \\
  \nonumber&\qquad\quad+ \sum_x(\alpha_x - 1)\log \theta_x\\
  & = \sum_x (c(x) + \alpha_x - 1)\log \theta_x -  N \log \sum_x a(x)\theta_x
\end{align}
We identify the parameters of the prior over $\theta$, $\alpha$, as  pseudocounts that are added to the \emph{counts} $c(x)$, but not to the \emph{possible counts} $a(x)$,
and so we estimate $\theta_e$ as the ratio of the sum of the number of times that $e$ was used plus the pseudocounts $\alpha$, with the number of times that it could have been used, $a(e)$.



This lower bound is tight whenever all
of the types of the expressions in the DSL are not polymorphic, in which case our DSL is equivalent to a PCFG
and this estimator is equivalent to the inside/outside algorithm.
Polymorphism introduces context-sensitivity to the DSL,
and exactly maximizing the likelihood with respect to $\theta$
becomes intractable,
so for domains with polymorphic types we use this estimator.

\section{Initial Domain-Specific Languages}
\subsection{Boolean Circuits}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    nand & $\text{bool}\to \text{bool}\to \text{bool}$ \\  \hline
\end{tabular}
\end{center}

\subsection{Symbolic Regression}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  + & $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$ \\
  *&  $\mathbb{R}\to \mathbb{R}\to \mathbb{R}$\\
  real&$\mathbb{R}$\\\hline
\end{tabular}
\end{center}


\subsection{String Editing}
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
  0&$\mathbb{Z}$\\
  +1&$\mathbb{Z}\to \mathbb{Z}$\\
  -1&$\mathbb{Z}\to \mathbb{Z}$\\
  split&$\text{char}\to \text{string}\to \left[\text{string} \right]$\\
  join&$\text{string}\to \left[\text{string} \right]\to \text{string}$\\
  map&$(\text{string}\to \text{string})$\\
  &$\qquad\quad\to\left[\text{string} \right]\to \left[\text{string} \right]$\\
  nth&$\mathbb{Z}\to \left[\text{string} \right]\to \text{string}$\\
  slice&$\mathbb{Z}\to \mathbb{Z}\to \text{string}\to \text{string}$\\
  length&$\text{string}\to \mathbb{Z}$\\
  ""&string\\
  ","\quad" "&char\\
  "<"\quad">" &char \\
  chr->str&$\text{char}\to \text{string}$\\
  trim&$\text{string}\to \text{string}$\\
  upper&$\text{string}\to \text{string}$\\
  lower&$\text{string}\to \text{string}$\\
  capitalize&$\text{string}\to \text{string}$\\\hline
\end{tabular}
\end{center}


\vfill


\subsection{List Functions}
The lower section with seven primitives designates those given to the
\emph{rich} DSL variant and not the \emph{base} variant. The rich variant is
not supplied the \texttt{if} primitive, because its primary intention is to
enable the learning of the lower five concepts.
Greek letters represent polymorphic types.
\begin{center}
\tt
\begin{tabular}{| l | l |}
  \hline
  \textrm{\emph{name}} & \textrm{\emph{type}} \\
  \hline
    empty & $\left[\alpha\right]$ \\
    singleton & $\alpha \to \left[\alpha\right]$ \\
    concat & $\left[\alpha\right] \to \left[\alpha\right] \to \left[\alpha\right]$ \\
    mapi & $(\mathbb{Z} \to \alpha \to \beta)$ \\
    & \qquad\qquad$\to \left[\alpha\right] \to \left[\beta\right]$ \\
    reducei & $(\beta \to \mathbb{Z} \to \alpha \to \beta)$\\
    & \qquad\qquad$\to \beta \to \left[\alpha\right] \to \beta$ \\
    \hline
    if & bool $\to \alpha \to \alpha \to \alpha$ \\
    \hline
    true & bool \\
    not & bool $\to$ bool \\
%UNUSED+UNNECESSARY:    and & bool $\to$ bool $\to$ bool \\
%UNUSED+UNNECESSARY:    or & bool $\to$ bool $\to$ bool \\
    0 $\cdots$ 5 & $\mathbb{Z}$ \\
    + & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    * & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    negate & $\mathbb{Z} \to \mathbb{Z}$ \\
    mod & $\mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}$ \\
    eq? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    gt? & $\mathbb{Z} \to \mathbb{Z} \to$ bool \\
    is-prime & $\mathbb{Z} \to$ bool \\
    is-square & $\mathbb{Z} \to$ bool \\
    range & $\mathbb{Z} \to$ [$\mathbb{Z}$] \\
    sort & $\left[\mathbb{Z}\right] \to \left[\mathbb{Z}\right]$ \\
    \hline
    sum & $\left[\mathbb{Z}\right] \to \mathbb{Z}$ \\
    reverse & $\left[\alpha\right] \to \left[\alpha\right]$ \\
    all & $(\alpha \to$ bool$) \to \left[\alpha\right] \to$ bool \\
    any & $(\alpha \to$ bool$) \to \left[\alpha\right] \to$ bool \\
    index & $\mathbb{Z} \to \left[\alpha\right] \to \alpha$ \\
    filter & $(\alpha \to$ bool$) \to \left[\alpha\right] \to \left[\alpha\right]$ \\
    slice & $\mathbb{Z} \to \mathbb{Z} \to \left[\alpha\right] \to \left[\alpha\right]$ \\
  \hline
\end{tabular}
\end{center}

\vfill

\section{Hyperparameters \& Implementation Details}
We set structure penalty $\lambda = 1$ (Eq.~5 of the main paper) and
smoothness parameter $\alpha = 10$ (Eq.~6 of the main paper)
for all experiments except for string editing.
For string editing we found that more strongly regularizing
the DSL was necessary,
so we set $\lambda = 5$.
We believe this is because
we have many more tasks for string editing than we do for the other domains.
The frontier size used for Boolean circuits was $5000$,
for symbolic regression we used $500$,
for string editing we used $10^5$,
and for list functions we used $10^4$.



Because the frontiers can become very large in later iterations of the algorithm,
we only keep around the top $10^4$ programs in the frontier $\mathcal{F}_x$ as measured by $\probability[x,p|\mathcal{D},\theta]$.




\section{Why not the ELBO Bound?}

If we were to maximize the ELBO bound to perform inference in
our generative model,
then, during DSL induction, we would seek a new $(\mathcal{D}^*,\theta^*)$ maximizing the following lower bound on the likelihood (along with an unimportant regularizing term on the DSL):
\begin{align}
  &  \sum_{x\in X}\expect_{p\sim Q_x}\left[\log \probability[p|\mathcal{D}^*,\theta^*] \right] \label{EM}\\
  & Q_x(p)\triangleq \probability[p|x,\mathcal{D},\theta]
\end{align}
where $(\mathcal{D},\theta)$ is our current estimate of the generative
model.  These equations fall out of an EM-style derivation, and one
could replace $Q_x(p)$ with the recognition model $q(p|x)$, either
using importance sampling (so the expectation in Eq.~\ref{EM} is taken
over $q$ and we reweigh using $Q_x$) or by directly using $q$ as our approximate posterior over the program that solves task $x$.

We do not maximize a bound of this form because it
takes an expectation over the \emph{previous} sleep cycle's
posterior over programs,
so the approximate posterior $Q_x$ at the next iteration
ends up being very close to previous approximate posterior.
Intuitively,
we want the DSL induction to be a function \emph{only} of the programs that we have found,
and \emph{not} be a function of how the previous generative model weighed them.
In practice,
we found that maximizing EM-style bounds, like the ELBO,
leads to a kind of hysteresis effect,
where the next generative model too closely matches the previous one,
causing the algorithm to easily become trapped in local optima.

\section{Example String Editing Tasks}



We automatically generated 600 string editing problems split 75/25 train/test.
The following are representative problems:


\begin{center}
\begin{tabular}{|l|l|}
  \hline Input&Output\\
  \hline\multicolumn{2}{|c|}{Double}\\
  \hline
vlxS & vlxSvlxS \\
 JkVC & JkVCJkVC \\
 dbxD & dbxDdbxD \\
 gQgB & gQgBgQgB \\
  \hline\multicolumn{2}{|c|}{Extract string delimited by ',' (inclusive),' '}\\
  \hline
LsR,uhMZI wIL & ,uhMZI  \\
 tMY,KdF hhT & ,KdF  \\
 UUPdC,UAd kCgtq & ,UAd  \\
 gQgGN,VMJ FjrXZ & ,VMJ \\
  \hline\multicolumn{2}{|c|}{Apply first 2 characters to input delimited by '.'}\\
  \hline
OhCQN.DlH.LdkJ.pLh & OhDlLdpL \\
 CZr.mveZr.dvjP.YMJpg & CZmvdvYM \\
 bpub.VkkZ.dYdPr & bpVkdY \\
 zjNA.wFwY.WMMqh.OFdbr & zjwFWMOF \\
  \hline\multicolumn{2}{|c|}{Apply first 2 characters to string delimited by ' ','.'}\\
  \hline
fNA YWyL.JDD & YW \\
 YWozt TPW.nczz & TP \\
 RJwiT dpW.uFU & dp \\
 FTp gNMo.JsZB & gN \\
  \hline\multicolumn{2}{|c|}{Apply strip to string delimited by ',','.'}\\
  \hline
iNEP,  GigW .Xlm & GigW \\
 xhpH,  QvwnW .mfK & QvwnW \\
 weWIC,IQjhWa .fGKz & IQjhWa \\
 xZUm, FpdE.ETcr & FpdE \\
  \hline\multicolumn{2}{|c|}{Drop first character}\\
  \hline
WYPvm & YPvm \\
 terHk & erHk \\
 asWM & sWM \\
 tBdT & BdT \\
\hline
\end{tabular}
\end{center}

\vfill


\section{Learned DSLs}

Here we present representative DSLs learned by
our model. DSL primitives discovered by the algorithm are prefixed with
\lstinline!#!.
Variables are prefixed with \lstinline!$!, and we use De Bruijn indices to model variables~\cite{pierce}.
We abbreviate lambda as \lstinline!\!.

\pagebreak
\onecolumn
\subsection{Boolean circuits}

\begin{lstlisting}
#(\ (\ (\ (nand (nand $0 $1) (nand $2 (nand $0 $2))))))
#(\ (\ (nand (nand $0 $1) (nand $0 $1))))
#(\ (\ (nand (nand $0 $0) (nand $1 $1))))
#(\ (\ (\ (\ (nand (nand $0 $1) (nand $2 $3))))))
#(\ (\ (\ (nand (nand $0 $0) (nand $1 (nand $2 $1))))))
#(\ (\ (#(\ (\ (nand (nand $0 $1) (nand $0 $1)))) $0 (nand $1 $0))))
#(\ (\ (\ (nand (nand $0 $1) (nand $0 (nand $2 $0))))))
#(\ (\ (#(\ (\ (\ (nand (nand $0 $1) (nand $0 (nand $2 $0)))))) $0 $1 (nand $1 $1))))
#(\ (\ (nand $0 (nand $1 $0))))
#(\ (#(\ (\ (\ (nand (nand $0 $1) (nand $2 (nand $0 $2)))))) (nand $0 $0)))
#(\ (\ (\ (\ (nand (nand $0 $1) (nand $2 (nand $3 $2)))))))
\end{lstlisting}

\subsection{Symbolic regression}
\begin{lstlisting}
#(\ (* (+ REAL $0) REAL))
#(\ (#(\ (+ $0 REAL)) (#(\ (* $0 (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0)))) $0)))
#(\ (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0)))
#(\ (#(\ (#(\ (+ $0 REAL)) (#(\ (* (#(\ (#(\ (+ $0 REAL)) (#(\ (* $0 (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0)))) $0))) $0) $0)) $0))) (+ $0 $0)))
#(\ (#(\ (+ $0 REAL)) (#(\ (* (#(\ (#(\ (+ $0 REAL)) (#(\ (* $0 (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0)))) $0))) $0) $0)) $0)))
#(\ (* (#(\ (+ $0 REAL)) $0) (#(\ (+ (* $0 (* $0 REAL)) REAL)) $0)))
#(\ (+ (* $0 (* $0 REAL)) REAL))
#(* REAL)
#(\ (* (#(\ (#(\ (+ $0 REAL)) (#(\ (* $0 (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0)))) $0))) $0) $0))
#(\ (* $0 (#(\ (+ (* $0 (* $0 REAL)) REAL)) (#(\ (+ $0 REAL)) $0))))
#(\ (+ $0 REAL))
\end{lstlisting}

\subsection{String editing}
\begin{lstlisting}
#(\ (\ (#(nth (incr 0)) (split $0 (#(\ (\ (join (chr->str $0) (map (\ (caseCapitalize $0)) $1)))) $1 $0)))))
#(\ (\ (join (chr->str $0) (map (\ (strip $0)) $1))))
#(\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (++ $0 $0))))
#(\ (#(join empty) (split '.' $0)))
#(#(\ (\ (\ (#(\ (\ (slice $0 (len $1) $1))) (nth $0 $1) (decr $2))))) (decr 0))
#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (slice $2 (incr $2) $0)))))
#(#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (#(\ (\ (slice $0 (len $1) $1))) $0 $2))))) (incr 0))
#(\ (\ (\ (#(\ (\ (slice $0 (len $1) $1))) (nth $0 $1) (decr $2)))))
#(\ (\ (strip (nth 0 (split $0 (#(nth (incr 0)) $1))))))
#(\ (#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (#(\ (\ (slice $0 (len $1) $1))) $0 $2))))) (decr $0)))
#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (#(\ (slice $0 (incr (incr $0)))) $2 $0)))))
#(\ (\ (slice $0 (incr $0) (nth (incr $0) $1))))
#(\ (\ (#(\ (\ (slice $0 (len $1) $1))) (#(\ (\ (strip (nth 0 (split $0 (#(nth (incr 0)) $1)))))) $0 $1) (incr 0))))
#(\ (\ (caseUpper (#(nth (incr 0)) (split $0 (#(\ (\ (join (chr->str $0) (map (\ (caseCapitalize $0)) $1)))) $1 $0))))))
#(\ (\ (#(\ (\ (++ (chr->str $0) (#(nth (incr 0)) $1)))) (split $0 (nth 0 $1)) $0)))
#(\ (#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (#(\ (\ (slice $0 (len $1) $1))) $0 $2))))) (decr (decr $0))))
#(\ (\ (join (chr->str $0) (map (\ (caseCapitalize $0)) $1))))
#(\ (\ (#(\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (++ $0 $0)))) (split '.' (#(\ (\ (strip (nth 0 (split $0 (#(nth (incr 0)) $1)))))) $0 $1)) '.')))
#(\ (\ (\ (#(\ (\ (++ (chr->str $0) (#(nth (incr 0)) $1)))) (split $0 (#(\ (\ (\ (++ (nth $0 (split $1 $2)) (chr->str $1))))) $1 $2 0)) $0))))
#(\ (\ (slice $0 (incr (incr $0)) (#(nth (incr 0)) $1))))
#(join empty)
#(\ (\ (\ (++ (nth $0 (split $1 $2)) (chr->str $1)))))
#(\ (\ (nth 0 (split $0 (#(nth (incr 0)) $1)))))
#(\ (slice $0 (incr (incr $0))))
#(nth (incr 0))
#(\ (join empty (map (\ (caseCapitalize $0)) $0)))
#(\ (\ (#(\ (\ (\ (join (chr->str $0) (map $1 $2))))) $0 (\ (#(\ (\ (slice $0 (len $1) $1))) $0 $2)))))
#(\ (\ (slice $0 (len $1) $1)))
#(\ (\ (\ (join (chr->str $0) (map $1 $2)))))
#(\ (\ (++ (chr->str $0) (#(nth (incr 0)) $1))))
\end{lstlisting}

\subsection{List functions}
\begin{lstlisting}
#(\ (\ (index $0 (sort $1))))
#(\ (\ (slice $0 (+ $1 4))))
#(mapi (\ (\ (eq? $0 $2))))
#(\ (mapi (\ (\ (gt? $0 $2)))))
#(\ (any (\ (eq? $0 $1))))
#(\ (mapi (\ (\ (+ $0 $2)))))
#(\ (\ (mapi (\ (\ (index $2 $3))) $1)))
#(\ (mapi (\ (\ (mod $0 $2)))))
#(\ (mapi (\ (\ (* $0 $2)))))
#(\ (\ (singleton (index $0 $1))))
#(\ (\ (slice $0 (sum $1) $1)))
#(slice 0)
#(\ (\ (index (negate $0) (sort $1))))
#(\ (++ (singleton $0)))
#(\ (\ (++ $0 (singleton $1))))
#(\ (\ (++ $0 (singleton (index $1 $0)))))
#(\ (\ (++ (singleton (index $0 $1)) $1)))
#(\ (\ (\ (#(\ (mapi (\ (\ (mod $0 $2))))) $0 (#(\ (mapi (\ (\ (+ $0 $2))))) $1 $2)))))
#(\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))
#(#(\ (mapi (\ (\ (gt? $0 $2))))) 3 empty)
#(all (\ (is-square $0)))
#(\ (\ (#(\ (mapi (\ (\ (mod $0 $2))))) $0 (#(\ (mapi (\ (\ (+ $0 $2))))) 1 $1))))
#(\ (\ (all (\ $0) (#(\ (\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))) $0 $1))))
#(\ (all (\ $0) (#(\ (\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))) $0 2)))
#(filter (\ (#(\ (all (\ $0) (#(\ (\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))) $0 2))) (singleton $0))))
#(\ (\ (\ (++ (#(slice 0) $0 $1) (#(\ (\ (slice $0 (sum $1) $1))) $1 $2)))))
#(\ (filter (\ (#(\ (\ (all (\ $0) (#(\ (\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))) $0 $1)))) $1 (singleton $0)))))
#(filter (\ (not (#(\ (all (\ $0) (#(\ (\ (#(\ (mapi (\ (\ (eq? $0 $2))))) 0 (#(\ (mapi (\ (\ (mod $0 $2))))) $0 $1)))) $0 2))) (singleton $0)))))
#(\ (++ $0 (++ $0 $0)))
#(\ (++ $0 (#(\ (++ $0 (++ $0 $0))) $0)))
#(index 0)
\end{lstlisting}

\bibliography{main}
\bibliographystyle{icml2018}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
