
%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{dblfloatfix}    % To enable figures at the bottom of page
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} \usepackage{stmaryrd}% for professional tables

\usepackage{longtable}
\usepackage[section]{placeins}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
%\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\system}{\textsc{DreamCoder}~}
\newcommand{\systemEnding}{\textsc{DreamCoder}}
\newcommand{\lowerBound}{\mathscr{L}}
\newcommand{\denotation}[1]{{\llbracket #1 \rrbracket}}
\newcommand{\code}[1]{{\footnotesize\texttt{#1}}}
\newcommand{\scode}[1]{{\tiny\texttt{#1}}}
\newcommand{\mcode}[1]{{\scriptsize\texttt{#1}}}
\newcommand{\codechar}[1]{{\footnotesize{\texttt{"#1"}}}}
% Use the following line for the initial blind version submitted for review:
%\usepackage[nohyperref]{icml2018}

\usepackage{xcolor}
\definecolor{pop1}{HTML}{1F78b4}
\definecolor{pop2}{HTML}{164C13}
\definecolor{pop3}{HTML}{d95F02}
\definecolor{orange}{HTML}{d95F02}
\definecolor{teal}{HTML}{1b9e77}
\newcommand{\pop}[1]{\textcolor{pop1}{#1}}
\newcommand{\popp}[1]{\textcolor{pop2}{#1}}
\newcommand{\tree}[1]{\textcolor{pop3}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}

\newcommand{\greenCode}[1]{{\footnotesize\popp{\code{#1}}}}
\newcommand{\blueCode}[1]{{\footnotesize\pop{\code{#1}}}}

%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{amsthm}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfig} 
\usepackage{fancyvrb}


\usepackage{caption}
%\usepackage{subcaption}

\fvset{fontsize=\footnotesize}

\usepackage{amssymb}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage{tabularx}


%\usepackage{verbatim}
 \usepackage{booktabs}
 % For algorithms
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{tikz}
\usepackage{circuitikz}
\usetikzlibrary{fit,bayesnet,calc,tikzmark}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.pathmorphing}
\tikzset{squiggle/.style={decorate, decoration={snake,amplitude=.4mm}}}
\usepackage{dsfont}
\usepackage{amsmath}

\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
 


% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.

\newcommand{\Expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\indicator}{\mathds{1}} %{{\rm I\kern-.3em E}}
\newcommand{\expect}{\mathds{E}} %{{\rm I\kern-.3em E}}
\newcommand{\probability}{\mathds{P}} %{{\rm I\kern-.3em P}}
\newcommand{\shift}[1]{\uparrow^{#1}}
\newcommand{\downshift}[1]{\downarrow^{#1}}
\newcommand{\substitute}[2]{[\$ #1 \mapsto #2]}
\newcommand{\reduce}{\longrightarrow}
\newcommand{\manyReduce}{\longrightarrow^*}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcommand{\NeuralNetwork}[1]{    \begin{tikzpicture}[x=2.5cm,y=1.25cm,transform canvas={scale=#1,shift={+(-1,2.5)}}]
      \tikzstyle{neuron}=[circle,fill=blue!50,minimum size=20pt]
      \fill[fill=white] (-0.25,-0.5) rectangle (2.25,-4.5);
      \node[rectangle] at (1,1) {};
      \foreach \name / \y in {1,...,4}
          \node[neuron] (I-\name) at (0,-\y) {};
      \foreach \name / \y in {1,...,3}
          \node[neuron] (H-\name) at (1,-\y-0.5) {};
      \foreach \name / \y in {1,...,4}
          \node[neuron] (O-\name) at (2,-\y) {};
      \foreach \source in {1,...,4}
          \foreach \dest in {1,...,3}
              \draw [-latex] (I-\source) -- (H-\dest);
      \foreach \source in {1,...,3}
          \foreach \dest in {1,...,4}
              \draw [-latex] (H-\source) -- (O-\dest);
    \end{tikzpicture}}
\newcommand{\spiral}[2]{
  \draw[ultra thick,->] ([shift={#1}]-30:#2) arc [radius = #2, start angle = -30, end angle = 90];
  \draw[ultra thick,->] ([shift={#1}]-30:#2) arc [radius = #2, start angle = -30, end angle = 95];

  
      \draw[ultra thick,->] ([shift={#1}]90:#2) arc [radius = #2, start angle = 90, end angle = 210];
      \draw[ultra thick,->] ([shift={#1}]90:#2) arc [radius = #2, start angle = 90, end angle = 205];
      
      \draw[ultra thick,->] ([shift={#1}]210:#2) arc [radius = #2, start angle = 210, end angle = 340];
      \draw[ultra thick,->] ([shift={#1}]210:#2) arc [radius = #2, start angle = 210, end angle = 335];
}
\newcommand{\legend}{
  \begin{tikzpicture}
    \node at (0,0) (uses){is};
    \draw[->,red] ([xshift=-0.6cm]uses.west)  -- (uses.west);
    \node at ([xshift=0.4cm]uses.east) {\NeuralNetwork{0.15}};
    \draw[thin] (-1,-0.4) rectangle (1.2,0.4);
  \end{tikzpicture}
  }

\tikzstyle{vecArrow} = [thick, decoration={markings,mark=at position                                
   1 with {\arrow[semithick]{open triangle 60}}},                                                   
   double distance=1.4pt, shorten >= 5.5pt,                                                         
   preaction = {decorate},                                                                          
   postaction = {draw,line width=1.4pt, white,shorten >= 4.5pt}]                                    
\tikzstyle{innerWhite} = [semithick, white,line width=1.4pt, shorten >= 4.5pt]                      
         
\begin{document}





%% \section{Introduction: Expertise}

%% Human learners acquire expertise in a wide range of domains: some of
%% us become experts in calculus, or cooking, or biology, music, tennis,
%% or software engineering, to name just a few, and every child develops
%% expertise in natural language, intuitive physics~\cite{Pete?},
%% intuitive psychology (theory-of-mind), and motor control.  This
%% picture contrasts sharply with the current state of machine
%% intelligence, where a machine is built to be an expert in a single
%% domain, like boardgames~\cite{alphaGo}, medical
%% diagnosis~\cite{mycin,CNNsForDiseaseDetection}, theorem
%% proving~\cite{eurisko}, or visual object recognition~\cite{alexNet}.
%% Thus an outstanding challenge in the long-term program of building
%% more humanlike machines is to develop an algorithm that, like people,
%% autonomously acquires expertise across many different kinds of
%% domains.

%% We contribute a model of the development of expertise that combines
%% two key ingredients.  First, an expert needs a sufficiently expressive
%% knowledge representation.  Following a long tradition in cognitive
%% science and AI, we represent knowledge as programs, which prior work
%% has used to represent expertise in recognition and generation of
%% handwriting and speech~\cite{lake2015human}, intuitive theories (of
%% kinship, taxonomy, etc.)~\cite{Ullman2012}, and natural language
%% grammar and
%% semantics~\cite{DBLP:journals/cogsr/SchmidK11,piantadosi2011learning}.
%% Second, atop this program representation, experts possess two kinds of
%% domain expertise.  They have at their disposal a powerful, yet
%% specialized, repertoire of concepts and abstractions: e.g., in
%% architecture, these are concepts like `arch' or `foundation'; in
%% software engineering these are libraries of code and domain-specific
%% languages (DSLs).  Here our model represents knowledge as programs,
%% and so we identify this kind of expertise with a DSL.  Experts also
%% have knowledge of when and how to deploy these domain-specific
%% concepts efficiently when solving new problems: e.g., mathematicians
%% efficiently search the space of proofs, intuiting which lemmas are
%% appropriate when; expert chefs intuit which compositions of
%% ingredients are likely tasty, before they actually start cooking.  For
%% our model, this aspect of expertise corresponds to the ability to
%% quickly assemble new, useful programs out of its DSL.

%% We integrate these ideas into a model called DreamCoder which acquires
%% expertise through a novel kind of wake/sleep or `dream' learning. The
%% model iterates through wake cycles -- where it solves problems by
%% writing programs -- and a pair of sleep cycles: a sleep cycle that
%% grows its DSL by replaying experiences from waking and consolidating
%% them into new abstractions, and a sleep cycle that improves its
%% knowledge of how to write programs by training a neural network on
%% replayed experiences as well as `dreams', or samples, from its DSL.

%% DreamCoder builds on multiple generations of AI research, going back
%% to the 1960's~\cite{solomonoff1964formal} when program-learning was
%% proposed as a paradigm for general AI.  Broadly speaking recent work
%% has either developed neural approaches for learning to efficiently
%% deploy a fixed
%% DSL~\cite{devlin2017robustfill,balog2016deepcoder,NGDS,spiral}, or
%% developed symbolic approaches for representing and searching through
%% spaces of
%% programs~\cite{gulwani2011automating,solar2008program,DBLP:books/daglib/0070933}.
%% We were motivated by approaches that learn or grow the
%% DSL~\cite{Dechter:2013:BLV:2540128.2540316,DBLP:conf/icml/LiangJK10,solomonoff1989system,DBLP:journals/corr/abs-1110-5667,stolle2002learning}.
%% Our goal with DreamCoder is to show that the combination of
%% neurally-guided search and DSL learning is a uniquely powerful way of
%% building systems that, like human learners, autonomously acquire the
%% expertise needed to navigate a new domain of problems.





\section{Introduction}

What would it take to build a machine that can learn everything a
person does over their lifetime?
Although this goal will remain distant for the foreseeable future,
we know that a general learning system like this would need to be able to acquire many different kinds of expertise.
Virtually every child becomes an expert in natural language, motor control, and social interaction, while learning rich
intuitive theories of kinship, taxonomy and physics. Many of those
children will grow into adults with competence in cooking, calculus, tennis, drawing
pictures, or writing software.
Expertise -- the ability to quickly solve new problems in a domain -- is crucial.
Human learning goes far beyond just memorizing a large set of facts or routines,
but involves higher-order skills,
like learning to learn new concepts or
learning to solve new types of problems more quickly and effectively.
Despite great advances in machine learning,
we are still far from
an AI with these abilities.
The modern AI toolkit has given us machines that learn to play challenging games at superhuman levels,
but cannot quickly transfer to similar games like a person would;
or, which can learn to generate convincing English prose,
but which do not then learn to analyze many different languages,
like a field linguist would.
Here, we contribute a computational model that takes a small step toward the goal of building machines that
grow into domain experts,
looking at much simpler kinds of problems that humans can learn to solve and indeed become experts in.

Our model is structured
around the hypothesis that
domain expertise
consists primarily of two ingredients.
First, domain experts have 
explicit, declarative concepts that are powerful yet finely-tuned.
A visual artist has concepts like arcs, spirals, symmetries, and perspectives;
a physicist has concepts like dot products, vector fields, and conservation laws;
and an architect has concepts like arches, supports, and bridges.
Second, experts have implicit, procedural skill in deploying those concepts
to quickly solve new problems:
at a glance, human domain
experts can intuit which compositions of concepts are likely to solve
the task at hand, even before searching for a solution.
Studies of human expertise
find that,
compared to novices,
experts can quickly categorize and classify problems based on the ``deep structure''
of the problem's solution~\cite{chi1981expertise,chi2012seeing}.
In short, we take the stance that expertise means both
having the right explicit concepts, and being able to quickly see how those concepts can be composed into a solution.

Because solutions to many kinds of problems can often be described as
some kind of
program~\cite{lake2015human,Ullman2012,DBLP:journals/cogsr/SchmidK11},
our model solves a specific problem by synthesizing a program solving
it. It gradually grows its explicit domain-specific knowledge for a
class of problems by assembling a library of code containing concepts
useful for the domain, and by training a neural network to quickly
infer, for a specific problem, what code in its library is likely to
solve it. We think of these two learned components as analogous to the
explicit concepts and implicit procedural skill that human experts
develop.  The model is an instance of what in the machine learning
community is called a wake-sleep algorithm, where for us, `waking'
corresponds to solving problems, while a pair of `sleep' cycles
correspond to improving the library of explicit concepts and training
the neural net to improve implicit procedural knowledge of how to
write code.


We will first investigate our model within classic program
synthesis domains for manipulating sequences of numbers and text, and
then consider visual and creative programs for drawing pictures and
building towers out of toy blocks, and finally consider programs for basic kinds of equation discovery and programming language design, altogether considering both
deterministic and probabilistic programs that act both generatively
(e.g., producing an artifact like an image or plan) and conditionally
(e.g., mapping inputs to outputs).

Our model iteratively creates new library routines that build on
concepts learned earlier in its learning trajectory, growing a library with nested
hierarchies of code.  We think of this cumulative nesting of
abstractions as a variety of deep representation
learning~\cite{lecun2015deep}.  Figure~\ref{exampleDSL} diagrams a
subset of these learned networks (the library). For example, the model
learns to sort sequences of numbers by invoking a library component 4
layers deep, or draws the leftmost pairs of images in
Figure~\ref{exampleDSL} using a depth-3 component.  For this reason we
refer to our approach as an instance of `deep program learning'.

\begin{figure}
  \includegraphics[width = \textwidth]{figures/dpl/list}
  \includegraphics[width = \textwidth]{figures/dpl/logo}
  \caption{}\label{exampleDSL}
\end{figure}






%% \section{Old introduction}

%% An old dream within AI is a machine that learns and reasons by writing
%% its own programs.  This vision stretches back to the
%% 1960's~\cite{solomonoff1964formal} and, if fully realized, could bring
%% us much closer to machines that learn and think like humans.
%% Computational models of cognition often explain the flexibility and
%% richness of human thinking in terms of program learning: from everyday
%% thinking and problem solving (motor program induction as an account of
%% recognition and generation of handwriting and
%% speech~\cite{lake2015human}; functional programs as a model of natural
%% language semantics~\cite{SOMETHING}) to learning problems that unfold
%% over longer developmental time scales: the child's acquisition of
%% intuitive theories (of kinship, taxonomy, etc.)~\cite{Ullman2012} and
%% natural language grammar~\cite{DBLP:journals/cogsr/SchmidK11}, to name
%% just a few.  An outstanding challenge, however, is to engineer
%% program-learners that display the same level of domain-generality as
%% the humans they are meant to model.

%% Recent program-learning systems developed within the AI and machine
%% learning community are impressive along many dimensions, authoring
%% programs for problem domains like drawing
%% pictures~\cite{spiral,ellis2017learning}, transforming
%% text~\cite{gulwani2011automating} and numerical
%% sequences~\cite{balog2016deepcoder}, robot motion
%% planning~\cite{devlin2017neural}, and reasoning over common sense
%% knowledge bases~\cite{muggleton2015meta}.  These systems work in
%% different ways, but typically hinge upon a carefully hand-engineered
%% Domain Specific Language (DSL).  The DSL restricts the space of
%% programs to contain the kinds of concepts needed for one specific
%% domain.  For example, a picture-drawing DSL could include concepts
%% like circles and spirals, and a DSL for numerical sequences could
%% include sorting and reversing lists of numbers.  Modern systems also
%% learn how to efficiently deploy the DSL on new
%% problems~\cite{devlin2017robustfill,balog2016deepcoder,NGDS}, but --
%% unlike human learners -- do not discover the underlying system of
%% concepts needed to navigate the domain.

%% We contribute a program-induction system that learns the
%% domain-specific concepts (DSL) while jointly learning how to use those
%% concepts.  This joint learning problem models two complementary
%% notions of domain expertise: domain experts have at their disposal
%% a powerful, yet specialized repertoire of concepts and abstractions
%% (analogous to the DSL) while also having accurate intuitions about
%% when and how to use those concepts to solve new problems.
%% Representative domains,
%% along with DSLs we learn for them,
%% are shown in Figure~\ref{exampleDSL}.

%% We call our system `DreamCoder' because it acquires these two kinds of
%% expertise through a novel kind of wake/sleep or `dream'
%% learning~\cite{hinton1995wake}, iterating through a wake cycle --
%% where it solves problems by writing programs -- and a pair of sleep
%% cycles, both of which are loosely biologically inspired by actual
%% sleep.  The first sleep cycle, which we refer to as
%% \textbf{consolidation}, grows the DSL by replying experiences from
%% waking and consolidating them into new code abstractions.  This cycle
%% is inspired by the formation of abstractions during sleep memory
%% consolidation~\cite{DUDAI201520}.  The second sleep cycle, which we
%% refer to as \textbf{dreaming}, improves the agents knowledge of how to
%% write code by training a neural network to help search for
%% programs. The neural net is trained on replayed experiences as well as
%% `fantasies', or samples, from the DSL.  These two kinds of dreams are
%% inspired by the distinct episodic replay and hallucination components
%% of dream sleep~\cite{fosse2003dreaming}.


%% This dream-learning architecture brings together two lines of prior work,
%% both of which have been separately influential within
%% artificial intelligence.
%% One line of work
%% considers the problem of learning new concepts, abstractions, or
%% `options' from
%% experience~\cite{Dechter:2013:BLV:2540128.2540316,DBLP:conf/icml/LiangJK10,solomonoff1989system,DBLP:journals/corr/abs-1110-5667,stolle2002learning},
%% while the other line of work considers the problem of learning how to
%% deploy those concepts
%% efficiently~\cite{devlin2017robustfill,balog2016deepcoder,NGDS}.  Our
%% goal with DreamCoder is to show that the combination of these ideas is
%% uniquely powerful, and pushes us toward program-writing systems that,
%% like human learners, autonomously acquire the expertise needed to
%% navigate a new domain of problems.


%% Each wake/sleep cycle creates new DSL components that build on
%% components learned in earlier sleep cycles, growing a DSL with nested 
%% hierarchies of code.  We identify this cumulative nesting of
%% abstractions as a variety of deep representation
%% learning~\cite{lecun2015deep}.  Figure~\ref{exampleDSL} diagrams a
%% subset of these learned networks (the DSL). For example, the system
%% learns to sort sequences of numbers by invoking a DSL component 4
%% layers deep, or draws the leftmost pairs of images in
%% Figure~\ref{exampleDSL} using a depth-3 component.  For this reason we
%% refer to \system as an instance of `deep program learning'.


%% Our goal with \system is to engineer a system that develops domain
%% expertise in humanlike ways. This involves learning both declarative and
%% procedural knowledge, like a DSL and a recognition model, but includes
%% other features of the development of expertise:
%% humans can become experts in many fields,
%% and so we evaluate our algorithm across six different problem domains;
%% a human expert doesn't become an expert overnight,
%% and needs more than a handful of example problems to learn from,
%% but doesn't need millions of examples;
%% similarly our algorithm's
%% learning trajectory unfolds over a series of wake/sleep cycles
%% requiring around a hundred problems per domain.


%\input{dpl1.tex} % old latex deep programming figures

\section{Deep Wake/Sleep Program Induction}\label{overviewSection}

Our model, called \systemEnding, works through a novel kind
of wake/sleep or `dream' learning~\cite{hinton1995wake}, iterating
through a wake cycle -- where it solves \textbf{tasks} by writing programs (Figure~\ref{threeCycles} top)
-- and a pair of sleep cycles, both of which are loosely biologically
inspired. The first sleep cycle, which we refer to as
\textbf{consolidation}, grows its library of code by replying
experiences from waking and consolidating them into new code
abstractions (Figure~\ref{threeCycles} left).  This cycle is inspired by the formation of abstractions
during sleep memory consolidation~\cite{DUDAI201520}.  The second
sleep cycle, which we refer to as \textbf{dreaming}, improves the
agents knowledge of how to write code by training a neural network to
help quickly search for programs. The neural net is trained on replayed
experiences as well as `fantasies', or samples, from the learned
library (Figure~\ref{threeCycles} right).  These two kinds of dreams are inspired by the distinct
episodic replay and hallucination components of dream
sleep~\cite{fosse2003dreaming}.

Viewed as a probabilistic inference problem, \system observes a set of
tasks, written $X$, and infers both a program solving each task, as
well as a prior distribution over programs likely to solve tasks in
the domain (Figure~\ref{threeCycles} middle). This prior is encoded by
a library, written $\mathcal{D}$, which combined with a
learned weight vector $\theta$
defines a generative model over programs
(Appendix~\ref{sampleProgram}).  The neural network learns to invert
this generative model by predicting, conditioned on a task, a
``posterior'' distribution over programs likely to solve that specific
task,
thus functioning as a \textbf{recognition model} and trained jointly with the generative model, in the spirit of the Helmholtz machine~\cite{hinton1995wake}.
Writing $Q(p|x)$ for the approximate
posterior predicted by the recognition model, wake/sleep cycles correspond to iteratively (and
approximately) solving for

\begin{array}{lr}
  \begin{array}{l}  p_x = \argmax_p \probability[x|p]\probability[p|\mathcal{D}^*,\theta^*]\text{, for each task $x\in X$}\end{array}
    &\text{\emph{Wake}}\\\\
\begin{array}{rl}  
\mathcal{D}^* = \argmax_\mathcal{D}&\int\probability\left[\mathcal{D},\phantom{^*}\theta \right]\prod_{x\in X}\sum_{\substack{p\text{ found during waking}}} \probability[x|p]\probability[p|\mathcal{D},\phantom{^*}\theta]\;\text{d}\theta\\
\theta^* = \argmax_\theta&\phantom{\int}\probability\left[\mathcal{D}^*,\theta \right]\prod_{x\in X}\sum_{p\text{ found during waking}} \probability[x|p]\probability[p|\mathcal{D}^*,\theta]
 \end{array}
&\text{\emph{Consolidation sleep}}\\\\
\begin{array}{l} Q(p|x)\approx \probability[p|x,\mathcal{D}^*,\theta^*]\end{array}
  &\text{\emph{Dream sleep}}
\end{array}

\noindent which serves to maximize a lower bound on the posterior over $(\mathcal{D},\theta)$ given $X$ (Appendix~\ref{probabilisticAppendix}).



%% The model inputs a collection of programming \textbf{tasks}, and then
%% alternatingly finds programs that solve tasks (Wake --
%% Figure~\ref{threeCycles} top); improves its library by analyzing programs
%% found during waking (Consolidation -- Figure~\ref{threeCycles} left);
%% and trains a neural network that efficiently guides search for
%% programs written using the library (Dreaming -- Figure~\ref{threeCycles} right).  The
%% learned library acts as a a prior on programs likely to solve tasks in the
%% domain, while the neural net looks at a specific task and produces a
%% ``posterior'' for programs likely to solve that specific task
%% (Figure~\ref{threeCycles} middle).  The neural network thus functions
%% as a \textbf{recognition model} supporting a form of approximate
%% Bayesian program induction, jointly trained with a \textbf{generative
%%   model} for programs encoded in the DSL, in the spirit of the
%% Helmholtz machine~\cite{hinton1995wake}. The recognition model ensures
%% that searching for programs remains tractable even as the library (and
%% hence the search space for programs) expands.  The generative model,
%% or library, distills out common abstractions across programs discovered
%% during waking, growing a network of increasingly deep and specialized
%% domain-specific concepts (Figure~\ref{exampleDSL}, bottom rows).

%% Formally, $EC^2$ takes as input a collection of problems, written $X$, which can be solved by programs, and infers a domain specific language -- written $\mathcal{D}$ -- which functions as a generative model over programs likely to solve problems in the domain, and which we identify as an expert's a priori beliefs about what good solutions look like. Alongside this generative model it trains a neural recognition model, in the spirit of the Helmholtz machine or wake-sleep algorithm~\cite{dayan1995helmholtz}. The recognition model takes as input a specific problem and quickly predicts an approximate posterior over programs likely to solve that problem. Writing $p_x$ for the program solving problem $x\in X$ and $Q(p|x)$ for the distribution predicted by the recognition model, $EC^2$ iteratively (and approximately) solves for
%% \begin{align}
%% p_x& = \argmax_p \probability[x|p]\probability[p|\mathcal{D}]\label{wakingObjective}\\
%% \mathcal{D}& = \argmax_\mathcal{D}\probability\left[\mathcal{D} \right]\prod_{x\in X}\sum_{\substack{p:\\p\text{ found during Explore}}} \probability[p|\mathcal{D}]\label{sleepg}\\
%% Q(p|x)&\approx \probability[p|x,\mathcal{D}]\label{dreamingObjective}
%% \end{align}




This 3-phase inference procedure works through two distinct kinds of
bootstrapping.  During each sleep cycle the next library bootstraps
off the concepts learned during earlier cycles, growing an
increasingly deep learned program representation.  In tandem the
generative and recognition models bootstrap each other: a more finely
tuned library of concepts yields richer dreams for the recognition
model to learn from, while a more accurate recognition model solves
more tasks during waking which then feed into the next library.

Waking consists of searching for task-specific programs with high
posterior probability, or programs which are a priori likely and which
solve a task. During a Wake cycle we sample a minibatch of tasks and find programs solving specific task by enumerating
programs in decreasing order of their probability under
the recognition model,  then checking if a program $p$ assigns
positive probability to a task ($\probability[x|p] > 0$). We represent
programs as polymorphicly typed $\lambda$-calculus expressions, an
expressive formalism including conditionals, variables, higher-order
recursive functions, and the ability to define new functions.

\section{Related work}

We build on several generations of research in AI, program synthesis,
and cognitive science, with program induction being one of the oldest
theoretical frameworks for concept learning within artificial
intelligence~\cite{solomonoff1964formal}, and the conceptually allied
`Language of Thought Hypothesis' being almost as
old~\cite{fodor1975language}. Recent neural program synthesis systems
pair a fixed domain-specific programming language (a `DSL') to a
learned neural network that guides program
search~\cite{spiral,balog2016deepcoder,devlin2017neural}, while recent
symbolic AI research has developed frameworks for learning the DSL by
inferring reusable pieces of
code~\cite{ecc,Dechter:2013:BLV:2540128.2540316,DBLP:conf/icml/LiangJK10,DBLP:conf/ecai/LinDETM14}.
A main goal of this work is to combine and refine these techniques
with the intention of building agents that, like humans, develop
domain expertise for new classes of problems.  Work in cognitive
modeling has often exploited program-like representations for both
concept~\cite{piantadosi2011learning,lake2015human,GoodmanEtAl2015-Chapter} and intuitive theory
learning~\cite{logical,Ullman2012},
and we view these models of concept learning as
a kind of program induction,
and see theory learning as similar to inducing a DSL or library.

Deep learning is a dominant machine intelligence paradigm that
has had many successes
at creating machines with impressive expertise at
specific tasks in vision, natural language, gameplaying, and many other areas. ideas: we want to become an expert from modest amounts of data; we want to be able to transfer to different types of problems, and to do this zero shot; 





\begin{figure}
  \begin{tikzpicture}[line width=0.4mm]
    \draw[fill=teal!5!white] (-1,1) -- (13,1) -- (13,-4) -- (-1,-4) -- (-1,1);
    \node at (5.5,1.25) {\textsc{\textbf{Wake}}};

    
    \begin{scope}[shift={(0.5,0.5)}]
      \node[align=right] at (0,-0.5) (d){\textbf{Library}\\
        $f_1$, $f_2$, ...};
      \node[align=center] at ([yshift = -2cm]d) (t){\textbf{Task}\\
        \footnotesize            \code{[7\, 2\, 3]}$\to$\code{[7\, 3]}         \\
        \footnotesize    \code{[4\, 3\, 2\, 1]}$\to$\code{[4\, 3]} };

      \node at ([xshift = 1.25cm]t.east) (nn){\NeuralNetwork{0.25}};
      \node[align = center, text width = 1cm] at ([yshift = 0.7cm,xshift=0cm]nn.north) {\baselineskip=0pt \small Recog. model\par};
      \draw [red,-{>[scale=0.2]}] (t.east) -- ([xshift = -0.5cm]nn.west);

      \node[draw,rounded corners, align=center, inner sep = 10] at ([xshift = 4.2cm,yshift = 1cm]t.east) (s){Neurally-Guided\\ Enumerative Search};

      \draw [red,->] ([xshift = 0.5cm]nn.east) -- ([yshift = -0.25cm]s.west);
      \draw [->,rounded corners,] (d.east) -- ([yshift = 2cm]nn.center) -- ([yshift = 0.25cm]s.west);

      \node[align=left] at ([xshift=3cm]s.east) (f) {\textbf{Programs:}\\
        \small    \code{($f_1$ $\ell$ ($\lambda$ (x) (> x 2)))}\\
        \small $\cdots\cdots\cdots$};
      \draw [->  ] (s.east) -- (f.west);

      \draw [->  ,rounded corners] (t.south) -- ([yshift = -0.5cm]t.south) -- ([yshift = -0.5cm] s.south |- t.south) -- (s.south);
    \end{scope}
    \begin{scope}[shift={(9.4,-3.5)},scale=0.6,line width=0.05mm]
      \node[obs,scale=0.7] at (3.5,3) (dx){Library};
      \node[latent,scale=0.7] at ([yshift=-1.7cm,xshift=0cm]dx) (zp){prog};
      \node[obs,scale=0.7] at ([yshift=-1.45cm]zp) (xp) {task};
      \node[latent,scale=0.7] at ([xshift=1.5cm]zp) (zp1){prog};
      \node[obs,scale=0.7] at ([xshift=1.5cm]xp) (xp1) {task};
      \draw [->] (zp1.south) -- (xp1.north);
      \draw [->] (dx.south) -- (zp1.north);
      \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);
      \node[latent,scale=0.7] at ([xshift=-1.5cm]zp) (zp1){prog};
      \node[obs,scale=0.7] at ([xshift=-1.5cm]xp) (xp1) {task};
      \draw [->] (zp1.south) -- (xp1.north);
      \draw [->] (dx.south) -- (zp1.north);
      \draw [->] (dx.south) -- (zp.north);
      \draw [->] (zp.south) -- (xp.north);
      \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);
      \draw [->,red] (xp.east) to[out = 30,in = -30] node(nn){} (zp.east);
    \end{scope}


    \node at (0,-4.75) {\textbf{\textsc{Sleep: Consolidation}}};
    \draw[fill=teal!5!white] (-3,-5) -- (3,-5) -- (3,-10) -- (5.5,-10) -- (5.5,-13) -- (-3,-13) -- (-3,-5);
    \node at (12,-4.75) {\textbf{\textsc{Sleep: Dreaming}}};
    \draw[fill=teal!5!white] (15,-5) -- (9,-5) -- (9,-10) -- (6.5,-10) -- (6.5,-13) -- (15,-13) -- (15,-5);

    \begin{scope}[shift={(9.5,-4.5)}]
      \node(dreaming) at (1,-1) {\underline{Fantasies}};
      \node[anchor=center] at ([yshift=-0.5cm]dreaming.south) (d){\textbf{Library}};
      \node at ([yshift=-1.75cm]d.south) (p1){program};
      \draw[squiggle,-> ] (d.south) -- node[sloped,above]{\small sample} (p1.north);

      \node(replay) at ([xshift=2cm]dreaming.east) {\underline{Replays}};
      \node[anchor=center,align=center] at ([yshift=-0.5cm]replay.south) (d){\textbf{progs. for task}};
      \node at ([yshift=-1.75cm]d.south) (p1){program};
      \draw[squiggle,-> ] (d.south) -- node[sloped,above]{\small sample} (p1.north);

      \node(p1) at (1.5,-6) {program};      
      \node at ([xshift = 2.0cm]p1.east) (t1){ task};
      \draw [-> ] (p1.east) -- node[above]{\small run} (t1.west);
      \node(n) at ([yshift=-1.2cm,xshift=1.25cm]p1.south) {
        \NeuralNetwork{0.17}};
      \draw [->,red] (t1.south) to[out = -90,in = 0]  ([xshift=0.4cm]n.east);
      \draw [dashed] (p1.south) to[out=-120,in=180] node[above,fill=white]{\color{black}Loss} ([xshift=-0.4cm]n.west);


      \node at ($(-0.25,0.5) + (p1.north)!0.5!(t1.north)$) {\underline{Train recognition model}};

      %% \node at ([xshift = 1.5cm]p1.east) (t1){ task};
      %% \draw [-> ] (p1.east) -- node[above]{\small run} (t1.west);
      %% \node(n) at ([yshift=-1.2cm,xshift=0.4cm]p1.south) {
      %%   \NeuralNetwork{0.17}};
      %% \draw [->,red] (t1.south) to[out = -90,in = 0]  ([xshift=0.4cm]n.east);
      %% \draw [dashed] (p1.south) to[out=-120,in=180] node[above,fill=white]{\color{black}Loss} ([xshift=-0.4cm]n.west);

      \begin{scope}[shift={(-3.8,-8)},scale=0.6,line width=0.05mm]
        \node[obs,scale=0.7] at (3.5,3) (dx){Library};
        \node[obs,scale=0.7] at ([yshift=-1.7cm,xshift=0cm]dx) (zp){prog};
        \node[obs,scale=0.7] at ([yshift=-1.45cm]zp) (xp) {task};
        \node[obs,scale=0.7] at ([xshift=1.5cm]zp) (zp1){prog};
        \node[obs,scale=0.7] at ([xshift=1.5cm]xp) (xp1) {task};
        \draw [->] (zp1.south) -- (xp1.north);
        \draw [->] (dx.south) -- (zp1.north);
        \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);
        \node[obs,scale=0.7] at ([xshift=-1.5cm]zp) (zp1){prog};
        \node[obs,scale=0.7] at ([xshift=-1.5cm]xp) (xp1) {task};
        \draw [->] (zp1.south) -- (xp1.north);
        \draw [->] (dx.south) -- (zp1.north);
        \draw [->] (dx.south) -- (zp.north);
        \draw [->] (zp.south) -- (xp.north);
        \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);
        \draw [->,red] (xp.east) to[out = 30,in = -30] node(nn){} (zp.east);
      \end{scope}


      \end{scope}

    % memory consolidation
    \begin{scope}[shift={(-2,-4.5)}]

      %% defined routines for creating fragmented syntax trees
      \newcommand{\syntaxOne}[1]{
        \begin{tikzpicture}[scale=#1,line width=0.35mm]          
          \node(l1) at (0,0) {};
          \node[color=pop3](p1) at (-1,-1) {\texttt{+}};
          \node[color=pop3](n1) at (0.7,-0.9) {\texttt{1}};
          \node(x1) at (0,-1) {\texttt{1}};
          \draw[color=pop3] (l1.south) -- (p1.north);
          \draw[color=pop3] (l1.south) -- (n1.north);
          \draw[color=pop3] (-0.5,-0.45) -- (x1.north);

          \node(t) at (-0.5,0.5) {};
          \draw (l1.south) -- (t.south);
          \node(c) at (-1.5,-0.2) {\texttt{cons}};
          \draw (t.south) -- (c.north);
        \end{tikzpicture}
      }
      \renewcommand{\syntaxTo}[1]{
        \begin{tikzpicture}[scale=#1,line width=0.35mm]          
            \node(l1) at (0,0) {};
            \node[color=pop3](p1) at (-1,-1) {\texttt{+}};
            \node[color=pop3](n1) at (0.7,-0.9) {\texttt{1}};
            \draw[color=pop3] (l1.south) -- (p1.north);
            \draw[color=pop3] (l1.south) -- (n1.north);
            \draw[color=pop3] (-0.5,-0.45) -- (0,-1);
            \node(c) at (-0.5,-1.5) {\texttt{car}};
            \node(z) at (0.5,-1.5) {\texttt{z}};
            \draw (0,-1) -- (c.north);
            \draw (0,-1) -- (z.north);
        \end{tikzpicture}
      }

      \node[align=center,anchor=center] at (0.4,-1.2) (f1){\textbf{Progs. for Task 1}:\\\code{(+ (car z) 1)}};
      \node[align=center] at ([xshift = 1.75cm]f1.east) (f2){\textbf{Progs. for Task 2}:\\\code{(cons (+ 1 1))}};
      \node(s1) at ([yshift=-0.5cm]f1.south) {\syntaxOne{0.8}};
      \node(s2) at ([yshift=-0.5cm]f2.south) {\syntaxTo{0.8}};
      \node(c)[align=center,rectangle, rounded corners, draw, minimum width = 4cm, minimum height = 1.5cm, anchor = north] at ($(s1.south)!0.5!(s2.south) + (0,-1)$) {Refactoring Algorithm};
      \draw [-> ] (s1.south) -- (s1.south|-c.north);
      \draw [-> ] (s2.south) -- (s2.south|-c.north);

      
      \node(d) at ([yshift = -1.8cm]c.south) {
        \begin{tikzpicture}[scale=0.9,line width=0.5mm]
          \node[align=center] at (0,0) {\textbf{new Library} w/ \texttt{(+ x 1)}:};
          \begin{scope}[shift={(0.6,-0.5)}]
            \node[pop3](p1) at (-1,-1) {\texttt{+}};
            \node[pop3](n1) at (0.6,-0.9) {\texttt{1}};
            \node[pop3](a) at (0,-1) {\texttt{ }};
            \draw[pop3] (0,0) -- (p1.north);
            \draw[pop3] (0,0) -- (n1.north);
            \draw[pop3] (-0.3,-0.3) -- (a.north);
          \end{scope}
      \end{tikzpicture}};
      \draw [-> ] (c.south) -- (d.north);



      \begin{scope}[shift={(4,-8)},scale=0.6,line width=0.05mm]
        \node[latent,scale=0.7] at (3.5,3) (dx){Library};
        \node[obs,scale=0.7] at ([yshift=-1.7cm,xshift=0cm]dx) (zp){prog};
        \node[obs,scale=0.7] at ([yshift=-1.45cm]zp) (xp) {task};
        \node[obs,scale=0.7] at ([xshift=1.5cm]zp) (zp1){prog};
        \node[obs,scale=0.7] at ([xshift=1.5cm]xp) (xp1) {task};
        \draw [->] (zp1.south) -- (xp1.north);
        \draw [->] (dx.south) -- (zp1.north);
        \node[obs,scale=0.7] at ([xshift=-1.5cm]zp) (zp1){prog};
        \node[obs,scale=0.7] at ([xshift=-1.5cm]xp) (xp1) {task};
        \draw [->] (zp1.south) -- (xp1.north);
        \draw [->] (dx.south) -- (zp1.north);
        \draw [->] (dx.south) -- (zp.north);
        \draw [->] (zp.south) -- (xp.north);
      \end{scope}


      \end{scope}


    
    %% center spiral
    \begin{scope}[shift={(3.25,-7.9)},scale=0.8]    
      \spiral{(3.5,1)}{3.5}
      \node[latent,scale=1] at (3.5,3) (dx){Library};
      \node[latent,scale=1] at ([yshift=-1.7cm,xshift=0cm]dx) (zp){prog};
      \node[obs,scale=1] at ([yshift=-1.45cm]zp) (xp) {task};
      \node[latent,scale=1] at ([xshift=2cm]zp) (zp1){prog};
      \node[obs,scale=1] at ([xshift=2cm]xp) (xp1) {task};
      \draw [->] (zp1.south) -- (xp1.north);
      \draw [->] (dx.south) -- (zp1.north);
      \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);
      
      \node[latent,scale=1] at ([xshift=-2cm]zp) (zp1){prog};
      \node[obs,scale=1] at ([xshift=-2cm]xp) (xp1) {task};
      \draw [->] (zp1.south) -- (xp1.north);
      \draw [->] (dx.south) -- (zp1.north);
      \draw [->,red] (xp1.east) to[out = 30,in = -30] node(nn){} (zp1.east);


      \draw [->,red] (xp.east) to[out = 30,in = -30] node(nn){} (zp.east);
      \draw [->] (dx.south) -- (zp.north);
      \draw [->] (zp.south) -- (xp.north);

      \node at ([yshift=-0.6cm]xp.south) {\legend};

    \end{scope}
    
  \end{tikzpicture}
  \caption{\textbf{Middle:} \system as a graphical model. Agent observes programming tasks (e.g., input/outputs for list processing or images for graphics programs), which it explains with latent programs, while jointly inferring a latent Domain Specific Language (DSL) capturing cross-program regularities. A neural network, called the \emph{recognition model} (red arrows) is trained to quickly infer programs with high posterior probability. \textbf{Top}: Wake phase infers programs while holding the DSL and recognition model fixed. \textbf{Left}: Sleep (Consolidation) phase infers DSL while holding the programs fixed by refactoring programs found during waking and extracting common components. \textbf{Right}: Sleep (Dreaming) phase trains recognition model to predict approximate posterior over programs conditioned on task. Trained on `Fantasies' (programs sampled from DSL) \& `Replays' (programs found during waking).}\label{threeCycles}
\end{figure}



%% Our programs are all
%% strongly typed.  We use the Hindley-Milner polymorphic typing
%% system~\cite{pierce} which is used in functional programming languages
%% like OCaml and Haskell.  We now define DSLs:
%% \begin{definition}
%% A DSL $\mathcal{D}$ is a set of typed $\lambda$-calculus expressions.
%% A weight vector $\theta$ for a DSL $\mathcal{D}$ is a vector of $|\mathcal{D}| + 1$ real numbers:
%% one number for each DSL element $e\in \mathcal{D}$, written $\theta_e$ and controlling the probability of  $e$ occurring in a program,
%% and a weight controlling the probability of a variable occurring in a program, $\theta_{\text{var}}$.
%% \end{definition}
%% \noindent Together with its weight vector, a DSL defines a distribution over
%% programs, $\probability[p|\mathcal{D},\theta]$.  We define this
%% distribution by specifying a procedure for drawing samples from
%% $\probability[p|\mathcal{D},\theta]$ (Appendix~\ref{generativeAppendix}).
%% Care must be taken to ensure that variable scoping rules are obeyed
%% and that programs are well-typed.  We ensure well-typed programs by
%% performing Hindley-Milner type inference~\cite{pierce} during
%% sampling, and assume that each task is annotated with the type of the
%% program that will solve it.
%% Appendix~\ref{enumerationAppendix}
%% explains how we enumerate,
%% rather than sample,
%% programs generated by Algorithm~\ref{sampleProgram}.

%% \begin{comment}

%%   \subsection{Wake: Solving tasks}\label{explorationSection}

%% During waking we enumerate programs from the DSL in decreasing order
%% of their probability according to the recognition model, and then
%% check if a program $p$ assigns positive probability to a task
%% ($\probability[x|p] > 0$); if so, we incorporate $p$ into the beam
%% $\mathcal{B}_x$. We represent programs as polymorphicly-typed
%% $\lambda$-calculus expressions, a representation closely resembling
%% Lisp and functional languages like Haskell and OCaml, including
%% variables, conditionals, higher-order recursive functions, and the
%% ability to create new functions.  We `batch' the tasks by randomly
%% shuffling the training set and playing small minibatches of tasks to
%% the agent during each wake cycle.


%% Why enumerate, when the program synthesis community has invented many
%% sophisticated algorithms that search for programs?~\cite{solar2008program,schkufza2013stochastic,feser2015synthesizing,osera2015type,polozov2015flashmeta}.
%% We have two reasons:
%% (1) A key point of our work is that learning the DSL, along with a neural recognition model, can make program induction tractable, even if the search algorithm is very simple.
%% (2) Enumeration is a general approach that can be applied to any program induction problem. Many of these more sophisticated approaches require special conditions on
%% the space of  programs.
%% \end{comment}
%% \begin{comment}
%%   \begin{figure}
%%     \begin{tikzpicture}[every node/.style={rounded corners,thick}]
%%       \node(x)[draw,align=center] at (0,0) {Task $x$\\e.g., input/outputs};
%%       \node(e)[draw, align=center] at ([xshift=3cm]x.east) {Domain-specific encoder\\
%%         e.g., RNN or CNN};
%%       %    \node(m)[draw,align = center] at ([xshift=1.1cm]e.east) {MLP};%\\Output: $(|\mathcal{D}| + 2)\times(|\mathcal{D}| + 1)\times A$};
%%       \node[draw,align = center](s) at ([xshift=2cm]e.east) {Enumerate\\(Algorithm~\ref{recognitionSample})};
%%       \node(f)[draw,align = center] at ([xshift=2cm]s.east) {Beam\\$\mathcal{B}_x$};

%%       \draw[->] (x.east) -- (e.west);
%%       \draw[->] (e.east) -- (s.west);
%%       %    \draw[->] (m.east) -- node(p)[fill=white,align=center,midway,inner sep=0,outer sep=0,rotate=90]{\small $Q_{ijk}(x)$}(s.west);
%%       \draw[->] (s.east) -- (f.west);
%%       \draw[->] (x.north) -- ([yshift=15]x.north) -- node[above = 0pt,midway]{\small likelihood $\probability[x|p]$}([yshift=15,xshift=40]s.north) -- ($(s.east)!0.5!(f.west)$);
%%       \node[align=center] at ($(0,-1) + (e.center)$) {$\underbrace{\hspace{4cm}}_{\normalsize\text{\normalsize Recognition model $Q(p|x)$}}$};
%%       \node[align=center] at ($(s.west)!0.5!(f.east) + (0,-1)$) {$\underbrace{\hspace{5cm}}_{\text{\normalsize Search}}$};
%%     \end{tikzpicture}
%%     \caption{Neurally-guided program inference pipeline. Recognition model outputs distribution over program $Q(p|x)$. Program output by enumerative search incorporated into beam if likelihood $\probability[x|p] > 0$}
%%     \label{neuralPipeline}
%%   \end{figure}
%% \end{comment}

%%   However, a drawback of   enumerative search  is that we have no
%% efficient means of solving for arbitrary constants that might occur in a
%% program. In Sec.~\ref{regressionSection},
%% we will show how to find programs with real-valued constants
%% by automatically differentiating through the program and setting the constants using gradient descent.
%% In Sec.~\ref{textSection}
%% we will show that the bottom-up neural recognition model can learn
%% which discrete constants should be included in a program.




\subsection{Consolidation-Sleep: Growing a Domain Specific Language}\label{consolidationSection}

The DSL offers a set of abstractions that allow an agent to concisely
express solutions to the tasks at hand. We automatically discover
these new abstractions by combining two ideas. First, we build on
techniques from the programming languages community to develop a new
algorithm for automatically refractoring programs, where this
refactoring exposes common reused subexpressions across the
programs found during waking.  Second, we use this automatic refactoring
process to search for DSLs that maximally compress these programs by
incorporating reused subexpressions into the DSL.

Mathematically this compression takes the form of
finding the DSL maximizing $\int \probability[\mathcal{D},\theta]\probability[X|\mathcal{D},\theta]\;\mathrm{d}\theta$ (Sec.~\ref{overviewSection}).
We replace this marginal with an AIC approximation~\cite{akaike1998information}
and marginalize over refactorings of programs found during waking, minimizing the following expression,
which can be interpreted as a kind of compression:
\begin{equation}
\underbrace{-\log \probability[\mathcal{D}] + \min_{\substack{\theta\\\\\\\\}}\Bigg(-\log
\probability[\theta|\mathcal{D}] + \|\theta\|_0}_{\text{Description length of }(\mathcal{D},\theta)} +\sum_{x\in
  X}\underbrace{-\log \sum_{\substack{p \text{ a refactoring of } p'\\ p'\text{ found during waking}}}\probability[x|p]\probability[p|\mathcal{D},\theta]}_{\text{Description length of programs for task }x}\Bigg)
\label{AIC}
\end{equation}
But a program has infinitely many possible refactorings, rendering
Eq.~\ref{AIC} intractable.  Rather than consider every refactoring we
bound the number of $\lambda$-calculus evaluation steps separating a
program from its refactoring.  Now the number of refactorings is
finite but astronomically large: Figure~\ref{mapFactor}A diagrams a
problem where the agent rediscovers the higher-order function
\code{map} starting from the basics of Lisp and the Y-combinator, but
where there are approximately $10^{14}$ possible refactorings -- a quantity
that grows exponentially both as a function of program size and a
function of the bound on evaluation steps. How can we tame this
combinatorial explosion?

To resolve this exponential growth we introduce a new data structure
combining ideas from version space
algebras~\cite{lau2001programming,mitchell1977version,polozov2015flashmeta}
and equivalence graphs~\cite{tate2009equality}. A version space is a
tree-shaped data structure that compactly represents a large set of
programs and supports efficient set operations like union,
intersection, and membership checking, while equivalence graphs are
data structures that track semantic equivalences between program
subexpressions.  In Appendix~\ref{appendixVersion}, we give a dynamic
program that takes as input a program and then outputs a version space
containing its refactorings while tracking semantically equivalent
subexpressions. Figure~\ref{mapFactor}B diagrams a subtree of a
version space containing refactorings of a small program. Our
technique is substantially more efficient than explicitly representing
the space of possible refactorings: for the example in
Figure~\ref{mapFactor}A, we represent the space of refactorings using
a version space with $10^6$ nodes, which encodes $10^{14}$
refactorings. Appendix~\ref{appendixCompression} specifies how we
combine this probabilistic and symbolic machinery to update the DSL.
At a high level, our approach is to search locally through the space
of DSLs, proposing small changes until Eq.~\ref{AIC} fails to
decrease.




%% However, there is a snag with this simple approach:
%% whenever we add a new expression $e$ to the DSL,
%% the programs found during waking
%% are not written in terms of $e$ ---
%% and so we must \emph{refactor}
%% the programs in terms of the new DSL component.
%% Concretely, imagine we wanted to discover a new DSL procedure for doubling numbers,
%% after having found the programs
%% \code{(cons (+ 9 9) nil)} and \code{($\lambda$ (x) (+ (car x) (car x)))}.
%% As human programmers,
%% we can look at these pieces of code and recognize that,
%% if we define a new procedure called \code{double},
%% defined as \code{($\lambda$ (x) (+ x x))},
%% then we can rewrite the original programs as
%% \code{(cons (double 9) nil)} and \code{($\lambda$ (x) (double (car x)))}.
%% This process is a kind of refactoring
%% where a new subroutine is defined (\code{double})
%% and the old programs rewritten
%% in terms of the new subroutine.
%% Figure~\ref{mapFactor}A diagrams a refactoring process where the agent
%% must rediscover the higher-order function \code{map} starting from the
%% basics of Lisp and the Y-combinator.

%% We refine our objective to compress
%% \emph{refactorings} of the programs found during waking, minimizing
%% \begin{equation}
%%         -\log \probability[\mathcal{D}] + 
%% \min_{\theta}\Bigg(-\log \probability[\theta|\mathcal{D}] + \|\theta\|_0 +\sum_{x\in X} - \log \underbrace{\sum_{\substack{p:\\\exists p'\in \mathcal{B}_x: p\manyReduce p'}}}_{\text{Refactors }\mathcal{B}_x}\probability[x|p]\probability[p|\mathcal{D},\theta]\Bigg)
%% \label{factorObjective}
%% \end{equation}
%% where $p\manyReduce p'$ is the standard notation for ``expression $p$
%% evaluates to $p'$ by the rules of $\lambda$-calculus''~\cite{pierce}.
%% Equation~\ref{factorObjective} captures the idea that we want to add
%% new components to the DSL while jointly refactoring our old programs
%% in terms of these new components.  But this joint optimization is
%% intractable, because there are infinitely many ways of refactoring a
%% program.  To make refactoring tractable we first limit the degree to
%% which a piece of code can be refactored: rather than consider every
%% refactoring, we bound the number of $\lambda$-calculus evaluation
%% steps separating a refactoring from its original program.  Now the
%% number of refactoring this is finite but astronomically large: for the
%% example in Figure~\ref{mapFactor} there are approximately $10^{14}$
%% possible refactorings -- a quantity that grows exponentially both as a
%% function of program size and a function of the bound on evaluation
%% steps. How can we tame this combinatorial explosion?











%% \textbf{Idea 1:} Limit the degree to which
%% a piece of code can be refactored.
%% Instead of considering every refactoring,
%% bound the number of $\lambda$-calculus evaluation steps
%% separating a refactoring from its original program.
%% Formally,
%% we define the set of $n$-step refactorings as:
%% \begin{equation}
%%   R_n(p) = \left\{p'\;:\;p'\underbrace{\reduce p''\reduce\cdots\reduce}_{\text{$\leq n$ times}} p \right\}
%% \end{equation}
%% where $p_1\reduce p_2$ is the standard notation for ``$p_1$ rewrites to $p_2$
%% in one step according to the rules of $\lambda$-calculus''~\cite{pierce}.
%% For example,
%% \begin{align*}
%%   \code{((lambda (x) (x x)) (lambda (y) y))}\reduce&\\
%%   \code{((lambda (y) y) (lambda (y) y))}\reduce&\\
%%   \code{(lambda (y) y)}&
%% \end{align*}
%% Returning to Equation~\ref{factorObjective},
%% this approximation gives the following objective:
%% \begin{equation}
%%         \log \probability[\mathcal{D}] + \argmax_{\theta}\Bigg(\log \probability[\theta|\mathcal{D}] - \|\theta\|_0 +\sum_{x\in X}\log \sum_{p\in \mathcal{B}_x}\probability[x|p]\max_{p'\in R_n(p)}\probability[p'|\mathcal{D},\theta]\Bigg)
%% \label{limitedObjective}
%%   \end{equation}
%% In practice, setting the number of refactoring steps $n$ to 3 suffices
%% to give a competent DSL learning algorithm.  Although the number of
%% refactorings is now finite, it is still prohibitively large, and in
%% fact grows exponentially quickly both as a function of $n$ and as a
%% function of the size of the program being refactored.  For example,
%% for the programs in Figure~\ref{mapFactor}, there are approximately
%% $10^{14}$ possible refactorings.  Next, we show how to tame this
%% exponential explosion.



%% In Appendix~\ref{appendixVersion}, we describe a dynamic program for
%% efficiently constructing a version space containing every $n$-step
%% refactoring.
%% This dynamic program, which we call $I\beta_n(p)$,
%% satisfies $\denotation{I\beta_n(p)} =  R_n(p)$.
%% In other words,
%% this dynamic program
%% builds a data structure
%% that represents the entire set of
%% refactorings -- but without having to explicitly enumerate all of the refactorings.




\begin{figure*}
  \centering\begin{tikzpicture}[every node/.style={inner sep=1,outer sep=0,rounded corners,thick}]
  \node(p1)[draw,rounded corners,thick] at (-1,0) {
    \begin{tabular}{l}
      \texttt{(Y ($\lambda$ (r l) (if (nil? l) nil}\\
      \texttt{ (cons (+ (car l) (car l))}\\
      \phantom{\texttt{(cons }}\texttt{ (r (cdr l))))))}
    \end{tabular}
  };
  \node(r1)[draw,inner sep=0,outer sep=0] at ([yshift=-2.5cm]p1.south) {
    \begin{tabular}{l}
      \texttt{(}\orange{\texttt{($\lambda$ (f) (Y ($\lambda$ (r l) (if (nil? l)}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{nil}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{(cons (f (car l))}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{ (r (cdr l)))))))}}\\
      \texttt{ ($\lambda$ (z) (+ z z)))}
    \end{tabular}
  };

  \node(p2)[draw] at ([xshift=4.5cm]p1.east) {
    \begin{tabular}{l}
      \texttt{(Y ($\lambda$ (r l) (if (nil? l) nil}\\
      \texttt{ (cons (- (car l) 1)}\\
      \phantom{\texttt{(cons }}\texttt{ (r (cdr l))))))}
    \end{tabular}
    
  };
  \node(r2)[draw] at ([yshift=-2.5cm]p2.south) {
    \begin{tabular}{l}
      \texttt{(}\orange{\texttt{($\lambda$ (f) (Y ($\lambda$ (r l) (if (nil? l)}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{nil}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{(cons (f (car l))}}\\
      \phantom{(($\lambda$ (f) (Y ($\lambda$ (r l)}\orange{\texttt{ (r (cdr l)))))))}}\\
      \texttt{ ($\lambda$ (z) (- z 1)))}
    \end{tabular}

  };

  \draw [->] (p1.south)  --(r1.north) node[fill=white,midway,align=center] {refactor\\($\approx 10^{14}$ refactorings)};
  \draw [->] (p2.south)  --(r2.north) node[fill=white,midway,align=center] {refactor\\($\approx 10^{14}$ refactorings)};

  \node[draw](m) at (3,-6.5) {
    \begin{tabular}{l}
      \fbox{\textsc{map}} = \orange{\texttt{($\lambda$ (f) (Y ($\lambda$ (r l) (if (nil? l) nil}}\\
      \phantom{\texttt{\emph{map}} = \texttt{($\lambda$ (f) (Y ($\lambda$ (r l) (if }}\orange{\texttt{(cons (f (car l))}}\\
      \phantom{\texttt{\emph{map}} = \texttt{($\lambda$ (f) (Y ($\lambda$ (r l) (if }}\orange{\texttt{(r (cdr l))))))}}\\
      \code{(}\fbox{\textsc{map}}\code{ ($\lambda$ (z) (+ z z)))}\quad(program rewritten w/ new \fbox{\textsc{map}} primitive)\\
      \code{(}\fbox{\textsc{map}}\code{ ($\lambda$ (z) (- z 1)))}\quad(program rewritten w/ new \fbox{\textsc{map}} primitive)\\
    \end{tabular}      
  };
  \draw [->](r1.south)--(m.north);
  \draw [->](r2.south)--(m.north);
  \node[fill=white] at ([yshift=0.5cm]m.north) {\textbf{Compress (MDL/Bayes objective)}};

  \node(t1)[draw] at ([yshift=1.5cm]p1.north) {\begin{tabular}{ll}
      \textbf{Task}:&\texttt{(1 2 3)$\to$(2 4 6)}\\
      &\texttt{(4 3 4)$\to$(8 6 8)}
  \end{tabular}};
  \draw [->] (t1.south)  --(p1.north) node[fill=white,midway] {Wake: program search};
  \node(t2)[draw] at ([yshift=1.5cm]p2.north) {\begin{tabular}{ll}
      \textbf{Task}:&\texttt{(1 2 3)$\to$(0 1 2)}\\
      &\texttt{(4 3 4)$\to$(3 2 3)}
  \end{tabular}};
  \draw [->] (t2.south)  --(p2.north) node[fill=white,midway] {Wake: program search};

  \node(panelA)[ultra thick, rounded corners=0, inner sep=10,outer sep=10, draw, fit=(t1) (t2) (m) (r1) (r2)] {}; \node at ($(0.5,0) + (panelA.west |- panelA.north)$) {\textbf{A}};

  \footnotesize
  \begin{scope}[shift={(-1,-12)}]  
    \node[draw, rounded corners](u1) at (0,0) {union};
    \node[draw, rounded corners](u2) at ($(0,-1) + (u1.south)$) {\code{(}union\code{ 1)}}; \draw (u1.south) -- (u2.north);
    \node[draw, rounded corners](u21) at ($(-1.3,-1) + (u2.south)$) {\code{(($\lambda$ (x) (x 1)) +)}}; \draw ([xshift=-0.2cm]u2.south) -- (u21.north);
    \node[draw, rounded corners](u22) at ($(2.3,-1) + (u2.south)$) {\code{(($\lambda$ (x) (+ x)) 1)}};  \draw ([xshift=-0.2cm]u2.south) -- (u22.north);

    \node[draw, rounded corners](u11) at ($(-1.75,-1) + (u1.south)$) {\code{(+ 1 1)}}; \draw (u1.south) -- (u11.north);
    \node[draw, rounded corners](u12) at ($(2.75,-1) + (u1.south)$) {\code{(($\lambda$ (x) (x 1 1)) +)}}; \draw (u1.south) -- (u12.north);

    \node(vs) at ($(u21.west)!0.5!(u12.east) + (0,-1.25)$) {$\underbrace{\hspace{8cm}}_{\text{\normalsize Subset of version space}}$};

    \node[anchor=north](p) at ($(0,2.5) + (u1)$) {\normalsize Program: \texttt{(+ 1 1)}};
    \draw[ultra thick,->] ($(0,-0.1) + (p.south)$) --node[sloped, above, inner sep=5]{Refactors} ($(0,0.1) + (u1.north)$);
    \node(g)[anchor=left] at ($(7,0) + (u12.east)$) {\includegraphics[width = 6cm]{figures/vs.eps}};
    \node(panelB)[ultra thick, rounded corners=0, inner sep=10,outer sep=10, draw, fit=(p) (u1) (vs) (g)] {}; \node at ($(0.5,0) + (panelB.west |- panelB.north)$) {\normalsize\textbf{B}}; 


    %% \node(panelC)[ultra thick, rounded corners=0, inner sep=8,outer sep=8, draw, fit=(g)] {}; \node at ($(0.5,0) + (panelC.west |- panelC.north)$) {\normalsize\textbf{C}}; 
  \end{scope}
  
  \end{tikzpicture}
  \caption{DSL learning as code refactoring. \textbf{Panel A:} During waking we discover programs for each task, then refactor the code from those programs to expose common subprograms (highlighted in \orange{orange}). Common subprograms are incorporated into the DSL when they increase a Bayesian objective. Intuitively, these new DSL components best compress the programs found during waking. \textbf{Panel B:} \# of possible refactorings grows exponentially with program size, so we represent refactorings using version spaces, which augment syntax trees with a \emph{union} operator whose children are themselves version spaces. Right graph: version spaces are exponentially more efficient than explicitly constructing set of refactorings. In this graph, refactored programs are of the form $1+1+\cdots  + 1$.}\label{mapFactor}
\end{figure*}


\subsection{Dream Sleep: Training a Neural Recognition Model}\label{recognitionSection}

During ``dreaming'' the system learns a recognition model that guides
program search.  It learns from (program, task) pairs drawn from two
sources of self-supervised data: \emph{replays} of programs discovered
during waking, and \emph{fantasies}, or programs drawn from the DSL.
Replays ensure that the recognition model is trained on the actual
tasks it needs to solve, and does not forget how to solve them.
Fantasies ensure that the recognition model has a large and highly
varied corpus of (program, task) pairs to learn from.

Formally, the recognition model $Q(p|x)$ should approximate the posterior
$\probability[p|\mathcal{D},\theta,x]$.
We can either train $Q$ to perform full posterior inference by minimizing the expected KL-divergence, $  \expect\left[\text{KL}\left(\probability[p|x,\mathcal{D},\theta]\|Q(p|x) \right) \right]$,
or we can train $Q$ to perform MAP inference
by maximizing $\expect\left[\max_{p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]} \log Q(p|x) \right]$,
where in both cases the expectation is taken over tasks. Taking this expectation over the empirical distribution of tasks trains $Q$ on replays; taking it over samples from the generative model trains $Q$ on fantasies.
We define a pair of alternative objectives for the recognition model,
$\mathcal{L}^{\text{posterior}}$ and $\mathcal{L}^{\text{MAP}}$,
which either train $Q$ to perform full posterior inference or MAP inference, respectively.
These objectives combine replays and fantasies:
\begin{align*}
  \mathcal{L}^{\text{posterior}} &= \mathcal{L}_{\text{Replay}}^{\text{posterior}} + \mathcal{L}_{\text{Fantasy}}^{\text{posterior}}&
  \mathcal{L}^{\text{MAP}} &= \mathcal{L}_{\text{Replay}}^{\text{MAP}} + \mathcal{L}_{\text{Fantasy}}^{\text{MAP}}\\
  \mathcal{L}_{\text{Replay}}^{\text{posterior}}& = \expect_{x\sim X}\left[\sum_{p\in \mathcal{B}_x}
    \frac{\probability\left[x,p|\mathcal{D},\theta \right]\log Q(p|x)}{\sum_{p'\in \mathcal{B}_x}\probability\left[x,p'|\mathcal{D},\theta \right]}\right] &
  \mathcal{L}_{\text{Replay}}^{\text{MAP}}& = \expect_{x\sim X}\left[\max_{\substack{p\in \mathcal{B}_x\\p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]}} \log Q(p|x) \right]  \\
  \mathcal{L}_{\text{Fantasy}}^{\text{posterior}} &= \expect_{(p,x)\sim(\mathcal{D},\theta) }\left[\log Q(p|x)\right]&
  \mathcal{L}_{\text{Fantasy}}^{\text{MAP}} &= \expect_{x\sim(\mathcal{D},\theta) }\left[\max_{\substack{p\\p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]}}\log Q(p)\right]
\end{align*}
We maximize $\mathcal{L}^{\text{MAP}}$ rather than
$\mathcal{L}^{\text{posterior}}$ for two reasons:
$\mathcal{L}^{\text{MAP}}$ prioritizes the shortest program solving a
task, thus more strongly accelerating enumerative search during waking;
and, combined with our parameterization of $Q$, described next, we
will show that $\mathcal{L}^{\text{MAP}}$ forces the recognition model
to break symmetries in the space of programs.

\noindent\textbf{Parameterizing $Q$.} The recognition model predicts a
fixed-dimensional tensor encoding a distribution over subroutines in
the DSL, conditioned on the local context in the syntax tree of the
program. This local context consists of the parent node in the syntax
tree, as well as which argument is being generated, functioning as a
kind of `bigram' model over trees. Figure~\ref{symmetry} (left)
diagrams this generative process.  This parameterization confers three
main advantages: (1) it supports fast enumeration and sampling of
programs, because the recognition model only runs once per
task, like in~\cite{balog2016deepcoder,ecc,menon2013machine} -- thus
we can fall back on fast enumeration if the target program is unlike
the training programs; (2) the recognition model provides fine-grained
information about the structure of the target program, similar
to~\cite{devlin2017robustfill,zavershynskyi2018naps}; and (3) in
conjunction with $\mathcal{L}^{\text{MAP}}$ the recognition model
learns to break symmetries in the space of programs.

\noindent\textbf{Symmetry breaking.} A good DSL not only exposes high-level building blocks, but also
carefully restricts the ways in which those building blocks are
allowed to compose.  For example,
%% a DSL for list manipulation should
%% contain both the empty list and a routine for appending lists, but
%% should not allow appending the empty list.  Similarly 
a DSL for arithmetic should disallow adding zero, or force
right-associative addition. A bigram parameterization of the recognition
model, combined with the $\mathcal{L}^{\text{MAP}}$ training
objective, interact in a way that breaks symmetries like these, allowing the agent to more efficiently explore the space of
programs.  This interaction occurs because the bigram parameterization
can disallow DSL primitives depending on their local syntactic
context, while the $\mathcal{L}^{\text{MAP}}$ objective forces all
probability mass onto a single member of a set of syntactically
distinct but semantically equivalent expressions
(Appendix~\ref{recognitionAppendix}).
We experimentally confirm this symmetry-breaking
behavior by training recognition models that minimize either
$\mathcal{L}^{\text{MAP}}$/$\mathcal{L}^\text{posterior}$ and which
use either a bigram parameterization/unigram\footnote{In the unigram variant $Q$ predicts a $|\mathcal{D}| + 1$-dimensional vector: $Q(p|x) = \probability[p|\mathcal{D},\theta_i = Q_i(x)]$,
  and was used in our prior work~\cite{ecc}} parameterization.
Figure~\ref{symmetry} (right) shows the result of training $Q$ in these four regimes
and then sampling programs.
On this particular run,
the combination of
bigrams and $\mathcal{L}^{\text{MAP}}$ learns to
avoid adding zero and associate addition to the right ---
different random initializations
lead to either right or left association.




%% \subsubsection{Parameterizing $Q$}\label{recognitionParameterization}

%% Broadly the literature contains two different approaches to
%% parameterizing conditional distributions over programs.  The first
%% approach~\cite{devlin2017robustfill,zavershynskyi2018naps} is to use a
%% recurrent network to predict the entire program token-by-token, which
%% has the advantage that, if the network is sufficiently powerful, it
%% can completely solve the synthesis problem.  The disadvantage is that
%% these models can perform poorly at out-of-sample
%% generalization~\cite{}, which is critical for our setting, as the
%% agent may need to solve new tasks that are qualitatively different
%% from the tasks it has solved so far.

%% %% Second, a powerful deep
%% %% recurrent network may be costly to sample or enumerate from --- so if
%% %% the network cannot easily solve a task, we cannot compensate with
%% %% rapid sampling or enumeration.  In contrast, state-of-the-art
%% %% enumerative program synthesizers evaluate millions of programs per
%% %% second~\cite{feser2015synthesizing}.

%% The second approach is to have $Q$ predict a fixed-dimensional weight
%% vector, which then biases a fast enumerator~\cite{balog2016deepcoder,ecc}
%% or sampler~\cite{menon2013machine}.  This approach can enjoy strong
%% out-of-sample generalization, because it can fall back on enumeration
%% or sampling when the target program is unlike the training programs.
%% A main drawback is that the neural net is deliberately handicapped,
%% and can only send so much information about the target program.

%% We adopt a middle ground between these two extremes.  Our recognition
%% model predicts a distribution over primitives in the DSL,
%% conditioned on the local context in the syntax tree of the
%% program. When predicting the next node to add to the syntax tree of a program,
%% the recognition model
%% conditions on the parent node, as well as
%% which argument is being generated.
%% This is a kind of `bigram' model over trees,
%% where the bigrams
%% take the form of (parent, child, argument index).
%% Figure~\ref{symmetry} (left) diagrams this generative process and Algorithm~\ref{recognitionSample}
%% specifies a sampling procedure for $Q(\cdot |x)$.
%% This parameterization
%% confers three main advantages:
%% (1) it supports fast enumeration
%% and sampling of programs,
%% because the
%% recognition model
%% only needs to run once for each task;
%% (2) it allows the recognition model to provide fine-grained
%% information about the structure of the target program;
%% and (3)
%% training this recognition model
%% causes it to learn to break symmetries in the space of programs,
%% described next.

%% \subsubsection{Learning to break symmetries in program space}\label{symmetricSection}

%% A good DSL not only exposes high-level building blocks, but also
%% carefully restricts the ways in which those building blocks are
%% allowed to compose.  For example,
%% %% a DSL for list manipulation should
%% %% contain both the empty list and a routine for appending lists, but
%% %% should not allow appending the empty list.  Similarly 
%% a DSL for arithmetic should contain both addition and the number zero
%% but disallow adding zero.  These restrictions break symmetries in the
%% space of programs.  A bigram parameterization of the recognition
%% model, combined with the $\mathcal{L}^{\text{MAP}}$ training
%% objective, interact in a way that breaks symmetries in the program
%% space, allowing the agent to more efficiently explore the space of
%% programs.  This interaction occurs because the bigram parameterization
%% can disallow DSL primitives depending on their local syntactic
%% context, while the $\mathcal{L}^{\text{MAP}}$ objective forces all
%% probability mass onto a single member of a set of syntactically
%% distinct but semantically equivalent expressions
%% (Appendix~\ref{recognitionAppendix}).




%% We experimentally confirm this symmetry-breaking
%% behavior by training recognition models that minimize either
%% $\mathcal{L}^{\text{MAP}}$/$\mathcal{L}^\text{posterior}$ and which
%% use either a bigram parameterization/unigram\footnote{In the unigram variant $Q$ predicts a $|\mathcal{D}| + 1$-dimensional vector: $Q(p|x) = \probability[p|\mathcal{D},\theta_i = Q_i(x)]$,
%%   and was used in our prior work~\cite{ecc}} parameterization.
%% Figure~\ref{symmetry} shows the result of training $Q$ in these four regimes for a DSL containing \code{+}, \code{0}, and \code{1}
%% and then sampling programs.
%% On this particular run,
%% the combination of
%% bigrams and $\mathcal{L}^{\text{MAP}}$ learns to
%% avoid adding zero and associate addition to the right ---
%% different random initializations
%% lead to either right or left association.

\begin{figure}
  \begin{minipage}[c]{0.3\textwidth}
    \begin{tikzpicture}[scale=1]%[every node/.style={inner sep=0,outer sep=0}]
    \node(l)[draw] at (0,0) {\texttt{$\lambda$ (a)}};
    \node(k)[draw] at ([yshift=-50]l.south) {\texttt{+}};
    \node(o)[draw] at ([xshift=-50,yshift=-50]k.south) {\texttt{9}};
    \node(m)[draw] at ([xshift=50,yshift=-50]k.south) {\texttt{*}};
    \node(x1)[draw] at ([xshift=50,yshift=-50]m.south) {\texttt{a}};
    \node(x2)[draw] at ([xshift=-50,yshift=-50]m.south) {\texttt{a}};

    \draw[->] (l.south)-- node[fill=white,align=center,midway,inner sep=0,outer sep=0]{$Q_{\text{start},\texttt{+},1}(x)$}(k.north);
    \draw[->] (k.south)--node[sloped, above]{$Q_{\texttt{+},\texttt{9},1}(x)$}(o.north);
    \draw[->] (k.south)--node[sloped, above]{$Q_{\texttt{+},\texttt{*},2}(x)$}(m.north);
    \draw[->] (m.south)--node[sloped, above]{$Q_{\texttt{*},\text{var},2}(x)$}(x1.north);
    \draw[->] (m.south)--node[sloped, above]{$Q_{\texttt{*},\text{var},1}(x)$}(x2.north);
\end{tikzpicture}
    \end{minipage}\hfill%
  \begin{tabular}{cll}
     \toprule
&     \multicolumn{1}{c}{Unigram}&\multicolumn{1}{c}{Bigram} \\\midrule
     \rotatebox[origin=c]{90}{$\mathcal{L}^{\text{posterior}}$}&
     \begin{tabular}{l}
       \emph{Three samples:}\\
       \code{(+ 1 0)}\\
\code{(+ (+ 0 0)}\\\code{\phantom{(+ }(+ 1 0))}\\
\code{(+ 1 1)}
\\
63.0\% right-associative\\37.4\% \code{+0}'s
       \end{tabular}
     &
     \begin{tabular}{l}
       \emph{Three samples:}\\
       \code{0}\\
\code{(+ (+ (+ 0 0)}\\\code{\phantom{(+ (+} (+ 0 1)) 1)}\\
\code{1}\\
55.8\% right-associative\\31.9\% \code{+0}'s
       \end{tabular}
     \\\\
     \rotatebox[origin=c]{90}{$\mathcal{L}^{\text{MAP}}$}&
     \begin{tabular}{l}
       \emph{Three samples:}\\
       \code{1}\\
       \code{(+ 1 (+ 1 (+ (+ 1 }\\\code{\phantom{(+ }(+ 1 1)) 1)))}\\
       \code{(+ (+ 1 1) 1)}
\\48.6\% right-associative\\
       0.5\% \code{+0}'s       
       \end{tabular}
     &
     \begin{tabular}{l}
       \emph{Three Samples:}\\
\code{(+ 1 (+ 1 (+ 1}\\\code{\phantom{(+ 1} (+ 1 (+ 1 1)))))}\\
\code{0}\\
\code{(+ 1 (+ 1 (+ 1 1)))}
       \\
       \textbf{97.9\% right-associative}\\
       2.5\% \code{+0}'s       
       \end{tabular}
\\    \bottomrule 
  \end{tabular}
  \caption{\textbf{Left:} Bigram parameterization of distribution over programs predicted by recognition model.
  Here the program (syntax tree shown above) is \texttt{($\lambda$ (a) (+ 9 (* a a )))}.
Each conditional distribution predicted by the recognition model is written $Q_{\text{parent},\text{child},\text{argument index}}(x)$, where $x$ is a task. \textbf{Right:} Agent learns to break symmetries in program space only when using both bigram parameterization and $\mathcal{L}^{\text{MAP}}$ objective, associating addition to the right and avoiding adding zero. \% right-associative calculated by drawing 500 samples from $Q$. $\mathcal{L}^{\text{MAP}}$/Unigram agent incorrectly learns to never generate programs with \code{0}'s, while $\mathcal{L}^{\text{MAP}}$/Bigram agent correctly learns that \code{0} should only be disallowed as an argument of addition. Tasked with building programs from \code{+}, \code{1}, and \code{0}. }\label{symmetry}
\end{figure}

%% \begin{figure}
%% \centering  \begin{tabular}{cll}
%%      \toprule
%% &     \multicolumn{1}{c}{Unigram}&\multicolumn{1}{c}{Bigram} \\\midrule
%%      $\mathcal{L}^{\text{posterior}}$&
%%      \begin{tabular}{l}
%%        \emph{Three samples:}\\
%%        \code{(+ 1 0)}\\
%% \code{(+ (+ 0 0) (+ 1 0))}\\
%% \code{(+ 1 1)}
%% \\
%% 63.0\% right-associative; 37.4\% \code{+0}'s
%%        \end{tabular}
%%      &
%%      \begin{tabular}{l}
%%        \emph{Three samples:}\\
%%        \code{0}\\
%% \code{(+ (+ (+ 0 0) (+ 0 1)) 1)}\\
%% \code{1}\\
%% 55.8\% right-associative; 31.9\% \code{+0}'s
%%        \end{tabular}
%%      \\\\
%%      $\mathcal{L}^{\text{MAP}}$&
%%      \begin{tabular}{l}
%%        \emph{Three samples:}\\
%%        \code{1}\\
%%        \code{(+ 1 (+ 1 (+ (+ 1 (+ 1 1)) 1)))}\\
%%        \code{(+ (+ 1 1) 1)}
%% \\48.6\% right-associative;
%%        0.5\% \code{+0}'s       
%%        \end{tabular}
%%      &
%%      \begin{tabular}{l}
%%        \emph{Three Samples:}\\
%% \code{(+ 1 (+ 1 (+ 1 (+ 1 (+ 1 1)))))}\\
%% \code{0}\\
%% \code{(+ 1 (+ 1 (+ 1 1)))}
%%        \\
%%        \textbf{97.9\% right-associative};
%%        2.5\% \code{+0}'s       
%%        \end{tabular}
%% \\    \bottomrule 
%%   \end{tabular}
%%   \caption{Agent learns to break symmetries in program space only when using both bigram parameterization and $\mathcal{L}^{\text{MAP}}$ objective, associating addition to the right and avoiding adding zero. \% right-associative calculated by drawing 500 samples from $Q$. $\mathcal{L}^{\text{MAP}}$/Unigram agent incorrectly learns to never generate programs with \code{0}'s, while $\mathcal{L}^{\text{MAP}}$/Bigram agent correctly learns that \code{0} should only be disallowed as an argument of addition. Tasked with building programs from \code{+}, \code{1}, and \code{0}. }\label{symmetry}
%%   \end{figure}



\section{Experiments}

\subsection{Programs that manipulate sequences}\label{sequences}
We first apply \system to two classic benchmark domains: list
processing and text editing. In both cases we solve tasks specified by
a input/output examples, starting with a generic functional programming
basis: \code{foldr}, \code{unfold}, \code{if}, \code{map},
\code{length}, \code{index}, \code{=}, \code{+}, \code{-}, \code{0},
\code{1}, \code{cons}, \code{car}, \code{cdr}, \code{nil}, and
\code{is-nil}.

\subsubsection{List Processing}\label{listSection}
We took 218 list manipulation tasks from our previous work~\cite{ecc},
each with 15 input/output examples.  In solving these tasks, the
system composed 16 new DSL components, and discovered multiple
higher-order functions. Each round of memory consolidation built on
components discovered in earlier sleep cycles --- for example the
agent first learns the higher-order function \code{filter}, uses
\code{filter} to learn to take the maximum element of a list, then
uses that routine to learn a new component for extracting the
$n^{\text{th}}$ largest element of a list, which it finally uses to
solve a task involving sorting a list of numbers (Figure~\ref{initialExampleDSL}).
This incremental, modular learning of deep hierarchies of DSL
components occurs because of the alternation between code writing
(during waking) and code refactoring (during the consolidation phase
of sleep).

%% \begin{figure}[b]\centering
%% \vspace{-0.5cm}  \begin{tabular}{lll}
%%     \toprule
%%     Name & Input & Output \\\midrule
%%     repeat-3 & [7\, 0] & [7\, 0\, 7\, 0\, 7\, 0] \\
%%     drop-3 & [0\, 3\, 8\, 6\, 4] & [6\, 4] \\
%%     rotate-2 & [8\, 14\, 1\, 9] & [1\, 9\, 8\, 14] \\
%%     count-head-in-tail & [1\, 2\, 1\, 1\, 3] & 2 \\
%%     keep-div-5 & [5\, 9\, 14\, 6\, 3\, 0] & [5\, 0] \\
%%     product & [7\, 1\, 6\, 2] & 84 \\
%%     \bottomrule
%%   \end{tabular}
%%   \captionof{table}{Some tasks in our list function domain.}\label{listExamples}\vspace{-0.5cm}
%% \end{figure}

\subsubsection{Text Editing}\label{textSection}
Synthesizing programs that edit text is a classic problem in the
programming languages and AI literatures~\cite{lau2001programming},
and algorithms that synthesize text editing programs ship in Microsoft
Excel~\cite{gulwani2011automatin}.  This prior work uses
hand-engineered DSLs and hand-engineered search strategies.  Here, we
will show that we can jointly learn both these ingredients and surpass
the state-of-the-art domain-general program synthesizers on a standard
text editing benchmark.

%% Because our enumerative search procedure cannot generate string %
%% constants, we instead enumerate programs with string-valued
%% parameters.  For example, to learn a program that prepends ``Dr.'', we
%% enumerate $\text{\code{(}}f_3\code{ string s)}$ -- where $f_3$ is the
%% learned appending primitive (Fig.~\ref{initialExampleDSL}) --- and then
%% define $\probability[x|p]$ by approximately marginalizing out the
%% string parameters via a simple dynamic program.
%% In Sec.~\ref{regressionSection}, we will use a similar trick to
%% synthesize programs containing real numbers, but using gradient
%% descent instead of dynamic programming.

We trained our system on 128 automatically-generated text editing tasks, with 4 input/output examples each.
We tested, but did not train, on the 108 text editing problems from the SyGuS~\cite{alur2016sygus} program synthesis competition. Before any learning,
\system solves 3.7\% of the problems within 10 minutes with an average search time of 235 seconds.
After learning,
it solves 79.6\%, and does so much faster,
solving them in an average of 40 seconds.
As of the 2017 SyGuS competition,
the best-performing synthesizer (CVC4) solves 82.4\% of the problems ---
but here, the competition conditions are 1 hour \& 8 CPUs per problem,
and with this more generous compute budget we
surpass
this previous
result and solve
84.3\% of the problems.
SyGuS additionally comes with a
different hand-engineered DSL \emph{for each text editing problem}. %\footnote{SyGuS text editing problems also prespecify the set of allowed string constants for each task. For these experiments, our system did not use this assistance.}
Here  we learned a single DSL
that applied generically to
all of the tasks,
and perform comparably to the best
prior work.

\subsection{Programs from visual input}
We consider three domains where the agent must infer a program from an
image (Figure~\ref{visualSpecs}).
First we consider programs that make plans and take actions:  drawing pictures and building towers out of blocks (Sec.~\ref{logoSection}-\ref{towerSection}).

\subsubsection{Programs that draw pictures}\label{logoSection}

Procedural visual concepts are studied across AI and cognitive science
--- Bongard problems~\cite{Moscow}, Raven's progressive
matrices~\cite{raven2003raven}, and Lake et al.'s BPL model of
omniglot~\cite{lake2015human} are prominent examples.  Here we take
inspiration from LOGO Turtle graphics~\cite{turtle}, tasking our agent
with drawing a modest corpus of images while equipping it with control
over a `pen', along with arithmetic operations on angles and
distances.

Inside its learned DSL we find interpretable parametric drawing
routines corresponding to the families of visual objects in its
training data, like polygons, circles, and spirals
(Figure~\ref{logoPrimitives}, left) -- without supervision the agent
has learned the basic types of objects in its visual world. It
additionally learns more abstract visual relationships, like
rotational symmetry, which it models by incorporating a higher-order
function into its DSL (Figure~\ref{logoPrimitives}, right).  %% This
%% abstraction comes from jointly compressing programs for all of the
%% training images into its DSL.

What does \system dream of?  Prior to learning samples from the DSL
are simple and largely unstructured (Figure~\ref{logoDreams}, left).
After training the samples become richly structured
(Figure~\ref{logoDreams}, right), compositionally recombining latent
building blocks and motifs acquired from the training data. This
offers a visual window into how the generative model bootstraps recognition
model training: as the DSL grows more finely tuned to the domain, the
neural net gets richer and more highly varied training data.


%% has primitives for moving the pen forward and rotating it (\code{move}),
%% picking the pen up and then putting it down (\code{pen-up}),
%% arithmetic operations on angles and distances,
%% and constants ($2\pi$, 0 through 9, and $\infty$\footnote{To ensure that programs terminate, we set $\infty = 20$}).

\begin{figure}\centering
  \includegraphics[width = 0.3\textwidth]{figures/logo16.png}
  \includegraphics[width = 0.3\textwidth]{figures/tower9.png}
  \includegraphics[width = 0.3\textwidth]{figures/sr16.png}
  \caption{Three domains where the agent infers a program from visual input. \textbf{Left}: 16 (out of 160) LOGO graphics tasks. Agent writes a program controlling a `pen' that draws the target picture. \textbf{Middle}: 9 (out of 112) tower building tasks. Agent writes a program controlling a `hand' that builds the target tower. \textbf{Right}: 16 (out of 200) symbolic regression tasks. Agent writes a program containing continuous real numbers that fits the points along the curve.}\label{visualSpecs}
\end{figure}
\begin{figure}
\centering  \begin{tabular}{cc}
    \toprule
    Parametric drawing routines&Higher-order drawing routine\\\midrule
    \begin{tabular}{rl}
      Semicircle:& \raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_24.png}}\\
      Circles:
      &\raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_23.png}}\\
      Spiral:&\raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_25.png}}\\
      Greek Spiral:&\raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_10.png}}\\
      S-Curves:&\raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_11.png}}        \\
        Polygons \& Stars:
&\raisebox{-.5\height}{\includegraphics[width = 0.3\textwidth]{figures/logo_primitives/logo_primitive_12.png}}
    \end{tabular}
    &
    \begin{tabular}{c}
      Rotational Symmetry:\\
          \includegraphics[width = 0.35\textwidth]{figures/rotationalmontage.png}
      \end{tabular}
    
    \\\bottomrule 
  \end{tabular}
  \caption{Example primitives learned by \system when trained on tasks in Figure~\ref{everyLogo}. Agent learns parametric routines for drawing families of curves (left) as well as subroutines that take entire programs as input (right). Each row of images on the left is the same code executed with different parameters. Each image on the right is the same code executed with different parameters and with a different subprogram provided as input.}\label{logoPrimitives}
  \end{figure}
\begin{figure}
\centering  \begin{tabular}{c|cc}
  \begin{tabular}{c}
    \includegraphics[width = 0.45\textwidth]{figures/initialDreams/montage.png}
    \end{tabular}&
      \begin{tabular}{c}
        \includegraphics[width = 0.45\textwidth]{figures/finalDreams/cherry_random_montage.png}
        \end{tabular}&
      \rotatebox[origin=c]{90}{Cherry picked\hspace{3cm}Random}
    \end{tabular}
  \caption{Sixteen dreams, or samples, from the DSL before (left) and after (right) training on tasks in Figure~\ref{everyLogo}. Blue: where the agent started drawing. Pink: where the agent ended drawing.}\label{logoDreams}
\end{figure}

\subsubsection{Building towers out of `Lego' blocks}\label{towerSection}

Inspired by the classic AI `copy task' --- where an agent must look at
an image of a tower made of toy blocks and re-create the
tower~\cite{towerCopy} --- we give \system 112 tower `copy tasks'
(Figure~\ref{towerTasks}).  Here the agent observes both an image of a
tower and the locations of each of its blocks, and must write a
program that plans how a simulated hand would build the tower.
These towers are built from Lego-style blocks
that snap together on a discrete grid.
The system starts with the same control flow primitives as with
LOGO graphics,
and learns parametric `options' for building blocks towers (Figure~\ref{tower}),
inferring concepts like arches, staircases, bridges and brick walls.
\begin{figure}
  \begin{tabular}{cc}
    \toprule Example learned DSL components&Samples from DSL\\\midrule
    \begin{tabular}{rl}
      Brickwall&\begin{tabular}{l}
        \includegraphics[width = 5cm]{figures/tower/tower_dsl_bricks.png}
      \end{tabular}\\
      Bridge&\begin{tabular}{l}
        \includegraphics[width = 5cm]{figures/tower/tower_dsl_bridge.png}
      \end{tabular}\\
      Staircase&\begin{tabular}{l}
        \includegraphics[width = 5cm]{figures/tower/tower_dsl_staircase.png}
        \end{tabular}
    \end{tabular}&
    \begin{tabular}{l}
      \includegraphics[width = 5cm]{figures/tower/dreams.png}
      \end{tabular}
    \\\bottomrule 
    \end{tabular}
  \caption{\textbf{Left}: Three (out of 19) learned DSL components for building towers out of Lego-style blocks. These components act like parametric options~\cite{stolle2002learning},
    giving higher-level building blocks that the agent can use to plan. \textbf{Right}: 16 random samples, or `dreams', from learned DSL.}\label{tower}
\end{figure}



\subsubsection{Symbolic Regression}\label{regressionSection}
Here, the agent observes points along the curve of a function, and
must write a program that fits those points.  We initially equip our
learner with addition, multiplication, and division, and task it with
solving 200 symbolic regression problems, each either a polynomial or
rational function.  The recognition model is a convnet that observes
an image of the target function's graph (Fig.~\ref{visualSpecs},
rightmost) --- visually, different kinds of polynomials and rational
functions produce different kinds of graphs, and so the convnet can
look at a graph and predict what kind of function best explains it.  A
key difficulty, however, is that these problems are best solved with
programs containing real numbers.  Our solution to this difficulty is
to enumerate programs with real-valued parameters, and then fit those
parameters by automatically differentiating through the programs the
system writes and use gradient descent to fit the parameters.  We
define the likelihood model, $\probability[x|p]$, by assuming a
Gaussian noise model for the input/output examples, and penalize the
use of real-valued parameters using the
BIC~\cite{Bishop:2006:PRM:1162264}.

We learn a DSL containing 13 new functions, mainly templates for
different pieces of polynomials or ratios of polynomials.  The model
also learns to find programs minimizing the number of continuous
parameters --- for example, learning to represent linear functions
with \code{(* real (+ x real))}.  This phenomenon arises from our
Bayesian framing: both the generative model's bias toward shorter
programs, and the likelihood model's BIC penalty.

\subsubsection{Generative Modeling of Text}
We investigate few-shot learning of generative models by tasking our
agent with inferring a probabilistic regular expression from a small
number (5) of strings - for example, observing the strings \$1.20 and \$9.42
and predicting the regex \$d

\begin{tabular}{ccc}
  \toprule   Observed text&Inferred regex&Held out testing text\\\midrule
  
  \end{tabular}

\subsection{Quantitative Results on Held-Out Tasks}\label{quantitative}
To evaluate the relative importance of DSL learning and recognition
model training, we evaluate on held-out testing tasks for each of our
domains, measuring both how many tasks are solved and how long it
takes to solve them across successive wake/sleep iterations
(Fig.~\ref{learningCurves}).  We always solve more held-out tasks --
and generally solve them in less time -- with both components
combined.  Why?  One hypothesis is that some tasks are best solved by
DSL learning and others by a neural network, and so including both of
our sleep cycles  takes the union of these sets of tasks.  Another
hypothesis is that the DSL and neural network interact
synergistically, bootstrapping off each other to solve tasks for which
neither alone suffice. We evaluate the relative weight of these interactions by
computing the ratio of the tasks solved uniquely by the full model to 





%% We compare with ablations of our model on held out tasks.
%% The purpose of this ablation study is 
%% both to examine the role of each component of \systemEnding,
%% as well as to compare with
%% prior approaches in the literature:
%% a head-to-head
%% comparison of program synthesizers is complicated by the fact that
%% each system, including ours, makes idiosyncratic 
%% assumptions about the space of programs and the statement of tasks.

%% Nevertheless, much prior work can be modeled within our setup. 
%% We compare with the following ablations (Tbl~\ref{baselineComparisons};
%% Fig~\ref{learningCurves}):
%% \\\noindent \textbf{No NN:} lesions the recognition model.
%% \\\noindent \textbf{NPS}, which does not learn the DSL,
%% instead learning the recognition model
%% from samples drawn from the fixed DSL.
%% We call this NPS (Neural Program Synthesis)
%% because this is closest to how
%% RobustFill~\cite{devlin2017robustfill} and DeepCoder~\cite{balog2016deepcoder} are trained.
%% \\\noindent \textbf{SE}, which lesions the recognition model and restricts the DSL  learning algorithm to
%% only add \textbf{S}ub\textbf{E}xpressions of programs in the beams to the DSL. This is how most prior approaches have learned libraries of functions~\cite{Dechter:2013:BLV:2540128.2540316,DBLP:conf/icml/LiangJK10,DBLP:conf/ecai/LinDETM14}.
%% \\\noindent \textbf{PCFG}, which lesions the recognition model and does not learn the DSL,
%% but instead learns the parameters of the DSL ($\theta$), learning the parameters of a PCFG while not learning any of the structure.
%% \\\noindent \textbf{Enum}, which enumerates a beam without any learning --- equivalently, our first search step.
%For each domain,
%% We are interested both in how many tasks the
%% agent can solve and how quickly it can find those solutions.
%% Tbl.~\ref{baselineComparisons}
%% compares our model against these alternatives.
%% We consistently
%% improve on the baselines,
%% and find that lesioning the recognition model
%% and lesioning it also slows down the convergence of the algorithm,
%% taking more iterations to reach a given number of tasks solved (Fig.~\ref{learningCurves}).
%% This supports a view of the recognition model as a way of amortizing the cost of search.

\begin{figure}
  \begin{tabular}{ccc}
    \begin{tabular}{c}
      \includegraphics[width = 5cm]{figures/learningCurves/text_hits.png}\\
      \includegraphics[width = 5cm]{figures/learningCurves/text_time.png}
    \end{tabular}&

    \begin{tabular}{c}
      \includegraphics[width = 5cm]{figures/learningCurves/logo_hits.png}\\
      \includegraphics[width = 5cm]{figures/learningCurves/logo_time.png}
    \end{tabular}&
    
    \begin{tabular}{c}
      \includegraphics[width = 5cm]{figures/learningCurves/rational_hits.png}\\
      \includegraphics[width = 5cm]{figures/learningCurves/rational_time.png}
    \end{tabular}\\\\\\

    \begin{tabular}{c}
      \includegraphics[width = 5cm]{figures/learningCurves/list_hard_hits.png}\\
      \includegraphics[width = 5cm]{figures/learningCurves/list_hard_time.png}
    \end{tabular}&

    \begin{tabular}{c}
      \includegraphics[width = 5cm]{figures/learningCurves/tower_hits.png}\\
      \includegraphics[width = 5cm]{figures/learningCurves/tower_time.png}
    \end{tabular}&

    \includegraphics[width = 5cm]{figures/learningCurves/regular}
  \end{tabular}
  \caption{Test set performance across wake/sleep iterations. Each curve is a run with a different random seed. Teal: Full model. Orange: Dreaming only (no DSL learning). Purple: Consolidation only (no recognition model). Search time plots show solid lines (time averaged over all tasks) and dotted lines (time averaged over solved tasks).}\label{learningCurves}      
\end{figure}

%% \begin{figure}
%%   \begin{tabular}{ccc}
%%     \includegraphics[width = 5cm]{figures/logoLearningCurve_batch.png}&
%%     \includegraphics[width = 5cm]{figures/textLearningCurve_batch.png}&
%%     \includegraphics[width = 5cm]{figures/listLearningCurve_batch.png}\\\\
%%     \includegraphics[width = 5cm]{figures/towerLearningCurve_batch.png}&
%%     \includegraphics[width = 5cm]{figures/rationalLearningCurve_batch.png}&
%%     \includegraphics[width = 5cm]{figures/regexLearningCurve_batch.png}
%%   \end{tabular}
%%   \caption{Test set performance across wake/sleep iterations. Orange: \systemEnding. Teal: No recognition model. Purple: No DSL learning. Solid lines: \% tasks solved within 10 minute timeout. Dashed lines: average search time. Shaded region is $\pm 1$ stddev search time.}\label{learningCurves}
%% \end{figure}


\section{Discussion}

\emph{(what we found)} Our goal was to build a model capturing
humanlike aspects of learning to be an expert in a new domain of
problems.  Two main findings of our work are that, first, a single
program-induction system can learn to solve large sets of problems
from many qualitatively different domains; and second, that fully
acquiring expertise in these domains hinges both upon explicit
declarative knowledge, and implicit procedural skill.  We have
deliberately placed the interplay of declarative and procedural
knowledge at the center of our model, and believe that both these
kinds of learning are crucial for building agents that, like humans,
autonomously learn to navigate a new domain of problems.

Our model's domain knowledge grows over a series of wake/sleep cycles,
with the solutions to easier problems bootstrapping
solving of harder tasks.
This dynamic is related to, but distinct from, curriculum learning. In curriculum learning approaches an agent solves a stream of pedagogically selected, increasingly difficult tasks; here we consider the case with the agent lacks a `teacher' and instead sees randomly sampled batches of tasks, effectively self-pacing its way through an implicit curriculum.
But humans select their tasks in much richer ways,
and can even generate their own tasks to solve,
either as stepping stones toward hard, unsolved problems or motivated by curiosity and aesthetics.
Building agents that generate their own problems in these humanlike ways is a necessary next step
if we want machines that push against boundary of human domain knowledge or which even discover whole new domains of problems,
both of which go far beyond the relatively small-scale demonstrations in this work.

A basic representational challenge is to
learn concepts fundamentally inexpressible in terms of the starting basis: \system invents procedures for
adding vectors, taking dot products, and calculating forces,
but can not make the jump needed to define new terms like `acceleration' and `coordinate system,'
concepts inexpressible as a library of procedures.
We believe approaches based either on term rewriting systems or defining new generalized algebraic datatypes could give traction to fundamental problems like these,
strategies which are closely allied with conceptual role semantics.
Indeed, humans often open up whole new fields of inquiry by introducing concepts not
definable in terms of prior abstractions,
like the meaning of `allele' in genetics
or `force' in physics.




related work

limitations

how will go beyond? generate own tasks; real DSL learning (all expressible in base language); lifelong learning (forgetting?)

\subsection{Learning from Scratch}
A long-standing dream within the program induction community
is ``learning from scratch'': starting with a \emph{minimal} Turing-complete programming language,
and then learning to solve a wide swath of
induction problems~\cite{solomonoff1964formal,schmidhuber2004optimal,hutter2004universal,solomonoff1989system}.
All existing systems,
including ours,
fall far short of this dream,
and it is unclear (and we believe unlikely)
that this dream could ever be fully realized.
How far can we push in this direction?
``Learning from scratch'' is subjective, but a reasonable
starting point is the set of primitives provided in 1959
Lisp~\cite{mccarthy1960recursive}: these include
conditionals, recursion, arithmetic, and the 
list operators \code{cons}, \code{car}, \code{cdr}, and \code{nil}.
A  basic first goal is to start with
these primitives,
and then recover a DSL that
more closely resembles modern functional languages like Haskell and OCaml.
Recall (Sec.~\ref{sequences})
that we initially provided our system with functional programming routines like
\code{map} and \code{fold}.

We ran the following experiment: \system was given a subset of the
1959 Lisp primitives, and tasked with solving 18 programming
exercises. A key difference between this setup and our previous
experiments is that, for this experiment, the system is given
primitive recursion, whereas previously we had sequestered recursion
within higher-order functions like \code{map}, \code{fold}, and
\code{unfold}.

After running for 93 hours on 48 CPUs, our
algorithm solves these 18 exercises, along the way assembling a DSL
with a modern repertoire of
functional programming idioms and subroutines, including \code{map},
\code{fold}, \code{unfold}, \code{index}, \code{length},
and  arithmetic operations like 
building lists of natural numbers between an interval (see  Appendix~\ref{appendixMcCarthy}).

We believe that program learners should \emph{not}
start from scratch,
but instead should start from
a rich, domain-agnostic
basis like those embodied in the standard libraries of modern  languages.
What this experiment shows is that \system doesn't \emph{need} to start from a rich basis,
and can in principle recover many of the amenities of modern programming systems,
provided it is given enough computational power and a suitable
spectrum of tasks.

\subsection{\system and the Exploration-Compression family of algorithms}

Our work sits within the Exploration-Compression (EC) family of
algorithms.  EC~\cite{Dechter:2013:BLV:2540128.2540316} is a program
induction framework where an agent alternates between searching, or
`exploring', the space of programs, and then updating its search
procedure by compressing programs found during exploration. \system
grew directly out of research on EC-style systems and both of our
sleep phases can be interpreted as a kind of compression:
consolidation aims to compactly refactor code, while the recognition
model aims to encode a program in as few bits as possible, conditioned
on a task. Our previous work, EC$^2$~\cite{ecc}, introduced
neurally-guided search into the EC framework and served as the
starting point for \systemEnding.  Our work here extends EC$^2$ by (1)
introducing a new refactoring compression algorithm, (2) enriching the
neural network with a new bigram-over-trees parameterization and loss
function that together learn to break symmetries, accelerating program
search during waking, and (3) demonstrating how this family of
approaches can be applied to planning and generative modeling
problems.  Other offshoots of EC include the neurosymbolic framework
in~\cite{lazaro2019beyond} and the hierarchical Bayesian program
learner in~\cite{DBLP:conf/icml/LiangJK10}. Closely aligned ideas go
even further back~\cite{solomonoff1989system,schmidhuber2004optimal}.

\subsection{Acquiring Domain Expertise}

One interpretation of our system is as a model of the acquisition of
domain expertise. Humans can acquire expertise across many domains --
cooking, coding, music, architecture, painting, tennis, or calculus,
to name a handful of examples, and every child develops expertise in
natural language, intuitive physics, motor control, kinship
relationships, and more. Domain experts learn domain-specific
abstractions, similar to a DSL: for example, a expert chef knows what
combinations of seasonings go together, or an expert mathematician
knows a wide set of useful theorems and lemmas.  Jointly, experts
learn how to recognize when to use these abstractions to solve
problems.  Becoming an expert involves a learning trajectory that
unfolds over relatively long time scales, but has modest data
requirements relative to the dominant machine learning paradigms. For
example, basic competency in cooking or coding might require on the
order of learning a hundred recipes or solving a hundred programming
exercises.  It is this learning regime that we have targeted with
\systemEnding: Learning about a domain from at most several hundred
tasks, but where the learning unfolds over many wake/sleep cycles.



\subsection{Prospects for program Induction as part of the generic AI toolkit}

Our aim with \system is to chart a path by which program induction can
become more broadly useful for AI.  This means viewing the AI
landscape through the lens of program learning, including the terrain
considered here --- simple kinds of generative modeling, inverse
graphics, planning, and programming by example --- but also many
others like reinforcement learning, commonsense reasoning, natural
language understanding, and causal inference. Can program induction
rise to the challenge?  We believe it can, provided we push jointly
along  many different axes of AI research; and provided we continue
to integrate learning algorithms -- both symbolic and neural, both
top-down and bottom-up -- into our artificial agents.

Our system, with its learned DSL and neural recognition model, is one
embodiment of this hybrid symbolic/neural approach, and enjoys some
success across small-scale problems in the different domains considered
here. Scaling to larger problems,
such as inferring 3-D object models (vs LOGO/Turtle),
learning natural image grammars (vs ),


\bibliography{main}
\bibliographystyle{plain}

\appendix

\section{Appendix}

\subsection{Neural Initial Problem Representations}

\begin{figure}
  \includegraphics[width = \textwidth]{figures/all_tsne.png}
  \caption{}\label{allTSNE}
\end{figure}

How do \systemEnding `s representations of problems themselves change, as the model acquires expertise? Within cognitive science, a rich body of evidence suggests that human experts learn to `see` problems differently in their domains -- even before solving a problem, human novices and experts seem to construct fundamentally different initial representations \cite{chi1981categorization}. Novices notice a problem’s `surface features`, the literal objects and details made obvious by the task description. Experts, on the other hand, somehow see past these details to pick out each problem’s `deep structures`, the underlying principles and concepts that govern its ultimate solution \cite{chi2012seeing,chi2006two}.


\systemEnding`s neural recognition model can be seen as forming analogous `initial` problem representations, implicitly encoded within the activations of the neural network. Intuitively, these initial neural representations map problem features to a `gist` over the model’s current domain-specific concepts -- they guide and constrain how the model searches for problem solutions, but are not symbolically structured solutions themselves. To probe how these representations change as the model gains a richer conceptual vocabulary within each domain, we compare how they cluster problems within that domain over the course of \systemEnding`s learning trajectory. Figure \ref{allTSNE} depicts TSNE visualizations of these neural problem representations at the first and last iteration of the algorithm. We color-code each problem using semantic, human-interpretable categories determined apriori by the task designers, but critically, the model itself never has access to these categories. Rather, as \systemEnding acquires domain expertise, the neural network adapts how it transforms each task`s surface features, restructuring its initial problem representation space to draw together problems that share deeper, more abstract conceptual similarities.


\subsection{Probabilistic Formulation of \system}\label{probabilisticAppendix}
Our objective is to infer the maximum a posteriori DSL $\mathcal{D}$ and
parameters $\theta$. Writing $J$ for the joint probability of $(\mathcal{D},\theta)$,
this corresponds to solving
\begin{align}\label{intractableObjectives}
\nonumber  J(\mathcal{D},\theta)\triangleq \probability[\mathcal{D},\theta]&\prod_{x\in X} \sum_p \probability[x|p]\probability[p|\mathcal{D},\theta]\\
  \mathcal{D}^* = \argmax_{\mathcal{D}}\int J(\mathcal{D},\theta)\;\mathrm{d}\theta& \qquad
  \theta^* =\argmax_\theta J(\mathcal{D}^*,\theta)
\end{align}
where $\probability[x|p]$ scores the likelihood of a task
$x\in X$ given a program $p$.\footnote{For example, for list
  processing, the likelihood is 1 if the program predicts the observed
  outputs on the observed inputs, and 0 otherwise; when learning a
  generative model or probabilistic program, the likelihood is the
  probability of the program sampling the observation.}

Evaluating Eq.~\ref{intractableObjectives}
entails marginalizing over the infinite set of all programs -- which is impossible.
We make a particle-based approximation to Eq.~\ref{intractableObjectives}
and instead marginalize over a finite \textbf{beam} of programs,
with one beam per task, collectively written $\left\{\mathcal{B}_x \right\}_{x\in X}$.
This particle-based approximation is written $\lowerBound (\mathcal{D},\theta,\left\{\mathcal{B}_x \right\})$
and acts as a lower bound on the joint density:
\begin{align}
 J(\mathcal{D},\theta)\geq \lowerBound  (\mathcal{D},\theta,\left\{\mathcal{B}_x \right\})\triangleq\probability[\mathcal{D},\theta]\prod_{x\in X} \sum_{p\in \mathcal{B}_x} \probability[x|p]\probability[p|\mathcal{D},\theta]\text{, where $|\mathcal{B}_x|$ is small}
\end{align}
In all of our experiments we set the maximum beam size $|\mathcal{B}_x|$ to 5.

Wake and sleep cycles correspond to alternate maximization of $\lowerBound$ w.r.t. $\left\{\mathcal{B}_x \right\}_{x\in X}$  (\textbf{Wake})
and $(\mathcal{D},\theta)$ (\textbf{Consolidation}):
\\\noindent \textbf{Wake: Maxing $\lowerBound$ w.r.t.\ the beams.} Here $(\mathcal{D},\theta)$ is fixed and we
want to find new programs to add to  the beams so that $\lowerBound$ increases the most.
$\lowerBound$ most increases by finding programs where $\probability[x|p]\probability[p|\mathcal{D},\theta]\propto\probability[p|x,\mathcal{D},\theta]$ 
is large, i.e., programs with high posterior probability,
which is the search objective during waking.

%% or programs which are a priori likely and which solve the task.
%% We find programs solving a task by
%% enumerating programs
%% from the DSL in decreasing order of their probability
%% under the recognition model,
%% and then check if a program $p$ assigns positive probability to a task
%% ($\probability[x|p] > 0$); if so, we incorporate $p$ into the beam
%% $\mathcal{B}_x$. We represent programs as polymorphicly typed $\lambda$-calculus expressions,
%% an expressive formalism
%% including conditionals,
%% variables,
%% higher-order recursive functions,
%% and the ability to define new functions.
\\\noindent \textbf{Sleep (Consolidation): Maxing $ \lowerBound$ w.r.t.\ the DSL.} Here $\left\{\mathcal{B}_x \right\}_{x\in X}$ is held fixed and the problem is to search the discrete space of DSLs and find one maximizing $\int \lowerBound \;\mathrm{d}\theta$,
and then update $\theta$ to $\argmax_\theta \lowerBound(\mathcal{D},\theta,\left\{\mathcal{B}_x \right\})$.

Finding programs solving tasks is difficult because of the infinitely
large, combinatorial search landscape. We ease this difficulty by
training a neural recognition model, $Q(p |x )$, during the
\textbf{Dreaming} phase: $Q$ is trained to approximate the posterior
over programs, $Q(p|x)\approx
\probability[p|x,\mathcal{D}]\propto\probability[x|p]\probability[p|\mathcal{D}]$.
Thus training the neural network amortizes the cost of finding
programs with high posterior probability.

\noindent\textbf{Sleep (Dreaming): tractably maxing $\lowerBound$ w.r.t. the
  beams.}  Here we train %% a neural network, $q$, to predict a
%% distribution over programs conditioned on a task. The objective of $q$
%% is
$Q(p|x)$ to assign high probability to programs $p$ where
$\probability[x|p]\probability[p|\mathcal{D},\theta]$ is large, because incorporating those programs
into the beams will most increase $\lowerBound$.


\subsection{\system pseudocode}\label{systemPseudocode}

Algorithm~\ref{theWholeThing} specifies how we integrate wake and
sleep cycles.
\begin{algorithm}
  \caption{Full \system algorithm}\label{theWholeThing}
  \begin{algorithmic}[1]
    \State \textbf{function} \systemEnding$(\mathcal{D}, X)$:
    \State {\bfseries Input:} Initial DSL $\mathcal{D}$, tasks $X$
    \State \textbf{Output:} Infinite stream of DSLs, recognition models, and beams
    \State \textbf{Hyperparameters:} Batch size $B$, enumeration timeout $T$, maximum beam size $F$
    \State $\theta\gets $uniform distribution
    \State $\mathcal{F}_x\gets\varnothing $, $\forall x\in X$\Comment{Initialize beams to be empty}
    \While{true}\Comment{Loop over epochs}
    \State shuffle $\gets$ random permutation of $X$\Comment{Randomize minibatches}
    \While{shuffle is not empty}\Comment{Loop over minibatches}
    \State batch $\gets$ first $B$ elements of shuffle\Comment{Next minibatch of tasks}
    \State shuffle $\gets$ shuffle with first $B$ elements removed
    \State $\forall x\in \text{batch}$: $\mathcal{F}_x\gets\mathcal{F}_x\cup \left\{p\;|\;p\in \text{enumerate}(\probability[\cdot |\mathcal{D},\theta],T)\text{ if }\probability[x|p] > 0 \right\}$\Comment{Wake}
    \State Train $Q(\cdot |\cdot )$ to minimize $\mathcal{L}^{\text{MAP}}$ across all $\left\{\mathcal{F}_x \right\}_{x\in X}$\Comment{Dream Sleep}
    \State $\forall x\in \text{batch}$: $\mathcal{F}_x\gets\mathcal{F}_x\cup \left\{p\;|\;p\in \text{enumerate}(Q(\cdot |x),T)\text{ if }\probability[x|p] > 0 \right\}$\Comment{Wake}
    \State $\forall x\in \text{batch}$: $\mathcal{F}_x\gets\text{ top }F\text{ elements of }\mathcal{F}_x\text{ as measured by }\probability[\cdot |x,\mathcal{D},\theta]$\Comment{Keep top $F$ programs}
    \State $\mathcal{D},\theta,\left\{\mathcal{F}_x \right\}_{x\in X}\gets $\textsc{Consolidate}$(\mathcal{D},\theta,\left\{\mathcal{F}_x \right\}_{x\in X})$\Comment{Consolidation Sleep}
    \State \textbf{yield} $(\mathcal{D},\theta)$, $Q$, $\left\{\mathcal{F}_x \right\}_{x\in X}$ \Comment{Yield the updated DSL, recognition model, and solutions found to tasks}
    \EndWhile
    \EndWhile

  \end{algorithmic}
\end{algorithm}

\subsection{Generative model over programs}\label{generativeAppendix}
Algorithm~\ref{sampleProgram} gives a stochastic procedure for drawing samples from $\probability[\cdot |\mathcal{D},\theta]$.
It takes as input the desired type of the unknown program,
and performs type inference during
sampling to ensure that
the program has the desired type.
It also maintains a \emph{environment}
mapping variables to types,
which ensures
that lexical scoping rules are obeyed.
\begin{algorithm}
  \caption{Generative model over programs}
  \label{sampleProgram}
  \begin{algorithmic}[1]
    \State \textbf{function} sample$(\mathcal{D}, \theta, \tau)$:
    \State {\bfseries Input:} DSL $(\mathcal{D},\theta)$, type $\tau$
    \State \textbf{Output:} a program whose type unifies with $\tau$
    \State  \textbf{return }sample$'(\mathcal{D}, \theta, \varnothing, \tau)$

    \Statex
    \State \textbf{function} sample$'(\mathcal{D}, \theta, \mathcal{E}, \tau)$:
    \State {\bfseries Input:} DSL $(\mathcal{D},\theta)$, environment $\mathcal{E}$, type $\tau$\Comment{Environment $\mathcal{E}$ starts out as $\varnothing$}
    \State \textbf{Output:} a program whose type unifies with $\tau$
    \If{$\tau = \alpha\to\beta$}\Comment{Function type --- start with a lambda}
    \State var $\gets$ an unused variable name
    \State body $\sim$ sample$'(\mathcal{D},\theta,\{\text{var}:\alpha\}\cup\mathcal{E},\beta)$\Comment{Recursively sample function body}
    \State \textbf{return} \code{(lambda (}var\code{) }body\code{)}
    %\Endif
    \Else\Comment{Build an application to give something w/ type $\tau$}
    \State $\text{primitives} \gets \{p | p: \tau' \in \mathcal{D}\cup\mathcal{E}$
    $\text{if }\tau\text{ can unify with yield}(\tau') \}$\Comment{Everything in scope w/ type $\tau$}
    \State $\text{variables}\gets\left\{p\;|\;p\in \text{primitives}\text{ and }p\text{ a variable} \right\}$
    \State Draw $e\sim \text{primitives}$, w.p. $\propto\begin{cases}
    \theta_e&\text{ if }e\in \mathcal{D}\\
    \theta_{var}/|\text{variables}|&\text{ if }e\in \mathcal{E}
    \end{cases}$
    \State Unify $\tau$ with yield$(\tau')$.\Comment{Ensure well-typed program}
    \State $\left\{\alpha_k \right\}_{k = 1}^K\gets\text{args}(\tau')$ 
    %   \State unify$(\tau,\beta)$
    \For{$k=1$ {\bfseries to} $K$}\Comment{Recursively sample arguments}
    \State $a_k\sim\text{sample}'(\mathcal{D},\theta,\mathcal{E},\alpha_k)$
    \EndFor
    \State \textbf{return} \code{(}$e\;a_1\; a_2\; \cdots\; a_K$\code{)}
    \EndIf
    \Statex
    \Statex\textbf{where:}
    \State yield$(\tau) = \begin{cases}
      \text{yield}(\beta)   &\text{ if }\tau = \alpha\to \beta\\
      \tau   &\text{ otherwise.}
    \end{cases}$ \Comment{Final return type of $\tau$}
    \State  args$(\tau) = \begin{cases}
      [\alpha] + \text{args}(\beta)   &\text{ if }\tau = \alpha\to \beta\\
      []   &\text{ otherwise.}
    \end{cases}$\Comment{Types of arguments needed to get something w/ type $\tau$}
  \end{algorithmic}
\end{algorithm}

\subsection{Enumerative program search}\label{enumerationAppendix}

Our current implementation of \system takes the simple and generic strategy of enumerating programs in
descending order of their probability under either $(\mathcal{D},\theta)$ or $Q(p|x)$.
Algorithm~\ref{sampleProgram} and~\ref{recognitionSample}
specify procedures for sampling
from these distributions,
but not for enumerating from them.
We combine two different enumeration strategies,
which allowed us to build a massively parallel program enumerator:
\begin{itemize}
\item \textbf{Best-first search:} Best-first search maintains a heap of
  partial programs ordered by their
  probability --- here a partial program means a program whose syntax tree
  may contain unspecified `holes'. Best-first search
  is guaranteed to enumerate programs in decreasing order of their probability,
  and has memory requirements that in general grow exponentially as a function of the description length of programs in the heap (thus linearly as a function of run time).
\item \textbf{Depth-first search:} Depth first search
  recursively explores the space of
  execution traces through Algorithm~\ref{sampleProgram} and~\ref{recognitionSample},
  equivalently maintaining a stack of partial programs.  
  In general it does not enumerate programs
  in decreasing order of probability,
  but has memory requirements that grow linearly as a function of the description length of the programs in the stack (thus logarithmically as a function of run time).
\end{itemize}

Our parallel enumeration algorithm (Algorithm~\ref{enumerationAlgorithm})
first performs a best-first search
until the best-first heap
is much larger than the number of CPUs.
At this point,
it switches to performing many depth-first searches in parallel,
initializing a depth first search
with one of the
entries in the best-first heap.
Because depth-first search does not
produce programs in decreasing order of their probability,
we wrap this entire procedure up into an outer loop
that first enumerates programs whose description length is between
$0$ to $\Delta$,
then programs with description length between $\Delta$ and $2\Delta$,
then $2\Delta$ to $3\Delta$, etc.,
until a timeout is reached.
This is similar in spirit to iterative deepening depth first search~\cite{Russell:2003:AIM:773294}.
 \begin{algorithm}
   \caption{Parallel enumerative program search algorithm }
   \label{enumerationAlgorithm}
   \begin{algorithmic}[1]
     \State \textbf{function} enumerate$(\mu, T, \text{CPUs})$:
     \State {\bfseries Input:} Distribution over programs $\mu$, timeout $T$, CPU count
     \State \textbf{Output:} stream of programs in approximately descending order of probability under $\mu$
     \State \textbf{Hyperparameter:} nat increase rate $\Delta$\Comment{We set $\Delta = 1.5$}
     \State lowerBound$\gets 0$
     \While{total elapsed time $ < T$}
     \State heap$\gets$newMaxHeap()\Comment{Heap for best-first search}
     \State heap.insert$(\text{priority} = 0,\text{value} = \text{empty syntax tree})$\Comment{Initialize heap with start state of search space}
     \While{$0 < |\text{heap}|\leq 10\times \text{CPUs}$}\Comment{Each CPU will get approximately 10 jobs (a partial program)}
     \State priority, partialProgram $\gets$ heap.popMaximum()
     \If{partialProgram is finished}\Comment{Nothing more to fill in in the syntax tree}
     \If{$\text{lowerBound}\leq -\text{priority} < \text{lowerBound} + \Delta$}
     \State \textbf{yield }partialProgram
     \EndIf
     \Else
     \For{child$\in $children(partialProgram)}\Comment{children$(\cdot )$ fills in next random choice in syntax tree.}
     \If{$-\log \mu(\text{child}) < \text{lowerBound} + \Delta$}\Comment{Child's description length small enough}
     \State heap.insert$(\text{priority} = \log \mu(\text{child}),\text{value} = \text{child})$
     \EndIf
     \EndFor
     \EndIf
     \EndWhile
     \State \textbf{yield from }ParallelMap$_\text{CPUs}(\text{depthFirst}(\mu,T - \text{elapsed time}, \text{lowerBound}, \cdot ), \text{heap.values()})$%\Comment{Launch parallel workers}
     \State $\text{lowerBound}\gets\text{lowerBound} + \Delta$\Comment{Push up lower bound on MDL by $\Delta$}
     \EndWhile
     \Statex
     \State \textbf{function} depthFirst$(\mu,T,\text{lowerBound},\text{partialProgram})$: \Comment{Each worker does a depth first search. Enumerates completions of partialProgram whose MDL is between lowerBound and $\text{lowerBound} + \Delta$}
     \State stack$\gets$[partialProgram]
     \While{$\text{total elapsed time} < T$ and $\text{stack}$ is not empty}
     \State partialProgram$\gets$stack.pop()
     \If{partialProgram is finished}
     \If{$\text{lowerBound}\leq-\log \mu(\text{partialProgram}) < \text{lowerBound} + \Delta$}
     \State \textbf{yield }partialProgram
     \EndIf
     \Else
     \For{$\text{child}\in \text{children}(\text{partialProgram})$}
     \If{$-\log \mu(\text{child}) < \text{lowerBound} + \Delta$}\Comment{Child's description length small enough}
     \State stack.push$(\text{child})$
     \EndIf
     \EndFor
     \EndIf

     \EndWhile
   \end{algorithmic}
 \end{algorithm}

 \subsection{Details of DSL learning}\label{appendixCompression}

 Algorithm~\ref{grammarInductionAlgorithm} specifies our DSL learning procedure.
 This integrates two toolkits:
 the machinery of version spaces and equivalence graphs (Appendix~\ref{appendixVersion})
 along with a probabilistic objective favoring compressive DSLs.
 The functions $I\beta(\cdot )$ and \textsc{refactor} construct
a version space from a program and extract the shortest program from a version space, respectively (Algorithm~\ref{grammarInductionAlgorithm}, lines 5-6, 14; Appendix~\ref{appendixVersion}).
To define the prior distribution over $(\mathcal{D},\theta})$ (Algorithm~\ref{grammarInductionAlgorithm}, lines 7-8), we penalize the syntactic complexity of the $\lambda$-calculus expressions in the DSL, defining $    \probability[\mathcal{D}]\propto\exp(-\lambda\sum_{p\in \mathcal{D}}\text{size}(p) )$ where $\text{size}(p)$  measures the size of the syntax tree of program $p$,
  and $\lambda$ controls how strongly we regularize the size of the DSL.
  We place a symmetric Dirichlet prior over the weight vector $\theta$.

To appropriately score each proposed $\mathcal{D}$ we must reestimate
 the weight vector $\theta$ (Algorithm~\ref{grammarInductionAlgorithm}, line 7).
Although this  may seem 
very similar to estimating the parameters of a probabilistic context free grammar,
for which we have effective approaches like the Inside/Outside algorithm~\cite{international2000derivation},
our DSLs are context-sensitive due to the presence of variables
in the programs and also due to the polymorphic typing system.
Appendix~\ref{mapAppendix} derives a tractable MAP estimator for $\theta$.



\begin{algorithm}%[H]
  %\setcounter{algorithm}{2} % there are two algorithms in the main paper.
  \caption{DSL Induction Algorithm}
  \label{grammarInductionAlgorithm}
  \begin{algorithmic}[1]
    \State {\bfseries Input:} Set of beams $\{\mathcal{B}_x\}$
    %         \STATE \textbf{Hyperparameters:} Pseudocounts $\alpha$, regularization parameter $\lambda$
    \State \textbf{Output:} DSL $\mathcal{D}$, weight vector $\theta$
    \State $\mathcal{D}\gets$ every primitive in $\{\mathcal{B}_x\}$
    \While{true}         
    \State $\forall p\in \bigcup_{x}\mathcal{B}_x: $ $v_p\gets I\beta(p)$ \Comment{Construct a version space for each program}
    \State Define $L(\mathcal{D}',\theta) =  \prod_x \sum_{p\in \mathcal{B}_x} \probability[x|p]\probability[\text{\textsc{refactor}}(v_p|\mathcal{D}')|\mathcal{D}',\theta]$ \Comment{Likelihood if $(\mathcal{D}',\theta)$ were the DSL}
    \State Define $\theta^*(\mathcal{D}') = \argmax_\theta \probability[\theta|\mathcal{D}'] L(\mathcal{D}',\theta)$ \Comment{MAP estimate of $\theta$}
    \State Define $\text{score}(\mathcal{D}') = \log \probability[\mathcal{D}'] + L(\mathcal{D}',\theta^*(\mathcal{D})) - \|\theta^*(\mathcal{D})\|_0$ \Comment{objective function}
    \State components $\gets$ $\left\{\textsc{refactor}(v|\mathcal{D})\;:\;\forall x, \forall p\in \mathcal{B}_x, \forall v\in \text{children}(v_p) \right\}$ \Comment{Propose many new DSL components}
    \State proposals $\gets$ $\left\{\mathcal{D}\cup\left\{c \right\}\;:\;\forall c\in \text{components} \right\}$ \Comment{Propose many new DSLs}
    \State $\mathcal{D}'\gets \argmax_{\mathcal{D}'\in \text{proposals}}\text{score}(\mathcal{D}') $\Comment{Get highest scoring new DSL}
    \State \textbf{if }$\text{score}(\mathcal{D}') < \text{score}(\mathcal{D})$\textbf{ return }$\mathcal{D},\theta^*(\mathcal{D})$\Comment{No changes to DSL led to a better score}
    \State $\mathcal{D}\gets\mathcal{D}'$ \Comment{Found better DSL. Update DSL.}
    \State $\forall x\;:\;\mathcal{B}_x\gets\left\{\text{\textsc{refactor}}(v_p|\mathcal{D})\;:\; p\in \mathcal{B}_x\right\}$\Comment{Refactor beams in terms of new DSL}
    \EndWhile
  \end{algorithmic}
\end{algorithm}

 \subsubsection{Refactoring code with version spaces}\label{appendixVersion}
 Formally, a version space is either:
\begin{itemize}
\item A deBuijn\footnote{deBuijn indices are an alternative way of naming variables in $\lambda$-calculus. When using deBuijn indices, $\lambda$-abstractions are written \emph{without} a variable name, and variables are written as the count of the number of $\lambda$-abstractions up in the syntax tree the variable is bound to. For example, $\lambda x.\lambda y. (x\;y)$ is written $\lambda\lambda (\$1\;\$0)$ using deBuijn indices. See~\cite{pierce} for more details.} index: written $\$i$, where $i$ is a natural number
\item An abstraction: written $\lambda v$, where $v$ is a version space
\item An  application: written $(f \;x)$, where both  $f$ and $x$ are version spaces
\item A  union: $\uplus V$, where $V$ is a set of version spaces
\item The empty set, $\varnothing$
\item The set of all $\lambda$-calculus expressions, $\Lambda$
\end{itemize}
The purpose of a version space to compactly represent a set of programs.
We refer to this set as the \textbf{extension} of the version space:
\begin{definition}
  The \textbf{extension} of a version space $v$ is written $\denotation{v}$
  and is defined recursively as:
  \begin{align*}
    \denotation{\$i}& = \left\{\$i \right\}&
    \denotation{\lambda v}& = \left\{\lambda e : e\in \denotation{v} \right\}&
    \denotation{(v_1\; v_2)}& = \left\{(e_1\;e_2) : e_1\in \denotation{v_1},\;e_2\in \denotation{v_2} \right\}\\
    \denotation{\uplus V}& = \left\{e : v\in V,\;e\in \denotation{v} \right\}&
    \denotation{\varnothing}& = \varnothing&
    \denotation{\Lambda}& = \Lambda
    \end{align*}
\end{definition}
Version spaces also support efficient membership checking, which we
write as  $e\in \denotation{v}$.
Important for our purposes, it is also efficient to refactor the
members of a version space's extension in terms of a new DSL.
We define $\textsc{refactor}(v|\mathcal{D})$ inductively as:
\begin{align*}
  \textsc{refactor}(v|\mathcal{D}) &= \begin{cases}
    e\text{, if $e\in \mathcal{D}$ and $e\in \denotation{v}$. Exploits the fact that $\denotation{e}\in v$ can be efficiently computed.}\\
    \textsc{refactor}'(v|\mathcal{D})\text{, otherwise.}
  \end{cases}
\end{align*}\vspace{-\baselineskip}%
\begin{align*}
  \textsc{refactor}'(e|\mathcal{D})& = e\text{, if $e$ is a leaf}&%\\\nonumber
  \textsc{refactor}'(\lambda b|\mathcal{D}) &= \lambda \textsc{refactor}(b|\mathcal{D})\\\nonumber
  \textsc{refactor}'(f\;x|\mathcal{D}) &= \textsc{refactor}(f|\mathcal{D})\;\textsc{refactor}(x|\mathcal{D})&%\\\nonumber
  \textsc{refactor}'(\uplus V|\mathcal{D}) &= \argmin_{e\in \left\{\textsc{refactor}(v|\mathcal{D})\;:\;v\in V \right\}}\text{size}(e|\mathcal{D})\nonumber
  \end{align*}
where $\text{size}(e|\mathcal{D})$ for program $e$ and DSL $\mathcal{D}$
is the size of the syntax tree of $e$,
when members of $\mathcal{D}$ are counted as having size 1.
Concretely, $  \textsc{refactor}(v|\mathcal{D})$ calculates $\argmin_{p\in \denotation{v}}\text{size}(p|\mathcal{D})$.


Recall that our goal is to define an operator over version spaces, $I\beta_n$,
which calculates the set of $n$-step refactorings of a program $p$,
e.g.,
the set of all programs $p'$ where
$p'\underbrace{\reduce q\reduce\cdots\reduce}_{\text{$\leq n$ times}} p$,
where $a \reduce b$
is the standard notation
for $a$ rewriting to $b$ according to the standard rewrite rules of $\lambda$-calculus~\cite{pierce}.

We define this operator in terms of another operator, $I\beta'$, which performs a single step of refactoring:
\begin{align*}
  I\beta_n(v)& = \uplus \left\{ \underbrace{I\beta'(I\beta'(I\beta'(\cdots}_{i \text{ times}} v))) \;:\; 0\leq i \leq n \right\}
\end{align*}
where
  \begin{align*}
    I\beta'(u)& = \uplus \left\{(\lambda b)v\;:\;v\mapsto b\in S_0(u)%\text{, when }v\not=\Lambda
    \right\}\cup
  \begin{cases}
    \text{if $u$ is a primitive or index or $\varnothing $:}&\varnothing\\
    \text{if $u$ is $\Lambda$:}&\left\{\Lambda \right\}\\
    \text{if $u = \lambda b$:}&\left\{\lambda I\beta'(b) \right\}\\
    \text{if $u = (f\;x)$:}&\left\{(I\beta'(f)\;x),\;(f\;I\beta'(x)) \right\}\\
    \text{if $u = \uplus V$:}&\left\{I\beta'(u')\;|\;u'\in V \right\}
  \end{cases}  
  \end{align*}
  where we have defined $I\beta'$
  in terms of another operator, $S_k:\text{VS}\to 2^{\text{VS}\times\text{VS}}$,
  whose purpose is to construct the set of
  substitutions that are refactorings of a program in a version space.
We define $S$ as:
  \begin{align*}
  S_k(v)& = \left\{\downshift{k}_0v\mapsto \$k \right\}\cup
  \begin{cases}
    \text{if $v$ is primitive:}&\left\{\Lambda\mapsto v \right\}\\    
    \text{if $v = \$i$ and $i < k$:}&\left\{\Lambda\mapsto \$i \right\}\\
    \text{if $v = \$i$ and $i\geq k$:}&\left\{\Lambda\mapsto \$(i + 1) \right\}\\
    \text{if $v = \lambda b$:}&\left\{v'\mapsto \lambda b' \;:\; v'\mapsto b'\in S_{k + 1}(b)  \right\}\\
    \text{if $v = (f\;x)$:}&\left\{v_1\cap v_2\mapsto (f'\;x') \;:\; v_1\mapsto f'\in S_k(f),\; v_2\mapsto x'\in S_k(x) \right\}\\
    \text{if $v = \uplus V$:}&\bigcup_{v'\in V}S_n(v')\\
    \text{if $v$ is $\varnothing$:}&\varnothing\\
    \text{if $v$ is $\Lambda$:}&\left\{\Lambda\mapsto\Lambda \right\}
  \end{cases}\\
  \downshift{k}_c \$i& = \$i\text{, when $i < c$}\\
  \downshift{k}_c \$i& = \$(i - k)\text{, when $i\geq c + k$}\\
  \downshift{k}_c \$i& = \varnothing \text{, when $c\leq i <  c + k$}\\
  \downshift{k}_c \lambda b &= \lambda\downshift{k}_{c + 1}b\\
  \downshift{k}_c (f\;x)& = (\downshift{k}_cf\;\downshift{k}_cx)\\
  \downshift{k}_c \uplus V& = \uplus \left\{\downshift{k}_c v \;|\;v\in V \right\}\\
  \downshift{k}_c v& = v\text{, when }v\text{ is a primitive or }\varnothing \text{ or }\Lambda
\end{align*}
where $\shift{k}$ is the shifting operator~\cite{pierce},
which adds $k$ to all of the free variables in a $\lambda$-expression or version space,
and we have defined a new operator, $\downshift{}$, whose purpose is to
undo the action of $\shift{}$.
We have written definitions recursively,
but implement them using a dynamic program:
we hash cons each version space,
and only calculate the operators $I\beta_n$,
$I\beta'$, and $S_k$ once per each version space.
%Refactoring is similarly done more quickly with dynamic programming (see Equation~\ref{FACTORING} for the recursive definition of \textsc{refactor}).

We now formally prove that $I\beta$ exhaustively enumerates the space of possible refactorings.
Our approach is to first prove that $S_k$ exhaustively enumerates
the space of possible substitutions that
could give rise to a program.
The following pair of technical lemmas are useful; both are easily proven by structural induction.
\begin{lemma}
  Let $e$ be a program or version space and $n$, $c$ be natural numbers. \\Then $\shift{-1}_{n + c}\shift{n + 1}_ce = \shift{n}_c e$,
  and in particular $\shift{-1}_{n}\shift{n + 1}e = \shift{n} e$.
\label{neutralizeShift}\end{lemma}
\begin{lemma}
  Let $e$ be a program or version space and $n$, $c$ be natural numbers. \\Then $\downshift{n}_{c}\shift{n}_ce =  e$,
  and in particular $\downshift{n}\shift{n}e =  e$.
  \label{neutralizeDown}
\end{lemma}


\begin{theorem}
  \textbf{Consistency of $S_n$}. \\If $(v\mapsto b)\in S_n(u)$ then
  for every $v'\in v$ and $b'\in b$ we have $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b'\in u$.
\end{theorem}
\begin{proof}
  Suppose $b = \$n$ and therefore, by the definition of $S_n$, also $v = \downshift{n}_0 u$.
  Invoking Lemmas~\ref{neutralizeShift} and~\ref{neutralizeDown}
  we know that $u = \shift{-1}_n\shift{n + 1}v$
  and so for every $v'\in v$ we have $\shift{-1}_n\shift{n + 1}v'\in u$.
  Because $b = \$n = b'$ we can rewrite this to $\shift{-1}_n[\$n\mapsto \shift{n + 1}v']b'\in u$.
  

  
  Otherwise assume $b\not= \$n$ and proceed by structural induction on $u$:

  \begin{itemize}
  \item If $u = \$i < n$ then we have to consider the case that $v = \Lambda$ and $b = u = \$i = b'$.
    Pick $v'\in \Lambda$ arbitrarily. Then $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b' = \shift{-1}_n\$i = \$i\in u$.
  \item If $u = \$i\geq n$ then we have consider the case that $v = \Lambda$ and $b = \$(i + 1) = b'$.
    Pick $v'\in \Lambda$ arbitrarily. Then $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b' = \shift{-1}_n\$(i + 1) = \$i\in u$.
  \item If $u$ is primitive then we have to consider the case that $v = \Lambda$ and $b = u = b'$.
    Pick $v'\in \Lambda$ arbitrarily. Then $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b' = \shift{-1}_nu = u\in u$.
  \item If $u$ is of the form $\lambda a$,
    then $S_n(u)\subset\left\{v\mapsto \lambda b\;|\; (v\mapsto b)\in S_{n + 1}(a) \right\}$.
    Let $v\mapsto \lambda b\in S_n(u)$.
    By induction for every $v'\in v$ and $b'\in b$ we have $\shift{-1}_{n + 1}[\$n\mapsto \shift{2 + n}v']b'\in a$,
    which we can rewrite to $\shift{-1}_{n}[\$n\mapsto \shift{1 + n}v']\lambda b'\in \lambda a = u$.
  \item If $u$ is of the form $(f\;x)$ then
    then $S_n(u)\subset\left\{v_f\cap v_x\mapsto (b_f\;b_x)\;|\; (v_f\mapsto b_f)\in S_{n}(f),\;(v_x\mapsto b_x)\in S_{n}(x) \right\}$.
    Pick $v'\in v_f\cap v_x$ arbitrarily.
    By induction for every $v_f'\in v_f$, $v_x'\in v_x$, $b_f'\in b_f$, $b_x'\in b_x$
    we have $\shift{-1}_{n}[\$n\mapsto \shift{1 + n}v_f'] b_f' \in f$ and $\shift{-1}_{n}[\$n\mapsto \shift{1 + n}v_x'] b_x' \in x$.
    Combining these facts gives
    $\shift{-1}_{n}[\$n\mapsto \shift{1 + n}v'] (b_f'\;b_x') \in (f\;x) = u$.
  \item If $u$ is of the form $\uplus U$ then
    pick $(v\mapsto b)\in S_n(u)$ arbitrarily.
    By the definition of $S_n$ there is a $z$ such that $(v\mapsto b)\in S_n(z)$,
    and the theorem holds immediately by induction.
  \item If $u$  is $\varnothing $ or $\Lambda$ then the theorem holds vacuously.
    \end{itemize}
\end{proof}
\begin{theorem}
  \textbf{Completeness of $S_n$}.\\
  If there exists programs $v'$  and $b'$, and a version space $u$, such that $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b'\in u$,
  then there also exists $(v\mapsto b)\in S_n(u)$ such that $v'\in v$ and $b'\in b$.
\end{theorem}
\begin{proof}
  As before we first consider the case that $b' = \$n$.
  If so then
  $\shift{-1}_n\shift{1 + n}v'\in u$ or (invoking Lemma~\ref{neutralizeShift}) that $\shift{n}v'\in u$
  and (invoking Lemma~\ref{neutralizeDown}) that $v'\in \downshift{n}u$.
  From the definition of $S_n$ we know that $(\downshift{n}u\mapsto \$n)\in S_n(u)$ which is what was to be shown.

  Otherwise assume that $b'\not=\$n$. Proceeding by structural induction on $u$:
  \begin{itemize}
  \item If $u = \$i$ then,
    because $b'$ is not $\$n$,
    we have $\shift{-1}_nb' = \$i$.
    Let $b' = \$j$,
    and so
    $$
    i =
    \begin{cases}
      j&\text{ if }j < n\\
      j - 1&\text{ if }j > n
    \end{cases}
    $$
    where $j = n$ is impossible because by assumption $b'\not=\$n$.
    
    If $j < n$ then
    $i = j$ and so $u = b'$.
    By the definition of $S_n$ we have
    $(\Lambda\mapsto \$i)\in S_n(u)$,
    completing this inductive step because $v'\in \Lambda$ and $b'\in \$i$.
    Otherwise assume $j > n$
    and so $\$i = \$(j - 1) = u$.
    By the definition of $S_n$ we have
    $(\Lambda\mapsto \$(i + 1))\in S_n(u)$,
    completing this inductive step because $v'\in \Lambda$
    and $b' = \$j = \$(i + 1)$.
  \item If $u$ is a primitive then,
    because $b'$ is not $\$n$,
    we have $\shift{-1}_nb' = u$,
    and so $b' = u$.
    By the definition of $S_n$ we have $(\Lambda\mapsto u)\in S_n(u)$
    completing this inductive step because $v'\in \Lambda$ and $b' = u$.
  \item If $u$ is of the form $\lambda a$ then,
    because of the assumption that $b'\not=\$n$,
    we know that $b'$ is of the form $\lambda c'$ 
    and that $\lambda \shift{-1}_{n + 1}[\$(n + 1)\mapsto \shift{2 + n}v']c'\in \lambda a$.
    By induction this means that there is a $(v\mapsto c)\in S_{n + 1}(a)$
    satisfying $v'\in v$ and $c'\in c$.
    By the definition of $S_n$ we also know that
    $(v\mapsto  \lambda c)\in S_n(u)$,
    completing this inductive step because $b' = \lambda c'\in \lambda c$.
  \item If $u$ is of the form $(f\;x)$
    then,
    because of the assumption that $b'\not=\$n$,
    we know that $b'$ is of the form $(b_f'\;b_x')$
    and that both
    $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b_f'\in f$ and $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b_x'\in x$.
    Invoking the inductive hypothesis twice
    gives a
    $(v_f\mapsto b_f)\in S_n(f)$ satisfying $v'\in v_f$, $b_f'\in b_f$
    and a
    $(v_x\mapsto b_x)\in S_n(x)$ satisfying $v'\in v_x$, $b_x'\in b_x$.
    By the definition of $S_n$
    we know that $(v_f\cap v_x\mapsto b_f\;b_x)\in S_n(u)$
    completing the inductive step because $v'$ is guaranteed to be
    in both $v_f$ and $v_x$ and we know that
    $b' = (b_f'\;b_x')\in (b_f\;b_x)$.

  \item If $u$ is of the form $\uplus U$
    then there must be a $z\in U$ such that $\shift{-1}_n[\$n\mapsto \shift{1 + n}v']b'\in z$.
    By induction there is a $(v\mapsto b)\in S_n(z)$ such that $v'\in v$ and $b'\in v$.
    By the definition of $S_n$ we know that $(v\mapsto b)$ is also in $S_n(u)$ completing the inductive step.
  \item If $u$  is $\varnothing $ or $\Lambda$ then the theorem holds vacuously.    
    \end{itemize}
\end{proof}

From these results the consistency and completeness of $I\beta$ follows:
\begin{theorem}
  \textbf{Consistency of $I\beta'$.}\\
  If $p\in \denotation{I\beta'(u)}$ then there exists $p'\in \denotation{u}$ such that $p\reduce p'$.
\end{theorem}
\begin{proof}
  Proceed by structural induction on $u$. If $p\in \denotation{I\beta'(u)}$ then, from the definition of $I\beta'$ and $\denotation{\cdot }$, at least one of the following holds:
  \begin{itemize}
  \item Case $p = (\lambda b')v'$ where $v'\in v$, $b'\in b$, and $v\mapsto b\in S_0(u)$:
    From the definition of $\beta$-reduction we know that $p\reduce \shift{-1}[\$0\mapsto \shift{1}v']b'$.
    From the consistency of $S_n$ we know that $\shift{-1}[\$0\mapsto \shift{1}v']b'\in u$.
    Identify $p' = \shift{-1}[\$0\mapsto \shift{1}v']b'$.
  \item Case $u = \lambda b$ and $p = \lambda b'$
    where $b'\in \denotation{I\beta'(b)}$:
    By induction there exists $b''\in \denotation{b}$ such that $b'\reduce b''$.
    So $p \reduce \lambda b''$.
    But $\lambda b''\in \denotation{\lambda b} = \denotation{u}$,
    so identify $p' = \lambda b''$.
  \item Case $u = (f\; x)$ and $p = (f'\; x')$ where $f'\in \denotation{I\beta'(f)}$ and $x'\in \denotation{x}$:
    By induction there exists $f''\in \denotation{f}$
    such that $f'\reduce f''$.
    So $(f'\;x')\reduce (f''\;x')$.
    But $(f''\;x')\in \denotation{(f\;x)} = \denotation{u}$,
    so identify $p' = (f''\;x')$.
  \item  Case $u = (f\; x)$ and $p = (f'\; x')$ where $x'\in \denotation{I\beta'(x)}$ and $f'\in \denotation{f}$: Symmetric to the previous case.
  \item Case $u = \uplus U$ and $p \in \denotation{I\beta'(u')}$ where $u'\in U$:
    By induction there is a $p'\in \denotation{u'}$ satisfying $p'\reduce p$.
    But $\denotation{u'}\subseteq \denotation{u}$, so also $p'\in \denotation{u}$.
  \item Case $u$ is an index, primitive, $\varnothing$, or $\Lambda$: The theorem holds vacuously.    
    \end{itemize}  
\end{proof}
\begin{theorem}
  \textbf{Completeness of $I\beta'$.}\\
  Let $p\reduce p'$ and $p'\in \denotation{u}$.
  Then $p\in \denotation{I\beta'(u)}$.
\end{theorem}
\begin{proof}
  Structural induction on $u$.
  If $u = \uplus V$
  then there is a $v\in V$
      such that $p' \in \denotation{v}$;
      by induction on $v$ combined with the definition of $I\beta'$ we have $p\in \denotation{I\beta'(v)}\subseteq \denotation{I\beta'(u)}$,
      which is what we were to show.
      Otherwise assume that $u\not=\uplus V$.
  
  From the definition of $p\reduce p'$
  at least one of these cases must hold:
  \begin{itemize}
  \item Case $p = (\lambda b')v'$ and $p' = \shift{-1}[\$0\mapsto \shift{1}v']b'$:
    Using the fact that $\shift{-1}[\$0\mapsto \shift{1}v']b'\in \denotation{u}$, we can invoke the completeness of $S_n$ to construct a $(v\mapsto  b)\in S_0(u)$
    such that $v'\in \denotation{v}$
    and $b'\in \denotation{b}$.
    Combine these facts with the definition of $I\beta'$
    to get
    $p = (\lambda b')v'\in \denotation{(\lambda b)v}\subseteq I\beta'(u)$.
  \item Case $p = \lambda b$ and $p' = \lambda b'$ where
    $b\reduce b'$:
    Because $p' = \lambda b'\in \denotation{u}$
    and by assumption $u\not=\uplus V$,
    we know that $u = \lambda v$ and $b'\in \denotation{v}$. By
    induction $b\in \denotation{I\beta'(v)}$.
    Combine with the definition of $I\beta'$ to get $p = \lambda b \in \denotation{\lambda I\beta'(v)}\subseteq \denotation{I\beta'(u)}$.
  \item Case $p = (f\;x)$ and $p' = (f'\;x)$ where $f\reduce f'$:
    Because $p' = (f'\;x)\in \denotation{u}$
    and by assumption $u\not=\uplus V$
    we know that $u = (a\;b)$
    where $f'\in \denotation{a}$
    and $x\in \denotation{b}$.
    By induction on $a$
    we know
    $f\in \denotation{I\beta'(a)}$.
    Therefore
    $p = (f\;x)\in \denotation{(I\beta'(a)\; b)}\subseteq\denotation{I\beta'((a\;b))}\subseteq\denotation{I\beta'(u)}$.
  \item Case $p = (f\;x)$ and $p' = (f\;x')$ where $x\reduce x'$: Symmetric to the previous case.    
  \end{itemize}
\end{proof}
Finally we have our main result:
\begin{theorem}
  \textbf{Consistency and completeness of $I\beta_n$.}
  Let $p$ and $p'$
  be programs.
  Then $p\underbrace{\reduce q\reduce\cdots\reduce}_{\text{$\leq n$ times}} p'$  if and only
  if $p\in \denotation{I\beta_n(p')}$.
\end{theorem}
\begin{proof}
  Induction on $n$.

  If $n = 0$
  then $\denotation{I\beta_n(p')} = \left\{p \right\}$
  and $p = p'$;
  the theorem holds immediately. Assume $n > 0$.
  
  If $p\underbrace{\reduce q\reduce\cdots\reduce}_{\text{$\leq n$ times}} p'$
  then $q\underbrace{\reduce\cdots\reduce}_{\text{$\leq n - 1$ times}} p'$;
  induction on $n$
  gives $q\in \denotation{I\beta_{n - 1}(p')}$.
  Combined with $p\reduce q$ we can invoke the completeness of $I\beta'$
  to get $p\in \denotation{I\beta'(I\beta_{n - 1}(p'))}\subset\denotation{I\beta_n(p')}$.
  
  If $p\in \denotation{I\beta_n(p')}$
  then
  there exists a $i\leq n$
  such that $p\in \denotation{\underbrace{I\beta'(I\beta'(I\beta'(\cdots}_{i \text{ times}} p')))}$.
  If $i = 0$
  then $p = p'$ and $p$ reduces to $p'$ in $0\leq n$ steps.
  Otherwise $i > 0$
  and $p\in \denotation{I\beta'(\underbrace{I\beta'(I\beta'(\cdots}_{i - 1 \text{ times}} p'))}$.
  Invoking the
  consistency of $I\beta'$
  we know that $p\reduce q$
  for a program $q\in \denotation{\underbrace{I\beta'(I\beta'(\cdots}_{i - 1 \text{ times}} p'))}\subseteq\denotation{I\beta_{i - 1}(p')}$.
  By induction $q\underbrace{\reduce\cdots\reduce}_{\text{$\leq i - 1$ times}} p'$,
  which combined with $p\reduce q$
  gives  $p\underbrace{\reduce q\reduce\cdots\reduce}_{\text{$\leq i\leq n$ times}} p'$.  
\end{proof}

\subsubsection{Tracking equivalences}

%% First observe that
%% we do not \emph{enumerate} every refactoring,
%% but only those for which every expression of the form $(\lambda x.b)v$ has
%% $x$ free in $b$.
%% For example, we do not refactor $\code{+}$ into $(\lambda x. \code{+})v$ where $v$ is arbitrary,
%% because there are infinitely many such refactorings.
%% Define the following relation,
%% which captures this restricted kind of
%% refactoring:
%% \begin{gather*}
%%   \frac{f\reduce^\sim f'}{f\;x \reduce^\sim f'\;x}\qquad
%%   \frac{x\reduce^\sim x'}{f\;x \reduce^\sim f\;x'}\qquad
%%   \frac{b\reduce^\sim b'}{\lambda b \reduce^\sim \lambda b'}\qquad
%%   \frac{\$0 \text{ occurs free in }b}{(\lambda b)v\reduce^\sim \shift{-1}[\$0\mapsto \shift{1}v]b}
%%   \end{gather*}
%% \begin{theorem}
%%   \textbf{Completeness of $I\beta'$.}
%%   Let $v$ be a version space and let $p\in v$.
%%   If $p'\reduce^\sim p$
%%   then $p'\in I\beta'(v)$.
%% \end{theorem}
%% \begin{proof}
%%   We know (from the definition of $\reduce^\sim$) that $p'$ and $p$ differ only
%%   in that there exists a single subexpression in
%%   $p'$ of the form $(\lambda b)v$
%%   where that subexpression occurs in
%%   $p$ as $\shift{-1}[\$0\mapsto \shift{1}v]b$.
%%   From Theorem~\ref{}
%% \end{proof}


\subsubsection{Computational complexity of DSL learning}

How long does each update to the DSL in
Algorithm~\ref{grammarInductionAlgorithm} take?  Constructing the
version spaces takes time linear in the number of programs (written
$P$) in the beams (Algorithm~\ref{grammarInductionAlgorithm}, line
5), and, in the worst case, exponential time as a function of the
number of refactoring steps $n$ --- but we bound the number of steps
to be a small number (typically $n = 3$).  Writing $V$ for the number
of version spaces, this means that $V$ is $O(P2^n)$.  The number of
proposals (line 10) is linear in the number of distinct version
spaces, so is $O(V)$.  For each proposal we have to refactor every
program (line 6), so this means we spend $O(V^2) = O(P^22^n)$ per DSL
update.  In practice this quadratic dependence on $P$ (the number of
programs) is prohibitively slow.  We now describe a linear time
approximation to the refactor step in
Algorithm~\ref{grammarInductionAlgorithm} based on beam search.

For each version space $v$ we calculate a \emph{beam}, which is a
function from a DSL $\mathcal{D}$ to a shortest program in
$\denotation{v}$ using primitives in $\mathcal{D}$.  Our strategy will
be to only maintain the top $B$ shortest programs in the beam;
throughout all of the experiments in this paper, we set $B = 10^6$,
and in the limit $B\to\infty$ we recover the exact behavior of \textsc{refactor}.
The following recursive equations
define how we calculate these beams;
the set `proposals' is defined in line 10 of Algorithm~\ref{grammarInductionAlgorithm},
and $\mathcal{D}$ is the current DSL:
\begin{align*}
  \text{beam}_v(\mathcal{D}')& = \begin{cases}
    \text{if }\mathcal{D}'\in \text{dom}(b_v)\text{: }&b_v(\mathcal{D}')\\
    \text{if }\mathcal{D}'\not\in \text{dom}(b_v)\text{: }&\textsc{refactor}(v,\mathcal{D})
  \end{cases}\\
  b_v& = \text{the $B$ pairs $(\mathcal{D}'\mapsto p)$ in $b_v'$ where the syntax tree of $p$ is smallest}\\
  b_v'(\mathcal{D}')& = \begin{cases}
    \text{if $\mathcal{D}'\in \text{proposals}$ and $e\in \mathcal{D}'$ and  $e\in v$: }e\\
    \text{otherwise if $v$ is a primitive or index:} v
    \text{otherwise if $v = \lambda b$: } \lambda \text{beam}_b(\mathcal{D}')\\
    \text{otherwise if $v = (f\;x)$: } (\text{beam}_f(\mathcal{D}')\;\text{beam}_x\mathcal{D}'))\\
    \text{otherwise if $v = \uplus V$: } \argmin_{e\in \left\{b'_{v'}(\mathcal{D}')\;:\;v'\in V \right\}}\text{size}(e|\mathcal{D}')
    \end{cases}
  \end{align*}
We calculate $\text{beam}_v(\cdot )$ for each version space using
dynamic programming.  Using a minheap to represent
$\text{beam}_v(\cdot )$, this takes time $O(VB\log B)$, replacing the
quadratic dependence on $V$ (and therefore the number of programs,
$P$) with a $B\log B$ term, where the parameter $B$ can be chosen
freely, but at the cost of a less accurate beam search.

After performing this beam search,
we take only the top $I$ proposals as measured by $-\sum_x\min_{p\in \mathcal{F}_x}\text{beam}_{v_p}(\mathcal{D}')$.
We set $I = 300$ in all of our experiments,
so $I \ll B$.
The reason why we
only take the top $I$ proposals (rather than take the top $B$) is because
parameter estimation (estimating $\theta$ for each proposal) is much more expensive than
performing the beam search ---
so we perform a very wide beam search and then at the very end
tim the beam down to
only $I = 300$ proposals.
Next,
we describe our MAP estimator for the continuous parameters ($\theta$) of the DSL.



\subsubsection{Estimating the continuous parameters $\theta$ of a DSL}\label{mapAppendix}
We use an EM algorithm to estimate the continuous parameters of the DSL, i.e. $\theta$.
Suppressing dependencies on $\mathcal{D}$, the EM updates are
\begin{align}
\label{maximizeStep}  \theta& = \argmax_\theta \log P(\theta) + \sum_x \expect_{q_x}\left[\log \probability\left[p|\theta \right] \right]\\
  q_x(p)&\propto \probability[x|p]\probability[p|\theta]\indicator\left[p\in \mathcal{F}_x \right]
\end{align}
In the M step of EM we will update $\theta$ by instead maximizing a lower bound on $\log \probability[p|\theta]$,
making our approach an instance of Generalized EM.

We write $c(e,p)$ to mean the number of times that primitive $e$ was used in program $p$; $c(p)= \sum_{e\in \mathcal{D}}c(e,p)$ to mean the total number of primitives used in program $p$; $c(\tau,p)$ to mean the number of times that type $\tau$ was the input to sample in Algorithm~\ref{sampleProgram} while sampling program $p$. Jensen's inequality gives a lower bound on the likelihood:
\begin{align*}
  &\sum_x\expect_{q_x}\left[  \log \probability[p|\theta] \right] =\\
  &\sum_{e\in \mathcal{D}} \log \theta_e \sum_x\expect_{q_x}\left[c(e,p_x) \right] -
  \sum_\tau\expect_{q_x}\left[\sum_x c(\tau,p_x)  \right]\log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
 =   &\sum_e C(e)\log \theta_e  - \beta\sum_\tau\frac{\expect_{q_x}\left[\sum_x c(\tau,p_x)  \right]}{\beta}\log \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
 \geq    &\sum_e C(e)\log \theta_e  - \beta\log \sum_\tau\frac{\expect_{q_x}\left[\sum_x c(\tau,p_x)  \right]}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  \\
     =     &\sum_e C(e)\log \theta_e  - \beta\log \sum_\tau\frac{R(\tau)}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e  
  %% &\geq\sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log \frac{1}{c(p)}\sum_{\tau\in R(p)} \sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e\\
  %% & = \sum_{e\in \mathcal{D}} c(e,p)\log \theta_e - c(p)\log\frac{1}{c(p)} \sum_{e\in \mathcal{D}} r(e,p)\theta_e
\end{align*}
where we have defined
\begin{align*}
  C(e)&\triangleq  \sum_x\expect_{q_x}\left[c(e,p_x) \right]\\
  R(\tau)&\triangleq \expect_{q_x}\left[\sum_x c(\tau,p_x)  \right]\\
  \beta&\triangleq\sum_\tau \expect_{q_x}\left[\sum_x c(\tau,p_x)  \right]
\end{align*}
Crucially it was defining $\beta$ that let us use Jensen's inequality. 
Recalling from the main paper that $P(\theta)\triangleq\text{Dir}(\alpha)$,
we have the following lower bound on M-step objective:
\begin{align}
\sum_e (C(e) + \alpha)\log \theta_e  - \beta\log \sum_\tau\frac{R(\tau)}{\beta}\sum_{\substack{e:\tau'\in \mathcal{D}\\\text{unify}(\tau,\tau')}}\theta_e    
\end{align}
Differentiate with respect to $\theta_e$, where $e:\tau$, and set to zero to obtain:
\begin{align}
  &  \frac{C(e) + \alpha}{\theta_e}\propto\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')\\
&  \theta_e\propto\frac{C(e) + \alpha}{\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')}
\end{align}
The above is our estimator for $\theta_e$.
The above estimator has an intuitive interpretation.
The quantity $C(e)$ is the expected number of times that we used $e$.
The quantity $\sum_{\tau'}\indicator\left[\text{unify}(\tau,\tau') \right] R(\tau')$
is the expected number of times that we \emph{could have} used $e$.
The hyperparameter $\alpha$ acts as pseudocounts that are
added to the number of times that we used each primitive,
and are not added to the number of times that we could have used each primitive.


We are only maximizing a lower bound on the log posterior; when is this lower bound tight?
This lower bound is tight whenever all
of the types of the expressions in the DSL are not polymorphic, in which case our DSL is equivalent to a PCFG
and this estimator is equivalent to the inside/outside algorithm.
Polymorphism introduces context-sensitivity to the DSL,
and exactly maximizing the likelihood with respect to $\theta$
becomes intractable,
so for domains with polymorphic types we use this estimator.

\subsection{Recognition model training}\label{recognitionAppendix}

Recall that our goal is to maximize either $\mathcal{L}^{posterior}$ or $\mathcal{L}^{MAP}$, defined as:
\begin{align*}
  \mathcal{L}^{\text{posterior}} &= \mathcal{L}_{\text{Replay}}^{\text{posterior}} + \mathcal{L}_{\text{Fantasy}}^{\text{posterior}}&
  \mathcal{L}^{\text{MAP}} &= \mathcal{L}_{\text{Replay}}^{\text{MAP}} + \mathcal{L}_{\text{Fantasy}}^{\text{MAP}}\\
  \mathcal{L}_{\text{Replay}}^{\text{posterior}}& = \expect_{x\sim X}\left[\sum_{p\in \mathcal{F}_x}
    \frac{\probability\left[x,p|\mathcal{D},\theta \right]\log Q(p|x)}{\sum_{p'\in \mathcal{F}_x}\probability\left[x,p'|\mathcal{D},\theta \right]}\right] &
\mathcal{L}_{\text{Replay}}^{\text{MAP}}& = \expect_{x\sim X}\left[\max_{\substack{p\in \mathcal{F}_x\\p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]}} \log Q(p|x) \right]  \\
  \mathcal{L}_{\text{Fantasy}}^{\text{posterior}} &= \expect_{(p,x)\sim(\mathcal{D},\theta) }\left[\log Q(p|x)\right]&
    \mathcal{L}_{\text{Fantasy}}^{\text{MAP}} &= \expect_{x\sim(\mathcal{D},\theta) }\left[\max_{\substack{p\\p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]}}\log Q(p)\right]
\end{align*}
The fantasy objectives are essential for data efficiency:
all of our experiments train \system on only a few hundred tasks, which is too little for
a high-capacity neural network.
Once we bootstrap a $(\mathcal{D},\theta)$,
we can draw unlimited samples from $(\mathcal{D},\theta)$
and train $Q$ on those samples.
But, evaluating $\mathcal{L}_{\text{Fantasy}}$ involves drawing programs from
the current DSL, running them to get their outputs,
and then training $Q$ to regress from the input/outputs to the program.
Since these programs map inputs to outputs,
we need to sample the inputs as well.
Our solution is to sample the inputs
from the empirical observed distribution of inputs in $X$.

The $\mathcal{L}_{\text{Fantasy}}^{\text{MAP}}$ objective involves
finding the MAP program solving a task drawn from the DSL.  To make
this tractable, rather than \emph{sample} programs as training data
for $\mathcal{L}_{\text{Fantasy}}^{\text{MAP}}$, we \emph{enumerate}
programs in decreasing order of their prior probability, tracking, for
each dreamed task $x$, the set of enumerated programs maximizing
$\probability[x,p|\mathcal{D},\theta]$.

We parameterize $Q$ using a bigram model over syntax trees.
Formally,
$Q$ predicts a $(|\mathcal{D}|+2)\times (|\mathcal{D}|+1)\times A$-dimensional tensor,
where $A$ is the maximum arity\footnote{The arity of a function is the number of arguments that it takes as input.} of any primitive in the DSL.
Slightly abusing notation, we write this tensor as $Q_{ijk}(x)$,
where $x$ is a task,
$i\in \mathcal{D}\cup\left\{\text{start},\text{var}\right\}$,
$j\in \mathcal{D}\cup\left\{\text{var} \right\}$,
and $k\in \left\{1,2,\cdots,A \right\}$.
The output $Q_{ijk}(x)$
controls the probability of
sampling primitive $j$ given that
$i$ is the parent node in the syntax tree
and we are sampling the $k^{\text{th}}$ argument.
Algorithm~\ref{recognitionSample}
specifies a procedure for drawing samples from $Q(\cdot |X)$.
 \begin{algorithm}
   \caption{Drawing from distribution over programs predicted by recognition model. Compare w/ Algorithm~\ref{sampleProgram}}
   \label{recognitionSample}
   \begin{algorithmic}[1]
     \State \textbf{function} recognitionSample$(Q, x, \mathcal{D}, \tau)$:
     \State {\bfseries Input:} recognition model $Q$, task $x$, DSL $\mathcal{D}$, type $\tau$
     \State \textbf{Output:} a program whose type unifies with $\tau$
     \State \textbf{return } $\text{recognitionSample}'(Q,x,\text{start},1,\mathcal{D},\varnothing,\tau)$
     \Statex

     \State \textbf{function} recognitionSample$'(Q, x, \text{parent}, \text{argumentIndex}, \mathcal{D}, \mathcal{E}, \tau)$:
     \State {\bfseries Input:} recognition model $Q$, task $x$, DSL $\mathcal{D}$, parent $\in \mathcal{D}\cup\left\{\text{start},\text{var} \right\}$, argumentIndex $\in \mathbb{N}$, environment $\mathcal{E}$, type $\tau$
     \State \textbf{Output:} a program whose type unifies with $\tau$
     \If{$\tau = \alpha\to\beta$}\Comment{Function type --- start with a lambda}
     \State var $\gets$ an unused variable name
     \State body $\sim$ recognitionSample$'(Q,x,\text{parent},\text{argumentIndex},\mathcal{D},\{\text{var}:\alpha\}\cup\mathcal{E},\beta)$%\Comment{Recursively sample function body}
     \State \textbf{return} \code{(lambda (}var\code{) }body\code{)}
     %\Endif
     \Else\Comment{Build an application to give something w/ type $\tau$}
     \State $\text{primitives} \gets \{p | p: \tau' \in \mathcal{D}\cup\mathcal{E}$
     $\text{if }\tau\text{ can unify with yield}(\tau') \}$\Comment{Everything in scope w/ type $\tau$}
     \State $\text{variables}\gets\left\{p\;|\;p\in \text{primitives}\text{ and }p\text{ a variable} \right\}$
     \State Draw $e\sim \text{primitives}$, w.p. $\propto\begin{cases}
       Q_{\text{parent},e,\text{argumentIndex}}(x)&\text{ if }e\in \mathcal{D}\\
Q_{\text{parent},\text{var},\text{argumentIndex}}(x)/|\text{variables}|&\text{ if }e\in \mathcal{E}
       \end{cases}$
     \State Unify $\tau$ with yield$(\tau')$.\Comment{Ensure well-typed program}
     \State newParent$\gets\begin{cases}
     e &\text{ if }e\in \mathcal{D}\\
     \text{var}&\text{ if }e\in \mathcal{E}\end{cases}$
     \State $\left\{\alpha_k \right\}_{k = 1}^K\gets\text{args}(\tau')$ 
     %   \State unify$(\tau,\beta)$
     \For{$k=1$ {\bfseries to} $K$}\Comment{Recursively sample arguments}
     \State $a_k\sim\text{recognitionSample}'(Q,x,\text{newParent},k,\mathcal{D},\mathcal{E},\alpha_k)$
     \EndFor
     \State \textbf{return} \code{(}$e\;a_1\; a_2\; \cdots\; a_K$\code{)}
     \EndIf
   \end{algorithmic}
 \end{algorithm}

 \noindent \textbf{Symmetry breaking.}  Why does the combination of
 $\mathcal{L}^{\text{MAP}}$ and the bigram parameterization lead to
 symmetry breaking?  The reason is twofold: (1) the objective
 $\mathcal{L}^{\text{MAP}}$ prefers symmetry breaking recognition
 models; and (2) the bigram parameterization permits certain kinds of
 symmetry breaking.
 To sharpen these intuitions,
 we prove (Theorem~\ref{symmetricproof})
 that any global optimizer of $\mathcal{L}^{\text{MAP}}$
 breaks symmetries,
 and then give a concrete worked out example
 contrasting the behavior of $\mathcal{L}^{\text{MAP}}$ and $\mathcal{L}^{\text{posterior}}$.
 

 \pagebreak\begin{theorem}\label{symmetricproof}
   Let $\mu(\cdot )$ be a distribution over tasks and let $Q^*(\cdot |\cdot )$ be a task-conditional distribution over programs satisfying
   $$
   Q^* = \argmax_Q\;\expect_\mu\left[\max_{\substack{p\\p\text{ maxing }\probability[\cdot |x,\mathcal{D},\theta]}}\log  Q(p|x) \right]
   $$
   where $(\mathcal{D},\theta)$ is a generative model over programs. Pick a task $x$ where $\mu(x) > 0$.
Partition $\Lambda$ into expressions that are observationally equivalent under $x$:
   $$
\Lambda = \bigcup_i \mathcal{E}^x_i\text{ where for any }p_1\in \mathcal{E}^x_i\text{ and }p_2\in \mathcal{E}^x_j\text{: }\probability[x|p_1] = \probability[x|p_2] \iff i = j
$$
Then there exists an equivalence class $\mathcal{E}^x_i$
that gets all the probability mass of $Q^*$ -- e.g.,
$Q^*(p|x) = 0$ whenever $p\not\in \mathcal{E}^x_i$ --
and there exists a program in that equivalence class
which gets all of the probability mass assigned by $Q^*(\cdot |x)$ -- e.g.,  there is a $p\in \mathcal{E}^x_i$
such that $Q^*(p|x) = 1$ -- and that program maximizes $\probability[\cdot |x,\mathcal{D},\theta ]$.
 \end{theorem}
 \begin{proof}
   We proceed by defining the set of ``best programs'' -- programs maximizing the posterior $\probability[\cdot |x,\mathcal{D},\theta]$ -- and then showing that a best program satisfies $Q^*(p|x) = 1$.
   Define the set of best programs $\mathcal{B}_x$ for the task $x$ by
   $$
\mathcal{B}_x = \left\{p\;|\;\probability[p|x,\mathcal{D},\theta] = \max_{p'\in \Lambda}\probability[p'|x,\mathcal{D},\theta] \right\}
$$
For convenience define
$$
f(Q) = \expect_\mu\left[\max_{p\in \mathcal{B}_x}\log Q(p|x) \right]
$$
and observe that $Q^* = \argmax_Q f(Q)$.


Suppose by way of contradiction that there is a $q\not\in \mathcal{B}_x $
where $Q^*(q|x) = \epsilon > 0$.
Let $p^*= \argmax_{p\in \mathcal{B}_x}\log Q^*(p|x)$.
Define
$$
Q'(p|x) = \begin{cases}
  0&\text{ if }p = q\\
  Q^*(p|x) + \epsilon&\text{ if }p = p^*\\
  Q^*(p|x)&\text{ otherwise.}
\end{cases}
$$
Then
\begin{align*}
  f(Q') - f(Q^*) &= \mu(x) \left(\max_{\substack{p\in \mathcal{B}_x}}\log  Q'(p|x) - \max_{\substack{p\in \mathcal{B}_x}}\log  Q^*(p|x) \right) = \mu(x)\left(\log \left(Q^*(p^*|x) + \epsilon \right) - \log Q^*(p^*|x) \right) > 0
\end{align*}
which contradicts the assumption that $Q^*$ maximizes $f(\cdot )$.
Therefore
for any $p\not\in \mathcal{B}_x$ we have
$Q^*(p|x) = 0$.

Suppose by way of contradiction that there are two distinct programs, $q$ and $r$,
both members of $\mathcal{B}_x$,
where $Q^*(q|x) = \alpha > 0$
and $Q^*(r|x) = \beta > 0$.
Let $p^*= \argmax_{p\in \mathcal{B}_x}\log Q^*(p|x)$.
If $p^*\not\in \left\{q,r \right\}$ then define
$$
Q'(p|x) = \begin{cases}
  0&\text{ if }p\in \left\{q,r \right\}\\
  Q^*(p|x) + \alpha + \beta&\text{ if }p = p^*\\
  Q^*(p|x)&\text{ otherwise.}
\end{cases}
$$
Then
\begin{align*}
  f(Q') - f(Q^*) &= \mu(x) \left(\max_{\substack{p\in \mathcal{B}_x}}\log  Q'(p|x) - \max_{\substack{p\in \mathcal{B}_x}}\log  Q^*(p|x) \right)\\ &= \mu(x)\left(\log \left(Q^*(p^*|x) + \alpha + \beta \right) - \log Q^*(p^*|x) \right) > 0  \end{align*}
which contradicts the assumption that $Q^*$ maximizes $f(\cdot )$.
Otherwise assume $p^*\in \left\{q,r \right\}$.
Without loss of generality let $p^* = q$.
Define
$$
Q'(p|x) = \begin{cases}
  0&\text{ if }p = r\\
  Q^*(p|x) + \beta&\text{ if }p = p^*\\
  Q^*(p|x)&\text{ otherwise.}
\end{cases}
$$
Then
\begin{align*}
  f(Q') - f(Q^*) &= \mu(x) \left(\max_{\substack{p\in \mathcal{B}_x}}\log  Q'(p|x) - \max_{\substack{p\in \mathcal{B}_x}}\log  Q^*(p|x) \right) = \mu(x)\left(\log \left(Q^*(p^*|x) + \beta \right) - \log Q^*(p^*|x) \right) > 0  \end{align*}
 which contradicts the assumption that $Q^*$ maximizes $f(\cdot )$.
 Therefore $Q^*(p|x) > 0$ for at most one $p\in \mathcal{B}_x$.
 But we already know that $Q^*(p|x) = 0$ for any $p\not\in \mathcal{B}_x$,
 so it must be the case that $Q^*(\cdot |x)$
 places all of its probability mass on exactly one $p\in \mathcal{B}_x$.
 Call that program $p^*$.

 Because the equivalence classes $\left\{\mathcal{E}_i^x \right\}$
 form a partition of $\Lambda$
 we know that $p^*$ is a member of exactly one equivalence class; call it $\mathcal{E}_i^x$.
 Let $q\in \mathcal{E}_j^x\not=\mathcal{E}_i^x$.
 Then because the equivalence classes form a partition we know that
 $q\not= p^*$
 and so $Q^*(q|x) = 0$,
 which was our first goal:
 \emph{any}
 program not in $\mathcal{E}_i^x$
 gets no probability mass.

 Our second goal --- that there is a member of $\mathcal{E}_i^x$ which gets all the probability mass assigned by $Q^*(\cdot |x)$ --- is immediate from $Q^*(p^*|x) = 1$.

 Our final goal --- that $p^*$ maximizes $\probability[\cdot |x,\mathcal{D},\theta]$
 --- follows from the fact that $p^*\in \mathcal{B}_x$.
 \end{proof}

 Notice that Theorem~\ref{symmetricproof} makes no guarantees as to the
 cross-task systematicity of the symmetry breaking; for example, an
 optimal recognition model could associate addition to the right for
 one task and associate addition to the left on another task. \emph{Systematic} breaking of symmetries
 must arise only as a consequence as
 the network architecture (i.e., it is more parsimonious to break symmetries the same way for every task than it is to break them differently for each task).
 
As a concrete example of symmetry breaking, consider an agent tasked with writing programs built from addition and the constants zero and one.
A bigram parameterization of $Q$ allows
it to represent the fact that it should never add zero ($Q_{\code{+},\code{0},0} = Q_{\code{+},\code{0},1} = 0$)
%or never multiply by  1 ($Q_{\code{*},\code{1},0} = Q_{\code{*},\code{1},1} = 0$), or
or that addition should always associate to the right
($Q_{\code{+},\code{+},0} = 0$).
The $\mathcal{L}^{\text{MAP}}$ training objective encourages
learning these canonical forms.
Consider two recognition models, $Q_1$ and $Q_2$,
and two programs in frontier $\mathcal{F}_x$,
$p_1=\code{(+ (+ 1 1) 1)}$ and $p_2=\code{(+ 1 (+ 1 1))}$,
where
\begin{align*}
  Q_1(p_1|x) = \frac{\epsilon}{2}&\qquad Q_1(p_2|x) = \frac{\epsilon}{2}\\
  Q_2(p_1|x) = 0&\qquad  Q_2(p_2|x) = \epsilon
\end{align*}
i.e., $Q_2$ breaks a symmetry by forcing right associative addition,
but $Q_1$ does not, instead splitting its probability mass equally between $p_1$ and $p_2$.
Now because $\probability[p_1|\mathcal{D},\theta] = \probability[p_2|\mathcal{D},\theta]$
(Algorithm~\ref{sampleProgram}),
we have
\begin{align*}
  \mathcal{L}^{\text{posterior}}_{\text{real}}(Q_1)& = \frac{\probability[p_1|\mathcal{D},\theta]\log \frac{\epsilon}{2} + \probability[p_2|\mathcal{D},\theta]\log \frac{\epsilon}{2}}{\probability[p_1|\mathcal{D},\theta] + \probability[p_2|\mathcal{D},\theta]} = \log \frac{\epsilon}{2}\\
  \mathcal{L}^{\text{posterior}}_{\text{real}}(Q_2)& = \frac{\probability[p_1|\mathcal{D},\theta]\log 0 + \probability[p_2|\mathcal{D},\theta]\log \epsilon}{\probability[p_1|\mathcal{D},\theta] + \probability[p_2|\mathcal{D},\theta]} = +\infty\\
  \mathcal{L}^{\text{MAP}}_{\text{real}}(Q_1)& = \log Q_1(p_1)           = \log Q_1(p_2)   = \log \frac{\epsilon}{2}\\
  \mathcal{L}^{\text{MAP}}_{\text{real}}(Q_2)& = \log Q_2(p_2) \phantom{ = \log Q_1(p_2) } = \log \epsilon\\
\end{align*}
So $\mathcal{L}^{\text{MAP}}$ prefers $Q_2$ (the symmetry breaking
recognition model), while $\mathcal{L}^{\text{posterior}}$ reverses
this preference.

How would this example work out if we did not have a bigram
parameterization of $Q$?  With a unigram parameterization, $Q_2$ would
be impossible to express, because it depends on local context within
the syntax tree of a program. So even though the objective function would prefer symmetry breaking,
a simple unigram model lacks the expressive power to
encode it.

To be clear,
our recognition model does not learn to break \emph{every} possible symmetry in every possible DSL.
But in practice we found that a bigrams combined with $\mathcal{L}^{\text{MAP}}$
works well,
and we use with this combination throughout the paper.


\subsection{Full set of LOGO  tasks}

\begin{figure}
  \includegraphics[width = \textwidth]{figures/fullLogo.png}
  \caption{Full set of LOGO graphics tasks that we apply our system to}
\end{figure}

\subsection{Full set of tower tasks}
\begin{figure}
  \includegraphics[width = \textwidth]{figures/fullTower.png}
  \caption{Full set of tower building tasks that we apply our system to}
\end{figure}

\subsection{Learning from Scratch: Tasks and DSL}\label{appendixMcCarthy}

We gave our system the following primitives: \code{if}, \code{=},
\code{>}, \code{+}, \code{-}, \code{0}, \code{1}, \code{cons},
\code{car}, \code{cdr}, \code{nil}, and \code{is-nil}, all of which
are present in some form in McCarthy's 1959
Lisp~\cite{mccarthy1960recursive}.\footnote{McCarthy's first version
  of Lisp used \code{cond} instead of \code{if}. Because we are using
  a typed language, we instead used \code{if}, because Lisp-style
  \code{cond} is unwieldy to express as a function in typed
  languages.}  We furthermore allowed functions to call themselves,
which we modeled using the Y combinator.  We did not use the
recognition model for this experiment: a bottom-up pattern recognizer
is of little use for acquiring this abstract knowledge from less than
two dozen problems.

Figure~\ref{learningFromScratch} shows the full set of tasks and the learned DSL.
\begin{figure}  \newcommand{\helpSize}{0.25cm}
  \begin{tabular}{ll}\toprule
    \normalsize \rotatebox[origin=c]{90}{\pop{Programs} \& Tasks}&%&\normalsize \popp{DSL}\\\midrule

    \begin{tabular}{ll}      
      \begin{tabular}{l}
        \code{[1\, 9]}$\to $\code{2}\\
        \code{[5\, 3\, 8]$\to$3}\\
        \blueCode{$f(\ell) = $($f_5$ $\ell$)}
      \end{tabular}&
      \begin{tabular}{l}
        \code{[[2 1]\, []]}$\to $\code{[1 0]}\\
        \code{[[] [] [9 8 9 9]]$\to$[0 0 4]}\\
        \blueCode{$f(\ell) = $($f_2$ $f_4$ $\ell$)}
      \end{tabular}\\\\

      \begin{tabular}{l}
        \code{[0 1 1 0 0]$\to$[1 1]}\\
        \code{[9 0 8]$\to$[9\, 8]}\\
        \blueCode{$f(\ell) = $($f_3$ (eq? 0) $\ell$)}        
      \end{tabular}&
      \begin{tabular}{l}
        \code{[1 -1 0 2]$\to$[1 2]}\\
        \code{[9 -5 5 0 8]$\to$[9 5 8]}\\
        \blueCode{$f(\ell) = $($f_3$ (gt? 1) $\ell$)}        
      \end{tabular}\\\\

      \begin{tabular}{l}
        \code{[2\, 1\, 4]$\to$[2\, 1\, 4\, 0]}\\
        \code{[9\, 8]$\to$[9\, 8\, 0]}\\
        \blueCode{$f(\ell) = $($f_0$ cons $\ell$ (cons 0 nil))}
      \end{tabular}&


      \begin{tabular}{l}
        \code{[2\, 1\, 4]$\to$[2\, 1]}\\
        \code{[9\, 8]$\to$[9]}\\
        \blueCode{$f(\ell) = $($f_6$ cdr ($\lambda$ (z) (empty? (cdr z))) $\ell$)}
      \end{tabular}\\\\

      \begin{tabular}{l}
        \code{[2\, 5\, 6\, 0\, 6]$\to$19}\\
        \code{[9\, 2\, 7\, 6\, 3]$\to$27}\\
        \blueCode{$f(\ell) = $($f_0$ + $\ell$ 0)}
      \end{tabular}&

      \begin{tabular}{l}
        \code{[4\, 2\, 6\, 4]$\to$[8\, 4\, 12\, 8]}\\
        \code{[2\, 3\, 0\, 7]$\to$[4\, 6\, 0\, 14]}\\
        \blueCode{$f(\ell) = $($f_2$ ($\lambda$ (x) (+ x x)) $\ell$)}
      \end{tabular}\\\\

      \begin{tabular}{l}
        \code{[4\, 2\, 6\, 4]$\to$[-4\, -2\, -6\, -4]}\\
        \code{[2\, 3\, 0\, 7]$\to$[-2\, -3\, -0\, -7]}\\
        \blueCode{$f(\ell) = $($f_2$ (- 0) $\ell$)}
      \end{tabular}&
      \begin{tabular}{l}
        \code{[4\, 2\, 6\, 4]$\to$[5\, 3\, 7\, 5]}\\
        \code{[2\, 3\, 0\, 7]$\to$[3\, 4\, 1\, 8]}\\
        \blueCode{$f(\ell) = $($f_2$ (+ 1) $\ell$)}
      \end{tabular}\\\\

      \begin{tabular}{l}
        \code{[1\, 5\, 2\, 9]}$\to$\code{[1\, 2]}\\
        \code{[3\, 8\, 1\, 3\, 1\, 2]}$\to$\code{[3\, 1\, 1]}\\
        \blueCode{$f(\ell) = $($f_6$ ($\lambda$ (l) (cdr (cdr l))) empty? $\ell$}
      \end{tabular}&
      
      \begin{tabular}{l}
        \code{3$\to $[0\, 1\, 2]}\\
        \code{2$\to $[0\, 1]}\\
        \blueCode{$f(n) = $($f_9$ $n$)}
      \end{tabular}\\\\

      
      \begin{tabular}{l}
        \code{3$\to $[0\, 1\, 2\, 3]}\\
        \code{2$\to $[0\, 1\, 2]}\\
        \blueCode{$f(n) = $($f_9$ (+ 1 $n$))}
      \end{tabular}&
      \begin{tabular}{l}
        \code{[9 2]$\to $[9 9 2 2]}\\
        \code{[1 2 3 4]$\to $[1 1 2 2 3 3 4 4]}\\
        \blueCode{$f(l) = $($f_0$ ($\lambda$ (a x) (cons x (cons x a))) l nil)}
      \end{tabular}\\\\


      \begin{tabular}{l}
        \code{0, [9 2 3]$\to $9}\\
        \code{3, [0 2 8 4 5 6]$\to $4}\\
        \blueCode{$f($n$,$l$) = $($f_{10}$ l n)}
      \end{tabular}&
      \begin{tabular}{l}
        \code{1, [9 2 3]$\to $9}\\
        \code{4, [0 2 8 1 5 6]$\to $1}\\
        \blueCode{$f($n$,$l$) = $($f_{10}$ l (+ 1 n))}
      \end{tabular}\\\\
      \begin{tabular}{l}
        \code{3$\to $[-3 -2 -1]}\\
        \code{4$\to $[-4 -3 -2 -1]}\\
        \blueCode{$f($n$) = $($f_{8}$ 0 n)}
      \end{tabular}&
      \begin{tabular}{l}
        \code{2$\to $[2 1]}\\
        \code{4$\to $[4 3 2 1]}\\
        \blueCode{$f($n$) = $($f_{8}$ 0 (- 0 n))}
      \end{tabular}      
    \end{tabular}
    \\\midrule 
    \rotatebox[origin=c]{90}{\normalsize \popp{DSL}}&
    \begin{tabular}{ll}
      \begin{tabular}{l}
        \popp{\code{$f_0($f$,$l$,$x$)\,=\,$(if (empty? l) x}}\\
        \phantom{\code{$f_0($f$,$l$,$x$)\,=\,$(if }}}\popp{\code{(f (car l) ($f_0$ (cdr l))))}}\\
          \hspace{\helpSize}($f_0$: \emph{fold})
      \end{tabular}&
      \begin{tabular}{l}
        \popp{\code{$f_1($p$,$f$,$n$,$x$)\,=\,$(if (p x) nil}}\\
        \phantom{\code{$f_1($f$,$l$,$x$)\,=\,$(if }}}\popp{\code{(cons (f x) ($f_1$ (n x))))}}\\
          \hspace{\helpSize}($f_1$: \emph{unfold})
      \end{tabular}\\
      \begin{tabular}{l}
        \greenCode{$f_2($f$,$l$)\,=\,$($f_0$ nil l ($\lambda$ (x a) (cons (f x) a)))}\\
        \hspace{\helpSize}($f_2$: \emph{map})
      \end{tabular}&
      \begin{tabular}{l}
                  \greenCode{$f_3($f$,$l$)\,=\,$($f_0$ nil l ($\lambda$ (x a) (if (f x) a (cons x a))))}\\
          \hspace{\helpSize}($f_3$: \emph{filter})
      \end{tabular}\\
      \begin{tabular}{l}
        \greenCode{$f_4($f$,$p$,$n$)\,=\,$($f_1$ p f (+ 1) n)}\\
        \hspace{\helpSize}($f_4$: \emph{generalization of range})
      \end{tabular}&
      \begin{tabular}{l}
        \greenCode{$f_5($l$)\,=\,$($f_0$ ($\lambda$ (a x) (+ 1 a)) l 0)}\\
        \hspace{\helpSize}($f_5$: \emph{length})
      \end{tabular}\\
      \begin{tabular}{l}
        \greenCode{$f_6($n$,$p$,$l$)\,=\,$($f_1$ p n car l)}\\
        \hspace{\helpSize}($f_6$: \emph{specialization of unfold})
      \end{tabular}&
      \begin{tabular}{l}
        \greenCode{$f_7($p$)\,=\,$($f_4$ ($\lambda$ (x) x) p 0)}\\
        \hspace{\helpSize}($f_7$: \emph{another generalization of range})
      \end{tabular}\\
      \begin{tabular}{l}
        \greenCode{$f_8($n$,$m$)\,=\,$($f_4$ ($\lambda$ (x) (- n x)) (eq? 0) m)}\\
        \hspace{\helpSize}($f_7$: \emph{count downwards})
      \end{tabular}&
      \begin{tabular}{l}
        \greenCode{$f_9($n$)\,=\,$($f_7$ (eq? n))}\\
        \hspace{\helpSize}($f_9$: \emph{range})
      \end{tabular}\\
      \begin{tabular}{l}
        \greenCode{$f_{10}($l$,$n$)\,=\,$(car ($f_0$ ($\lambda$ (a x) (cdr a)) ($f_9$ n) l))}\\
        \hspace{\helpSize}($f_{10}$: \emph{index})
      \end{tabular}&
      
      
      
    \end{tabular}
%%     \begin{tabular}{l}
%%         \popp{\code{$f_1($i$,$l$)\,=\,$(if (= i 0) (car l)}}\\
%%       \phantom{\code{$f_1($f$,$l$,$x$)\,=\,$(if}}}\popp{\code{($f_1$ (- i 1) (cdr l))))}}\\
%%         \hspace{\helpSize}($f_1$: \emph{index})\\

%%         \greenCode{$f_4(\ell)$\,=\,(if (empty? $\ell$) 0 (+ 1 ($f_4$ (cdr $\ell$)))))}\\
%%         \hspace{\helpSize}($f_4$: \emph{length})\\
%%         \greenCode{$f_5(\code{f},\code{m},\code{n})$\,=\,($f_0$ (= m) f (+ 1) n)}\\
%%         \hspace{\helpSize}($f_5$: \emph{generalization of range})

%% %        -0.222777	int -> int -> list(int)	#(lambda (lambda (fix1 $0 (lambda (lambda (#(lambda (lambda (lambda (if $0 empty (cons $1 $2))))) (map (lambda (#(+ 1) $0)) ($1 (#(lambda (- $0 1)) $0))) 0 (eq? $0 $3)))))))

%%       \end{tabular}
    %% : (lambda (#(#(lambda (lambda (lambda (lambda (fix1 $0 (lambda (lambda (if (empty? $0) $3 ($4 ($5 $0) ($1 (cdr $0))))))))))) (lambda (car $0))) (lambda (lambda (+ $0 $1))) 0 $0))
    %% (lambda (lambda (lambda (lambda (fix1 $0 (lambda (lambda (#(lambda (lambda (lambda (if $0 empty (cons $1 $2))))) ($1 ($3 $0)) ($4 $0) ($5 $0)))))))))
    %% (lambda (lambda (fix1 $0 (lambda (lambda (#(lambda (lambda (lambda (if $0 empty (cons $1 $2))))) (#(lambda (lambda (fix1 $0 (lambda (lambda (if (empty? $0) empty (cons ($3 (car $0)) ($1 (cdr $0))))))))) (lambda (#(+ 1) $0)) ($1 (#(lambda (- $0 1)) $0))) 0 (eq? $0 $3)))))))
    \\\bottomrule 
    \end{tabular}
  \caption{Bootstrapping a standard library of functional programming routines, starting from recursion along with primitive operations found in 1959 Lisp. Complete set of tasks and learned DSL shown. Learned DSL components are numbered in the order that they are learned, \emph{i.e.}, the agent first learns fold, then unfold, then uses fold to define map, etc.}\label{learningFromScratch}
  \end{figure}


%% \begin{figure}
%%   \begin{tikzpicture}
%%     \node at (0,0) (d){DSL};
%%     \node at ([yshift = -2cm]d) (t){$\text{Task}$};

%%     \node at ([xshift = 2cm]t) (nn){
%%       \begin{tikzpicture}[x=2.5cm,y=1.25cm,transform canvas={scale=0.2,shift={+(-1,2.5)}}]
%%         \tikzstyle{neuron}=[circle,fill=blue!50,minimum size=20pt]
%%         \fill[fill=white] (-0.25,-0.5) rectangle (2.25,-4.5);
%%         \node[rectangle] at (1,1) {};
%%         \foreach \name / \y in {1,...,4}
%%             \node[neuron] (I-\name) at (0,-\y) {};
%%         \foreach \name / \y in {1,...,3}
%%             \node[neuron] (H-\name) at (1,-\y-0.5) {};
%%         \foreach \name / \y in {1,...,4}
%%             \node[neuron] (O-\name) at (2,-\y) {};
%%         \foreach \source in {1,...,4}
%%             \foreach \dest in {1,...,3}
%%                 \draw [-latex] (I-\source) -- (H-\dest);
%%         \foreach \source in {1,...,3}
%%             \foreach \dest in {1,...,4}
%%                 \draw [-latex] (H-\source) -- (O-\dest);
%%       \end{tikzpicture}
%%     };
%%     \node[align = center, text width = 1cm] at ([yshift = 0.6cm]nn.north) {\baselineskip=0pt \small Recognition model\par};
%%     \draw [->] (t.east) -- ([xshift = -0.5cm]nn.west);

%%     \node[draw,rounded corners, inner sep = 10] at ([xshift = 4.2cm,yshift = -1cm]) (s){Search};
%%     \node at ([xshift=-7pt,yshift=5pt]s.north west) {$\mathcal{D}$};

%%     \draw [->] ([xshift = 0.5cm]nn.east) -- ([yshift = -0.25cm]s.west);
%%     \draw [->,rounded corners] (d.east) -- ([yshift = 2cm]nn.center) -- ([yshift = 0.25cm]s.west);

%%     \node[align=left] at (7,-1) (f) {Frontier\\{\small (set of programs)}};
%%     \draw [->  ] (s.east) -- (f.west);

%%     \draw [->  ,rounded corners] (t.south) -- ([yshift = -0.5cm]t.south) -- ([yshift = -0.5cm] s.south |- t.south) -- (s.south);
%%     \node at ([xshift = 0.5cm,yshift = -0.75cm]s.south) {Spec};

%%     \node at (4,-3.5) {\textbf{\textsc{Wake: Problem Solving}}};
    
    
%%   \end{tikzpicture}

%%   \vspace{1cm}
  
  

%%   \begin{tikzpicture}
%%     \node at (0,0) (f1){Frontier$_1$};
%%     \node at ([yshift = -1cm]f1.south) (f2){Frontier$_2$};
%%     \node at ([yshift = -1cm]f2.south) (f3){Frontier$_3$};

%%     \node at ([xshift = 2cm]f1.east) (p1){program$_1$};
%%     \node at ([xshift = 2cm]f2.east) (p2){program$_2$};
%%     \node at ([xshift = 2cm]f3.east) (p3){program$_3$};


%%     \draw [->,squiggle ] (f1.east) -- node[above]{\small sample} (p1.west);
%%     \draw [->,squiggle ] (f2.east) -- node[above]{\small sample} (p2.west);
%%     \draw [->,squiggle ] (f3.east) -- node[above]{\small sample} (p3.west);
    
%%     \node at ([xshift = 1.5cm]p1.east) (t1){task$_1$};
%%     \node at ([xshift = 1.5cm]p2.east) (t2){task$_2$};
%%     \node at ([xshift = 1.5cm]p3.east) (t3){task$_3$};

%%     \node at ([yshift = -0.5cm]p3.south) {\textsc{\textbf{Sleep-R: Experience Replay}}};
%%   \end{tikzpicture}

%%   \begin{tikzpicture}
%%     \node[align=center] at (0,0) (d){DSL\\$(\mathcal{D},\theta)$};
%%     \node at ([xshift = 3cm]d.east) (p2){program};
%%     \node at ([yshift = 1.5cm]p2) (p1){program};
%%     \node at ([yshift = -1.5cm]p2) (p3){program};


%%     \draw[squiggle,-> ] (d.east) -- node[above]{\small sample} (p2.west);
%%     \draw[squiggle,-> ] (d.east) -- (p1.west);
%%     \draw[squiggle,-> ] (d.east) -- (p3.west);

%%     \node at ([xshift = 2cm]p1.east) (t1){task};
%%     \node at ([xshift = 2cm]p2.east) (t2){task};
%%     \node at ([xshift = 2cm]p3.east) (t3){task};
%%     \draw [-> ] (p1.east) -- node[above]{\small execute} (t1.west);
%%     \draw [-> ] (p3.east) -- node[above]{\small execute} (t3.west);
%%     \draw [-> ] (p2.east) -- node[above]{\small execute} (t2.west);

%%     \node at ([yshift = -0.5cm]p3.south) {\textsc{\textbf{Sleep-R: Dreaming}}};
%%   \end{tikzpicture}
  
%%   \vspace{2cm}
  
%%   \begin{tikzpicture}
%%     \node at (0,0) (f1){Frontier$_1$};
%%     \node at ([yshift = -1cm]f1.south) (f2){Frontier$_2$};
%%     \node at ([yshift = -0.7cm]f2.south) (ff){\textbf{$\vdots$}};
%%     \node at ([yshift = -1.2cm]f2.south) (ff){\textbf{$\vdots$}};
%%     \node at ([yshift = -1cm]ff.south) (f3){Frontier$_N$};

%%     \node(c)[rectangle, rounded corners, draw, minimum width = 3cm, minimum height = 6cm, anchor = north west] at (2,1) {};
%%     \node[anchor=north] at (c.north) {Compression};

%%     \draw [-> ] (f1.east) -- (c.west|-f1.east);
%%     \draw [-> ] (f2.east) -- (c.west|-f2.east);
%%     \draw [-> ] (f3.east) -- (c.west|-f3.east);

%%     \node[right](d) at ([xshift = 1.2cm,yshift = 0.7cm]c.east) {DSL $\mathcal{D}$};
%%     \node[right](t) at ([xshift = 1.2cm,yshift = -0.7cm]c.east) {Weights $\theta$};
%%     \draw [-> ] (c.east) -- (d.west);
%%     \draw [-> ] (c.east) -- (t.west);

%%     \node at (c.center) {
%% \begin{tikzpicture}[scale=0.7]
%%     %% \node[rotate=30] at (-2,0) {\begin{tabular}{c}
%%     %%     \footnotesize Program:\\
%%     %%     \code{($\lambda$ (x) (+ (- x) 1))}
%%     %% \end{tabular}};
%%     %\node at (,0.5) {\code{cons}};
%% %    \node [rotate=90] at (-2.3,-0.5) {\small program};
    
%%           \node(l1) at (0,0) {};
%%   \node[color=pop3](p1) at (-1,-1) {\code{+}};
%%   \node[color=pop3](n1) at (0.7,-0.9) {\code{1}};
%%   \node(x1) at (0,-1) {\code{1}};
%%   \draw[color=pop3] (l1.south) -- (p1.north);
%%   \draw[color=pop3] (l1.south) -- (n1.north);
%%   \draw[color=pop3] (-0.5,-0.45) -- (x1.north);

%%   \node(t) at (-0.5,0.5) {};
%%   \draw (l1.south) -- (t.south);
%%   \node(c) at (-1.5,-0.2) {\code{cons}};
%%   \draw (t.south) -- (c.north);
  
%% %    \draw  (l1.south) -- (-0.5,0.5);

%%   %% \node(c) at (-0.5,-1.5) {\code{-}};
%%   %% \node(z) at (0.5,-1.5) {\code{x}};

%%   %% \draw (0,-1) -- (c.north);
%%   %% \draw (0,-1) -- (z.north);
  
%%   \begin{scope}[shift={(-1,-2.5)}]
%%       \node(l1) at (0,0) {};
%%   \node[color=pop3](p1) at (-1,-1) {\code{+}};
%%   \node[color=pop3](n1) at (0.7,-0.9) {\code{1}};
%%   %\node(x1) at (0,-1) {};
%%   \draw[color=pop3] (l1.south) -- (p1.north);
%%   \draw[color=pop3] (l1.south) -- (n1.north);
%%   \draw[color=pop3] (-0.5,-0.45) -- (0,-1);


%%   \node(c) at (-0.5,-1.5) {\code{car}};
%%   \node(z) at (0.5,-1.5) {\code{z}};

%%   \draw (0,-1) -- (c.north);
%%   \draw (0,-1) -- (z.north);

%% %  \node [rotate=90] at (-2.3,-0.7) {\small program};
  
%%   \end{scope}

%% \begin{scope}[shift={(0,-5)}]
%%   \node[pop3](p1) at (-1,-1) {\code{+}};
%%   \node[pop3](n1) at (0.8,-0.7) {\code{1}};
%%   \node[pop3](a) at (0,-1) {\code{ }};
%%   %\node(x1) at (0,-1) {};
%%   \draw[pop3] (0,0) -- (p1.north);
%%   \draw[pop3] (0,0) -- (n1.north);
%%   \draw[pop3] (-0.55,-0.4) -- (a.north);
%% %  \node [rotate=90] at (-2.3,-0.7) {\small fragment};

%%   \end{scope}

%% \end{tikzpicture}
%%     };

%%     \node at ([yshift=-2.5cm,xshift = 4cm]c.south) {\textsc{\textbf{Sleep-G: Memory Consolidation}}};

%%     \end{tikzpicture}
%% \end{figure}

%% \begin{table*}%[t!]
%%   \makebox[\textwidth][c]{
%%     \scriptsize
%%   \tabcolsep=4pt
%%   \renewcommand\code\texttt
%%   \renewcommand\codechar[1]{\texttt{"#1"}}
%%   \newcommand{\helpSize}{0.25cm}
%%   \begin{tabular}{cccc}
%%     \toprule
%%     &{\normalsize Symbolic regression}&{\normalsize Laws of motion}&\\\midrule
%%     \rotatebox[origin=c]{90}{\normalsize \pop{Programs} \& Tasks}&{\tabcolsep=7pt
%%     \begin{tabular}{cc}
%%       \includegraphics[width = 3em]{figures/functions/4.png}&
%%       \includegraphics[width = 3em]{figures/functions/146}\\
%%       \pop{\code{$f($x$) = $($f_1$ x)}}&    \pop{\code{$f($x$) = $($f_6$ x)}}\\
%%       ~\\
%%       \includegraphics[width = 3em]{figures/functions/112.png}&
%%         \includegraphics[width = 3em]{figures/functions/92.png}
%%       \\
%%       \pop{\code{$f($x$) = $($f_4$ x)}}&    \pop{\code{$f($x$) = $($f_3$ x)}}\\

%%     \end{tabular}
%%     }
%%     &
%%     \begin{tabular}{cc}
%%       \includegraphics[width = 15em]{figures/massOnSpring.png} &
%%             \includegraphics[width = 15em]{figures/planets.png}
%%       \\
%%       \begin{tabular}{l}
%%               \blueCode{$f($o,$\Delta) = $($f_3$ o $\Delta$}\\
%%       \hspace{1cm}\blueCode{(- (* $k$ (pos o))}\\
%%       \hspace{1cm}\phantom{\code{(- }}\blueCode{(* $-9.8$ $\hat{x}$)))}\\
%%       \end{tabular}&
%%       \begin{tabular}{l}
%%               \blueCode{$f($a,b,$\Delta) = $($f_3$ a $\Delta$}\\
%%       \hspace{1cm}\blueCode{(/ (* $G$ (mass a) (mass b))}\\
%%       \hspace{1cm}\phantom{\code{(- }}\blueCode{(square (- (pos a) (pos b)))))}\\
%%         \end{tabular}
%%       \end{tabular}
%%     &

%%     ~\\
%%     \midrule
%%     \rotatebox[origin=c]{90}{\normalsize \popp{DSL}}&
%%       \begin{tabular}{l}
%%     \popp{$f_0($\code{x}$)\,=\,$\code{(+ x real)}}\\
%%     \popp{$f_1($\code{x}$)\,=\,$\code{($f_0$ (* real x))} }\\
%%     \popp{$f_2($\code{x}$)\,=\,$\code{($f_1$ (* x (}$f_0$\code{ x)))}}\\
%%     \popp{$f_3($\code{x}$)\,=\,$\code{($f_0$ (* x (}$f_2$\code{ x)))}}\\
%%     \popp{$f_4($\code{x}$)\,=\,$\code{($f_0$ (* x (}$f_3$\code{ x)))}}\\
%%     \hspace{\helpSize}\emph{($f_4$: 4th order polynomial)}\\
%%     \popp{$f_5($\code{x}$)\,=\,$\code{(/ real x)}}\\
%%     \popp{$f_6($\code{x}$)\,=\,$\code{($f_5$ ($f_0$ x))}}\\
%%     \hspace{\helpSize}\emph{($f_6$: rational function)}\\

%%   \end{tabular}
%%       &
%%       \begin{tabular}{l}
%%         \greenCode{$f_0($\code{o},$\Delta)\, = \,$(set-pos o (+ (pos o) (* $\Delta$ (vel o))))}\\
%%         \hspace{\helpSize}\emph{($f_0$: integrates position)}\\
%%         \greenCode{$f_1($\code{o},\code{a},$\Delta)\, = \,$(set-vel o (+ (vel o) (* $\Delta$ a)))}\\
%%         \hspace{\helpSize}\emph{($f_1$: integrates velocity)}\\
%%         \greenCode{$f_2($\code{o},\code{F}$)\, = \,$(/ F (mass o))}\\
%%         \hspace{\helpSize}\emph{($f_2$: Newton's second law)}\\
%%         \greenCode{$f_3($\code{o},$\Delta$,\code{F}$)\, = \,$($f_0$ o $\Delta$ ($f_1$ o ($f_2$ o F) $\Delta$))}\\
%%         \hspace{\helpSize}\emph{($f_3$: applies Newton's second law and integrates)}
        
%%         \end{tabular}


%% &


%%   \\\bottomrule\\
%% \end{tabular}}\vspace{-0.5cm} 
%% \caption{Top: Tasks from three domains we apply our algorithm to, each followed by the programs \system discovers for them. Bottom: Several examples from learned DSL. Notice that learned DSL primitives can call each other, and that \system rediscovers higher-order functions like \code{filter} ($f_1$ under List Functions)}\label{initialExampleDSL}%\vspace{-0.5cm}
%% \end{table*}

\subsection{Learned DSLs}
Here we present representative DSLs learned by our model. DSL primitives
discovered by the algorithm are prefixed with \lstinline!#!.
Variables are prefixed with \lstinline!$!, and we adopt De Bruijn indices to
model bound variables~\cite{pierce}.


\lstset{
  basicstyle=\footnotesize,
  escapechar=@,
  breaklines=true,
  breakatwhitespace=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}


\subsubsection{List processing}
\begin{lstlisting}
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range ($0 (+ 1))))))
#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0))))))
#(+ 1 (+ 1 1))
#(@$\lambda$@ (@$\lambda$@ (fold $1 (cons $0 empty) (@$\lambda$@ (@$\lambda$@ (cons $1 $0))))))
#(+ 1 #(+ 1 (+ 1 1)))
#(@$\lambda$@ (map (@$\lambda$@ (if ($1 $0) (+ $0 1) 0))))
#(@$\lambda$@ (cdr (cdr $0)))
#(@$\lambda$@ (@$\lambda$@ (fold (#(@$\lambda$@ (cdr (cdr $0))) (#(@$\lambda$@ (cdr (cdr $0))) $1)) $0 (@$\lambda$@ (@$\lambda$@ (cdr (#(@$\lambda$@ (@$\lambda$@ (fold $1 (cons $0 empty) (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $0 (car $0))))))))
#(@$\lambda$@ (car (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $0 (@$\lambda$@ (empty? (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $1 (@$\lambda$@ (gt? $0 $1))))))))
#(@$\lambda$@ (@$\lambda$@ (length (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $1 (@$\lambda$@ (eq? $0 $1))))))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $1 (@$\lambda$@ (is-prime (+ $1 (mod $0 #(+ 1 #(+ 1 (+ 1 1))))))))))
#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0))))))
#(@$\lambda$@ (map (@$\lambda$@ (mod $0 $1))))
#(@$\lambda$@ (map (@$\lambda$@ (eq? (mod $0 $1) 0))))
#(@$\lambda$@ (gt? (mod $0 #(+ 1 (+ 1 1))) 0))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (car (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $0 (@$\lambda$@ (empty? (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $1 (@$\lambda$@ (gt? $0 $1)))))))) (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $1 (@$\lambda$@ (gt? $1 (length (#(@$\lambda$@ (@$\lambda$@ (fold $1 empty (@$\lambda$@ (@$\lambda$@ (if ($2 $1) (cons $1 $0) $0)))))) $2 (@$\lambda$@ (gt? $1 $0))))))))))
\end{lstlisting}

\subsubsection{Text editing}
\begin{lstlisting}
#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0))))))
#(@$\lambda$@ (@$\lambda$@ (cons '.' (cons $0 $1))))
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0))))
#(+ 1)
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0))))))))
#(@$\lambda$@ (@$\lambda$@ (cons (car $0) (#(@$\lambda$@ (@$\lambda$@ (cons '.' (cons $0 $1)))) (cons '.' empty) (car $1)))))
#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (if (char-eq? $2 $0) $1 $0)))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (cons LPAREN $0) (cons RPAREN empty)))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $0 (length (cdr (cdr $1))))))
#(@$\lambda$@ (@$\lambda$@ (cdr (fold (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) $1 $0) $1 (@$\lambda$@ (@$\lambda$@ (cdr $0)))))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $0 (length (cdr (cdr $1)))))) (cdr (cdr $0))))
#(#(+ 1) 1)
#(@$\lambda$@ (@$\lambda$@ (cons (car $1) (cons $0 empty))))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) (#(@$\lambda$@ (@$\lambda$@ (cdr (fold (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) $1 $0) $1 (@$\lambda$@ (@$\lambda$@ (cdr $0))))))) (#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) (cons LPAREN $0) (cons RPAREN empty))) $1) $0) $0)))
#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $1 (cons $0 $2)))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $0 STRING))
#(#(+ 1) #(#(+ 1) 1))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) STRING $0) SPACE))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $1 (cons $0 $2))))) (cons $0 $1))))
#(#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) STRING)
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (cons (car $0) (#(@$\lambda$@ (@$\lambda$@ (cons '.' (cons $0 $1)))) (cons '.' empty) (car $1))))) (#(@$\lambda$@ (@$\lambda$@ (cdr (fold (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) $1 $0) $1 (@$\lambda$@ (@$\lambda$@ (cdr $0))))))) $1 $0) $1)))
#(#(+ 1) #(#(+ 1) #(#(+ 1) 1)))
#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $1 (#(@$\lambda$@ (@$\lambda$@ (cdr (fold (#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $1 (fold (cdr $1) 0 (@$\lambda$@ (@$\lambda$@ (#(+ 1) (if (char-eq? $2 $1) 0 $0)))))))) $1 $0) $1 (@$\lambda$@ (@$\lambda$@ (cdr $0))))))) (#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (fold $1 $0 (@$\lambda$@ (@$\lambda$@ (cons $1 $0)))))) $0 STRING)) (#(@$\lambda$@ (@$\lambda$@ (map (@$\lambda$@ (index $0 $2)) (range $0)))) $2 #(#(+ 1) #(#(+ 1) #(#(+ 1) 1))))) $0)))))
\end{lstlisting}

\subsubsection{Graphics}
\begin{lstlisting}
  #(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0)))))))
#(logo_DIVA logo_UA 4)
#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY)
#(logo_PT (@$\lambda$@ (logo_FWRT logo_UL logo_ZA $0)))
#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $1 (@$\lambda$@ (@$\lambda$@ (logo_FWRT (logo_MULL $2 $1) $4 $0)))))))
#(@$\lambda$@ (logo_forLoop 7 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) 7 logo_epsA $2 $0)))))
#(@$\lambda$@ (#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY) (logo_SUBA logo_UA logo_epsA) (logo_MULL logo_epsL $0)))
#(@$\lambda$@ (logo_forLoop logo_IFTY (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 logo_epsA (logo_FWRT logo_epsL logo_epsA $0))))))
#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) 4 #(logo_DIVA logo_UA 4))
#(logo_DIVA logo_UA)
#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $1 (@$\lambda$@ (@$\lambda$@ (logo_GETSET $2 (logo_FWRT logo_ZL $4 $0))))))))
#(#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY) (logo_DIVA logo_epsA 2) logo_epsL)
#(#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY) logo_epsA logo_epsL)
#(logo_forLoop 3 (@$\lambda$@ (@$\lambda$@ (#(#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY) (logo_DIVA logo_epsA 2) logo_epsL) (logo_FWRT logo_ZL #(logo_DIVA logo_UA 4) $0)))))
#(@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $1 (@$\lambda$@ (@$\lambda$@ (logo_GETSET $2 (logo_FWRT logo_ZL $4 $0)))))))) (#(logo_DIVA logo_UA) $0) $0))
#(@$\lambda$@ (#(logo_PT (@$\lambda$@ (logo_FWRT logo_UL logo_ZA $0))) (#(@$\lambda$@ (logo_forLoop logo_IFTY (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 logo_epsA (logo_FWRT logo_epsL logo_epsA $0)))))) logo_epsL $0)))
#(@$\lambda$@ (#(logo_PT (@$\lambda$@ (logo_FWRT logo_UL logo_ZA $0))) (logo_FWRT logo_UL #(logo_DIVA logo_UA 4) $0)))
#(@$\lambda$@ (#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) logo_IFTY) logo_epsA (logo_MULL logo_epsL $0)))
#(logo_FWRT logo_UL logo_UA)
#(@$\lambda$@ (#(@$\lambda$@ (logo_forLoop 7 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (logo_forLoop $2 (@$\lambda$@ (@$\lambda$@ (logo_FWRT $2 $3 $0))))))) 7 logo_epsA $2 $0))))) (logo_MULL logo_epsL $0)))
\end{lstlisting}

\subsubsection{Towers}
\begin{lstlisting}
  #(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0))))))
#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0))))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left 4 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0))))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left 8 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (left 4 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0)))))))))
#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left $3 (3x1 (right 3 (3x1 (left $4 $0)))))))))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left 8 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (left 2 (3x1 $0))))))))
#(@$\lambda$@ (1x3 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (1x3 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) (right 4 $0))))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (1x3 $0)))))
#(tower_loopM 4 (@$\lambda$@ (@$\lambda$@ (right 6 (3x1 $0)))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (1x3 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (1x3 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) (right 4 $0)))))) $0)))))
#(@$\lambda$@ (right 4 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0))))
#(@$\lambda$@ (right 2 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0)))
#(@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (1x3 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (1x3 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) (right 4 $0)))))) $0))))) $0 (right 2 (#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left 4 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0)))))) $0 $1)))))
#(tower_loopM 5 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (right 2 (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0))) $0))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (tower_embed (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left $3 (3x1 (right 3 (3x1 (left $4 $0))))))))))) 3 6 3 $0)) $0)))))
#(@$\lambda$@ (@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (tower_embed (@$\lambda$@ (#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left $3 (3x1 (right 3 (3x1 (left $4 $0))))))))))) 4 5 $4 $0)) $0))))))
#(#(@$\lambda$@ (@$\lambda$@ (@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (left $3 (3x1 (right 3 (3x1 (left $4 $0))))))))))) 4 5)
#(@$\lambda$@ (@$\lambda$@ (right 6 (tower_embed $0 $1))))
#(@$\lambda$@ (tower_loopM $0 (@$\lambda$@ (@$\lambda$@ (#(@$\lambda$@ (right 4 (#(@$\lambda$@ (1x3 (1x3 (right 4 (1x3 (1x3 $0)))))) (#(@$\lambda$@ (1x3 (left 4 (1x3 (right 2 (3x1 $0)))))) $0)))) $0)))))
\end{lstlisting}
\subsubsection{Symbolic regression}
\begin{lstlisting}
  #(@$\lambda$@ (+. $0 REAL))
#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL)))
#(/. REAL)
#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL))) (*. (#(@$\lambda$@ (+. $0 REAL)) $0) $0)))
#(@$\lambda$@ (/. (#(/. REAL) $0) $0))
#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL))) (*. $0 (#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL))) (*. (#(@$\lambda$@ (+. $0 REAL)) $0) $0))) $0))))
#(@$\lambda$@ (#(/. REAL) (#(@$\lambda$@ (+. $0 REAL)) $0)))
#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. (#(@$\lambda$@ (#(@$\lambda$@ (#(@$\lambda$@ (+. $0 REAL)) (*. $0 REAL))) (*. (#(@$\lambda$@ (+. $0 REAL)) $0) $0))) $0) $0)))
\end{lstlisting}

\subsection{Hyperparameters and training details}

\noindent\textbf{Neural net architecture} The recognition model for domains with sequential structure (list processing, text editing, regular expressions) is a
recurrent neural network.
We use a bidirectional GRU~\cite{cho2014learning} with 64 hidden units that reads each input/output pair; we concatenate the input and output along with a special delimiter
symbol between them.
We use a 64-dimensional vectors to embed symbols in the input/output.
We MaxPool the final hidden unit activations in the GRU along both passes of the bidirectional GRU.

The recognition model for domains with 2D visual structure (LOGO
graphics, tower building, and symbolic regression) is a convolutional
neural network.
We take our convolutional architecture from~\cite{snell2017prototypical}.

We follow the RNN/CNN by an MLP with 128 hidden units and a ReLU activation which then outputs the $Q_{ijk}$ matrix described in~\ref{recognitionAppendix}.

\noindent\textbf{Neural net training} We train our recognition models using Adam~\cite{kingma2014adam} with a learning rate of 0.001.

\noindent\textbf{\system Hyperparameters}
Due to the high computational cost we
performed only an informal coarse
hyperparameter search.
The most important parameter is
the enumeration timeout during the wake phase;
domains that present more challenging program synthesis
problems require either longer timeouts, more CPUs, or both.
\begin{center}
  \begin{tabular}{ccccccc}
    \toprule
    Domain&Timeout&CPUs&Batch size&$\lambda$ (\ref{appendixCompression})&$\alpha$ (\ref{mapAppendix})&Max beam size (\ref{systemPseudocode})\\\midrule
    Lists&7m&64&10&1.5&30&5\\
    Text&7m&64&10&1.5&30&5\\
    Graphics&1h&96&50&1.5&30&5\\
    Symbolic regression&2m & 40&10 & 1&30&5\\
    Towers& 1h& 64&50 & 1.5&30&5\\
    Regexes&30m & 64&40 & 1.5&30&5\\
    \bottomrule     
  \end{tabular}
\end{center}


%\subsection{Results on held out text patterns}
%\makeatletter
%\renewcommand{\verbatim@font}{\ttfamily\tiny} 
%\input{regex.tex}

\end{document}

